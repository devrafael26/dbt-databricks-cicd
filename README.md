
![Pipeline ELT Databricks + DBT](https://github.com/devrafael26/dbt-databricks-cicd/blob/main/ELT%20Databricks%20DBT.png?raw=true)


1ï¸âƒ£ ExtraÃ§Ã£o â€” Conectei ao SQL Server via notebook Python, extraÃ­ a tabela e salvei os dados em arquivo CSV.

 2ï¸âƒ£ Carga no Databricks â€” Fiz o upload do CSV, atravÃ©s do Data Ingestion, onde a plataforma reconhece automaticamente a estrutura, define tipos de dados e permite escolher o Catalog e Schema para criaÃ§Ã£o da tabela.

 3ï¸âƒ£ Armazenamento em Delta Lake â€” A tabela foi criada no formato Delta na camada Bronze do Delta Lake (dados brutos). A partir daÃ­, utilizei o dbt para estruturar as camadas raw, staging e mart.

 4ï¸âƒ£ TransformaÃ§Ã£o com DBT â€” Organizei o projeto em camadas raw, staging e mart, garantindo um fluxo claro de transformaÃ§Ã£o de dados:

 â€¢ Raw â†’ Dados brutos refletindo fielmente a fonte.
 
 â€¢ Staging â†’ PadronizaÃ§Ã£o e estruturaÃ§Ã£o dos dados para uso interno.
 
 â€¢ Mart â†’ AplicaÃ§Ã£o de regras de negÃ³cio e dados prontos para anÃ¡lise.

Na camada raw, utilizei testes de schema para garantir integridade e consistÃªncia com a estrutura esperada do SQL Server.

Na camada staging, padronizo os dados e aplico testes como por exemplo not_null e non_negative, para manter a qualidade.

Na camada mart, aplico regras de negÃ³cio e mais testes para assegurar que os resultados finais atendam Ã s expectativas do negÃ³cio.

Aqui a parte em que achei mais legal! ğŸ˜ 

Os testes automatizados como assert_source_structure, not_null, non_negative e accepted_values, alÃ©m da documentaÃ§Ã£o centralizada, facilitam a governanÃ§a e manutenÃ§Ã£o ao longo de todo o fluxo.

Por exemplo: se algum tipo de dado mudar no SQL Server ou surgirem valores diferentes do esperado no schema, testes como assert_source_structure e accepted_values jÃ¡ acusam o problema antes mesmo de chegar ao Databricks.

AÃ­ vocÃª pensa: â€œMeu Deus, quantos testes, Rafael!â€ E Ã© isso mesmo â€” testes que nÃ£o acabam mais! ğŸ˜‚
Brincadeiras Ã  parte, isso garante algo muito importante na construÃ§Ã£o de ELT/ETL: a rastreabilidade, essencial para encontrar rapidamente a origem de qualquer problema.

AlÃ©m disso, configurei GitHub Actions para automatizar testes e deploy do DBT, garantindo que cada alteraÃ§Ã£o seja validada e integrada de forma contÃ­nua.

Por fim, conectei o Databricks ao Power BI para criar grÃ¡ficos e dashboards, mostrando os dados transformados e prontos para anÃ¡lise.
