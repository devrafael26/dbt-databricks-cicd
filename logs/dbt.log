[0m13:55:40.496468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023085152250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023085150590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023085152690>]}


============================== 13:55:40.509469 | 91b15751-623c-42d6-b65a-6198b932e628 ==============================
[0m13:55:40.509469 [info ] [MainThread]: Running with dbt=1.10.3
[0m13:55:40.513469 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt init dbt_databricks_cicd', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:55:40.602465 [debug] [MainThread]: Starter project path: C:\Users\diniz\Python\Lib\site-packages\dbt\include\starter_project
[0m13:55:40.740468 [info ] [MainThread]: 
Your new dbt project "dbt_databricks_cicd" was created!

For more information on how to configure the profiles.yml file,
please consult the dbt documentation here:

  https://docs.getdbt.com/docs/configure-your-profile

One more thing:

Need help? Don't hesitate to reach out to us via GitHub issues or on Slack:

  https://community.getdbt.com/

Happy modeling!

[0m13:55:40.742469 [info ] [MainThread]: Setting up your profile.
[0m13:55:40.755483 [debug] [MainThread]: Command `dbt init` succeeded at 13:55:40.754468 after 0.46 seconds
[0m13:55:40.757468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230FE95DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002308515D590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230FE6C8510>]}
[0m13:55:40.758467 [debug] [MainThread]: Flushing usage events
[0m13:55:41.769467 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:45:37.663258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028FB22AE390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028FB22ADF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028FB22AE290>]}


============================== 14:45:37.670660 | 551868b6-09c8-4fa4-8bae-0325ff622ce6 ==============================
[0m14:45:37.670660 [info ] [MainThread]: Running with dbt=1.10.3
[0m14:45:37.673661 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'invocation_command': 'dbt debug --target sqlserver', 'send_anonymous_usage_stats': 'True'}
[0m14:45:37.732410 [info ] [MainThread]: dbt version: 1.10.3
[0m14:45:37.733411 [info ] [MainThread]: python version: 3.11.1
[0m14:45:37.735446 [info ] [MainThread]: python path: C:\Users\diniz\Python\python.exe
[0m14:45:37.736447 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m14:45:37.743405 [info ] [MainThread]: Using profiles dir at C:\Users\diniz\.dbt
[0m14:45:37.744405 [info ] [MainThread]: Using profiles.yml file at C:\Users\diniz\.dbt\profiles.yml
[0m14:45:37.745406 [info ] [MainThread]: Using dbt_project.yml file at D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\dbt_project.yml
[0m14:45:37.747405 [info ] [MainThread]: Configuration:
[0m14:45:37.748406 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m14:45:37.750406 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m14:45:37.751404 [info ] [MainThread]: Required dependencies:
[0m14:45:37.753407 [debug] [MainThread]: Executing "git --help"
[0m14:45:37.846409 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:45:37.848409 [debug] [MainThread]: STDERR: "b''"
[0m14:45:37.850410 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:45:37.853410 [info ] [MainThread]: Connection test skipped since no profile was found
[0m14:45:37.855408 [info ] [MainThread]: [31m2 checks failed:[0m
[0m14:45:37.857407 [info ] [MainThread]: Could not load dbt_project.yml
Profile loading failed for the following reason:
Runtime Error
  The profile 'meu_projeto_dbt' does not have a target named 'sqlserver'. The valid target names for this profile are:
   - dev

Runtime Error
  The profile 'sqlserver_dbt_cicd' does not have a target named 'sqlserver'. The valid target names for this profile are:
   - dev

Runtime Error
  The profile 'dbt_databricks_cicd' does not have a target named 'sqlserver'. The valid target names for this profile are:
   - dev


[0m14:45:37.860412 [info ] [MainThread]: Project loading failed for the following reason:
 project path <D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\dbt_project.yml> not found

[0m14:45:37.864409 [debug] [MainThread]: Command `dbt debug` failed at 14:45:37.863415 after 0.34 seconds
[0m14:45:37.865407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028FB23170D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028FB2317750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028FAB848690>]}
[0m14:45:37.868408 [debug] [MainThread]: Flushing usage events
[0m14:45:38.675114 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:56:59.174010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF26A1D250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF26A1F350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF26A1F090>]}


============================== 07:56:59.189675 | 7bc27e43-11cd-4729-8b16-c35bb33ef816 ==============================
[0m07:56:59.189675 [info ] [MainThread]: Running with dbt=1.10.3
[0m07:56:59.189675 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'log_path': 'logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'invocation_command': 'dbt debug --target sqlserver', 'send_anonymous_usage_stats': 'True'}
[0m07:56:59.236528 [info ] [MainThread]: dbt version: 1.10.3
[0m07:56:59.236528 [info ] [MainThread]: python version: 3.11.1
[0m07:56:59.236528 [info ] [MainThread]: python path: C:\Users\diniz\Python\python.exe
[0m07:56:59.236528 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m07:56:59.252147 [info ] [MainThread]: Using profiles dir at C:\Users\diniz\.dbt
[0m07:56:59.252147 [info ] [MainThread]: Using profiles.yml file at C:\Users\diniz\.dbt\profiles.yml
[0m07:56:59.252147 [info ] [MainThread]: Using dbt_project.yml file at D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\dbt_project.yml
[0m07:56:59.252147 [info ] [MainThread]: Configuration:
[0m07:56:59.252147 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m07:56:59.252147 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m07:56:59.252147 [info ] [MainThread]: Required dependencies:
[0m07:56:59.252147 [debug] [MainThread]: Executing "git --help"
[0m07:56:59.361511 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m07:56:59.361511 [debug] [MainThread]: STDERR: "b''"
[0m07:56:59.361511 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m07:56:59.361511 [info ] [MainThread]: Connection test skipped since no profile was found
[0m07:56:59.361511 [info ] [MainThread]: [31m2 checks failed:[0m
[0m07:56:59.377157 [info ] [MainThread]: Could not load dbt_project.yml
Profile loading failed for the following reason:
Runtime Error
  The profile 'meu_projeto_dbt' does not have a target named 'sqlserver'. The valid target names for this profile are:
   - dev

Runtime Error
  The profile 'sqlserver_dbt_cicd' does not have a target named 'sqlserver'. The valid target names for this profile are:
   - dev

Runtime Error
  The profile 'dbt_databricks_cicd' does not have a target named 'sqlserver'. The valid target names for this profile are:
   - dev


[0m07:56:59.377157 [info ] [MainThread]: Project loading failed for the following reason:
 project path <D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\dbt_project.yml> not found

[0m07:56:59.377157 [debug] [MainThread]: Command `dbt debug` failed at 07:56:59.377157 after 0.35 seconds
[0m07:56:59.377157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF266FB110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF26A13250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF26A13550>]}
[0m07:56:59.377157 [debug] [MainThread]: Flushing usage events
[0m07:57:00.970963 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m07:59:14.793149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194EB821C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194E832BD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194EB823C90>]}


============================== 07:59:14.808778 | 45db8e4b-6b9d-4c25-adc3-9c2ff28ea03b ==============================
[0m07:59:14.808778 [info ] [MainThread]: Running with dbt=1.10.3
[0m07:59:14.808778 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt debug --target sqlserver', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m07:59:14.855649 [info ] [MainThread]: dbt version: 1.10.3
[0m07:59:14.855649 [info ] [MainThread]: python version: 3.11.1
[0m07:59:14.855649 [info ] [MainThread]: python path: C:\Users\diniz\Python\python.exe
[0m07:59:14.855649 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m07:59:20.541128 [info ] [MainThread]: Using profiles dir at C:\Users\diniz\.dbt
[0m07:59:20.541128 [info ] [MainThread]: Using profiles.yml file at C:\Users\diniz\.dbt\profiles.yml
[0m07:59:20.541128 [info ] [MainThread]: Using dbt_project.yml file at D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\dbt_project.yml
[0m07:59:20.541128 [info ] [MainThread]: Configuration:
[0m07:59:20.541128 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m07:59:20.541128 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m07:59:20.541128 [info ] [MainThread]: Required dependencies:
[0m07:59:20.556743 [debug] [MainThread]: Executing "git --help"
[0m07:59:20.666124 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m07:59:20.666124 [debug] [MainThread]: STDERR: "b''"
[0m07:59:20.666124 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m07:59:20.666124 [info ] [MainThread]: Connection test skipped since no profile was found
[0m07:59:20.666124 [info ] [MainThread]: [31m2 checks failed:[0m
[0m07:59:20.681745 [info ] [MainThread]: Could not load dbt_project.yml
Profile loading failed for the following reason:
Runtime Error
  The profile 'meu_projeto_dbt' does not have a target named 'sqlserver'. The valid target names for this profile are:
   - dev

Runtime Error
  The profile 'sqlserver_dbt_cicd' does not have a target named 'sqlserver'. The valid target names for this profile are:
   - dev


[0m07:59:20.681745 [info ] [MainThread]: Project loading failed for the following reason:
 project path <D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\dbt_project.yml> not found

[0m07:59:20.681745 [debug] [MainThread]: Command `dbt debug` failed at 07:59:20.681745 after 6.03 seconds
[0m07:59:20.681745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194E4D7BAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194E4D7BB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194EB84B8D0>]}
[0m07:59:20.681745 [debug] [MainThread]: Flushing usage events
[0m07:59:21.291192 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m08:07:05.738680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000292AB2F1B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000292AB2F0B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000292AB2F2E50>]}


============================== 08:07:05.738680 | e209bcf5-f9d0-4613-af66-aaa8da9fedf2 ==============================
[0m08:07:05.738680 [info ] [MainThread]: Running with dbt=1.10.3
[0m08:07:05.738680 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt debug --target sqlserver', 'send_anonymous_usage_stats': 'True'}
[0m08:07:05.785556 [info ] [MainThread]: dbt version: 1.10.3
[0m08:07:05.785556 [info ] [MainThread]: python version: 3.11.1
[0m08:07:05.785556 [info ] [MainThread]: python path: C:\Users\diniz\Python\python.exe
[0m08:07:05.785556 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m08:07:06.400586 [info ] [MainThread]: Using profiles dir at C:\Users\diniz\.dbt
[0m08:07:06.400586 [info ] [MainThread]: Using profiles.yml file at C:\Users\diniz\.dbt\profiles.yml
[0m08:07:06.400586 [info ] [MainThread]: Using dbt_project.yml file at D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\dbt_project.yml
[0m08:07:06.400586 [info ] [MainThread]: Configuration:
[0m08:07:06.400586 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m08:07:06.400586 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m08:07:06.400586 [info ] [MainThread]: Required dependencies:
[0m08:07:06.416206 [debug] [MainThread]: Executing "git --help"
[0m08:07:06.494291 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m08:07:06.494291 [debug] [MainThread]: STDERR: "b''"
[0m08:07:06.494291 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m08:07:06.494291 [info ] [MainThread]: Connection test skipped since no profile was found
[0m08:07:06.509914 [info ] [MainThread]: [31m2 checks failed:[0m
[0m08:07:06.509914 [info ] [MainThread]: Could not load dbt_project.yml
Profile loading failed for the following reason:
Runtime Error
  The profile 'meu_projeto_dbt' does not have a target named 'sqlserver'. The valid target names for this profile are:
   - dev


[0m08:07:06.509914 [info ] [MainThread]: Project loading failed for the following reason:
 project path <D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\dbt_project.yml> not found

[0m08:07:06.509914 [debug] [MainThread]: Command `dbt debug` failed at 08:07:06.509914 after 0.92 seconds
[0m08:07:06.509914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000292AB3AA8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000292AB2F3110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000292A48F86D0>]}
[0m08:07:06.509914 [debug] [MainThread]: Flushing usage events
[0m08:07:07.119366 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:51:41.921165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014707427A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014707427390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147074279D0>]}


============================== 13:51:41.921165 | 2f1e4d17-763e-4f02-bef5-b798081c56db ==============================
[0m13:51:41.921165 [info ] [MainThread]: Running with dbt=1.10.3
[0m13:51:41.936778 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select top_customers.sql', 'send_anonymous_usage_stats': 'True'}
[0m13:51:48.296275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2f1e4d17-763e-4f02-bef5-b798081c56db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001470747C050>]}
[0m13:51:48.405579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2f1e4d17-763e-4f02-bef5-b798081c56db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014707457250>]}
[0m13:51:48.421210 [info ] [MainThread]: Registered adapter: sqlserver=1.9.0
[0m13:51:49.374294 [debug] [MainThread]: checksum: 1bfacb87e6914c9942cfa2c4badfc1df19d859fe8ec086286ef238a7d920f97b, vars: {}, profile: , target: , version: 1.10.3
[0m13:51:49.561832 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m13:51:49.561832 [debug] [MainThread]: previous checksum: 1bfacb87e6914c9942cfa2c4badfc1df19d859fe8ec086286ef238a7d920f97b, current checksum: 136566b3e36444f529acd895ee0315ebaa97f244a4cd6a9860c3053f0a594331
[0m13:51:49.561832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2f1e4d17-763e-4f02-bef5-b798081c56db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147063B8990>]}
[0m13:51:53.509491 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_databricks_cicd.example
[0m13:51:53.525110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2f1e4d17-763e-4f02-bef5-b798081c56db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147072142D0>]}
[0m13:51:53.607963 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m13:51:53.639182 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m13:51:53.748547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2f1e4d17-763e-4f02-bef5-b798081c56db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147091316D0>]}
[0m13:51:53.748547 [info ] [MainThread]: Found 510 macros
[0m13:51:53.764172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f1e4d17-763e-4f02-bef5-b798081c56db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014707413ED0>]}
[0m13:51:53.764172 [warn ] [MainThread]: The selection criterion 'top_customers.sql' does not match any enabled nodes
[0m13:51:53.764172 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m13:51:53.764172 [debug] [MainThread]: Command end result
[0m13:51:53.811045 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m13:51:53.811045 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m13:51:53.826670 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\run_results.json
[0m13:51:53.826670 [debug] [MainThread]: Command `dbt run` succeeded at 13:51:53.826670 after 12.05 seconds
[0m13:51:53.826670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014708CDC150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014700BE0390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014700BE0F90>]}
[0m13:51:53.826670 [debug] [MainThread]: Flushing usage events
[0m13:51:54.420644 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:57:34.848089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAA84AB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAA84A510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAA849E50>]}


============================== 12:57:34.863713 | 4ba7188d-4a67-44ba-ba32-99aa26fb1cff ==============================
[0m12:57:34.863713 [info ] [MainThread]: Running with dbt=1.10.3
[0m12:57:34.863713 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'invocation_command': 'dbt debug --target sqlserver', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:57:34.926216 [info ] [MainThread]: dbt version: 1.10.3
[0m12:57:34.926216 [info ] [MainThread]: python version: 3.11.1
[0m12:57:34.926216 [info ] [MainThread]: python path: C:\Users\diniz\Python\python.exe
[0m12:57:34.926216 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m12:57:35.566838 [info ] [MainThread]: Using profiles dir at C:\Users\diniz\.dbt
[0m12:57:35.566838 [info ] [MainThread]: Using profiles.yml file at C:\Users\diniz\.dbt\profiles.yml
[0m12:57:35.566838 [info ] [MainThread]: Using dbt_project.yml file at D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\dbt_project.yml
[0m12:57:35.566838 [info ] [MainThread]: adapter type: sqlserver
[0m12:57:35.566838 [info ] [MainThread]: adapter version: 1.9.0
[0m12:57:35.754380 [info ] [MainThread]: Configuration:
[0m12:57:35.754380 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:57:35.754380 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:57:35.754380 [info ] [MainThread]: Required dependencies:
[0m12:57:35.754380 [debug] [MainThread]: Executing "git --help"
[0m12:57:35.848088 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:57:35.848088 [debug] [MainThread]: STDERR: "b''"
[0m12:57:35.848088 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:57:35.863726 [info ] [MainThread]: Connection:
[0m12:57:35.863726 [info ] [MainThread]:   server: LAPTOP-NV4PR600
[0m12:57:35.863726 [info ] [MainThread]:   database: MY_DB
[0m12:57:35.863726 [info ] [MainThread]:   schema: dbo
[0m12:57:35.863726 [info ] [MainThread]:   UID: dbt_user
[0m12:57:35.863726 [info ] [MainThread]:   client_id: None
[0m12:57:35.863726 [info ] [MainThread]:   authentication: sql
[0m12:57:35.863726 [info ] [MainThread]:   encrypt: True
[0m12:57:35.879338 [info ] [MainThread]:   trust_cert: True
[0m12:57:35.879338 [info ] [MainThread]:   retries: 3
[0m12:57:35.879338 [info ] [MainThread]:   login_timeout: 0
[0m12:57:35.879338 [info ] [MainThread]:   query_timeout: 0
[0m12:57:35.879338 [info ] [MainThread]:   trace_flag: False
[0m12:57:35.879338 [info ] [MainThread]:   port: 1433
[0m12:57:35.879338 [info ] [MainThread]: Registered adapter: sqlserver=1.9.0
[0m12:57:36.926216 [debug] [MainThread]: Acquiring new sqlserver connection 'debug'
[0m12:57:36.926216 [debug] [MainThread]: Using sqlserver connection "debug"
[0m12:57:36.926216 [debug] [MainThread]: On debug: select 1 as id
[0m12:57:36.926216 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:57:36.926216 [debug] [MainThread]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m12:57:37.160585 [debug] [MainThread]: sqlserver adapter: Connected to db: MY_DB
[0m12:57:37.160585 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m12:57:37.160585 [debug] [MainThread]: On debug: Close
[0m12:57:37.160585 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m12:57:37.176221 [info ] [MainThread]: [32mAll checks passed![0m
[0m12:57:37.176221 [debug] [MainThread]: Command `dbt debug` succeeded at 12:57:37.176221 after 2.48 seconds
[0m12:57:37.176221 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:57:37.176221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BA9FB8BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BAC002E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BA40810D0>]}
[0m12:57:37.176221 [debug] [MainThread]: Flushing usage events
[0m12:57:37.769986 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:57:50.976124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8A7596950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8A7596A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8A7595810>]}


============================== 12:57:50.991753 | 994df4c5-fb78-47e3-8bc2-31c4a888b3f5 ==============================
[0m12:57:50.991753 [info ] [MainThread]: Running with dbt=1.10.3
[0m12:57:50.991753 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'invocation_command': 'dbt compile --target sqlserver', 'send_anonymous_usage_stats': 'True'}
[0m12:57:52.116789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '994df4c5-fb78-47e3-8bc2-31c4a888b3f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8A75ED150>]}
[0m12:57:52.226170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '994df4c5-fb78-47e3-8bc2-31c4a888b3f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8A75C7610>]}
[0m12:57:52.226170 [info ] [MainThread]: Registered adapter: sqlserver=1.9.0
[0m12:57:53.022997 [debug] [MainThread]: checksum: 136566b3e36444f529acd895ee0315ebaa97f244a4cd6a9860c3053f0a594331, vars: {}, profile: , target: sqlserver, version: 1.10.3
[0m12:57:53.116748 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m12:57:53.132418 [debug] [MainThread]: previous checksum: 136566b3e36444f529acd895ee0315ebaa97f244a4cd6a9860c3053f0a594331, current checksum: 1bfacb87e6914c9942cfa2c4badfc1df19d859fe8ec086286ef238a7d920f97b
[0m12:57:53.132418 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m12:57:53.132418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '994df4c5-fb78-47e3-8bc2-31c4a888b3f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8A6C87F10>]}
[0m12:57:57.444872 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two models with the name "stg_ecommerce".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for ref("stg_ecommerce").
  
  To fix this, change the name of one of these resources:
  - model.dbt_databricks_cicd.stg_ecommerce (dbt_databricks_cicd/models\staging\stg_ecommerce.sql)
  - model.dbt_databricks_cicd.stg_ecommerce (dbt_databricks_cicd/models\raw\stg_ecommerce.sql)
[0m12:57:57.460497 [debug] [MainThread]: Command `dbt compile` failed at 12:57:57.460497 after 6.63 seconds
[0m12:57:57.460497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8A75E7F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8A75E6310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8A910B4D0>]}
[0m12:57:57.460497 [debug] [MainThread]: Flushing usage events
[0m12:57:58.038766 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:00:46.935483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7B99B6650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7B99B4ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7B99B6150>]}


============================== 13:00:46.935483 | e925cd27-bfe3-4227-9215-8ef5ef34f32f ==============================
[0m13:00:46.935483 [info ] [MainThread]: Running with dbt=1.10.3
[0m13:00:46.951108 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'invocation_command': 'dbt compile --target sqlserver', 'send_anonymous_usage_stats': 'True'}
[0m13:00:47.841739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e925cd27-bfe3-4227-9215-8ef5ef34f32f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7B9A07190>]}
[0m13:00:48.029231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e925cd27-bfe3-4227-9215-8ef5ef34f32f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7BB250F10>]}
[0m13:00:48.029231 [info ] [MainThread]: Registered adapter: sqlserver=1.9.0
[0m13:00:48.794893 [debug] [MainThread]: checksum: 136566b3e36444f529acd895ee0315ebaa97f244a4cd6a9860c3053f0a594331, vars: {}, profile: , target: sqlserver, version: 1.10.3
[0m13:00:48.857404 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m13:00:48.872984 [debug] [MainThread]: previous checksum: 136566b3e36444f529acd895ee0315ebaa97f244a4cd6a9860c3053f0a594331, current checksum: 1bfacb87e6914c9942cfa2c4badfc1df19d859fe8ec086286ef238a7d920f97b
[0m13:00:48.872984 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m13:00:48.872984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e925cd27-bfe3-4227-9215-8ef5ef34f32f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7B8954A50>]}
[0m13:00:51.482364 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_databricks_cicd.src_ecommerce' (dbt_databricks_cicd/models\raw\src_ecommerce.sql) depends on a source named 'sqlserver_data.ecommerce' which was not found
[0m13:00:51.482364 [debug] [MainThread]: Command `dbt compile` failed at 13:00:51.482364 after 4.69 seconds
[0m13:00:51.482364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7B99B69D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7B99B7290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7B3120550>]}
[0m13:00:51.482364 [debug] [MainThread]: Flushing usage events
[0m13:00:52.076249 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:32:54.562335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B4E0D16C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B4E0D16E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B4E0CC5810>]}


============================== 13:32:54.577920 | eff3b742-ca70-44e3-8bdd-f883728b7bf2 ==============================
[0m13:32:54.577920 [info ] [MainThread]: Running with dbt=1.10.3
[0m13:32:54.577920 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt compile --target sqlserver', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:32:55.577965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'eff3b742-ca70-44e3-8bdd-f883728b7bf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B4E253BC50>]}
[0m13:32:55.687293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'eff3b742-ca70-44e3-8bdd-f883728b7bf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B4DFD14210>]}
[0m13:32:55.687293 [info ] [MainThread]: Registered adapter: sqlserver=1.9.0
[0m13:32:56.437292 [debug] [MainThread]: checksum: 136566b3e36444f529acd895ee0315ebaa97f244a4cd6a9860c3053f0a594331, vars: {}, profile: , target: sqlserver, version: 1.10.3
[0m13:32:56.499840 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m13:32:56.515422 [debug] [MainThread]: previous checksum: 136566b3e36444f529acd895ee0315ebaa97f244a4cd6a9860c3053f0a594331, current checksum: 1bfacb87e6914c9942cfa2c4badfc1df19d859fe8ec086286ef238a7d920f97b
[0m13:32:56.515422 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m13:32:56.515422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'eff3b742-ca70-44e3-8bdd-f883728b7bf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B4E03D8390>]}
[0m13:32:59.140465 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_databricks_cicd.src_ecommerce' (dbt_databricks_cicd/models\raw\src_ecommerce.sql) depends on a source named 'sqlserver_data.ecommerce' which was not found
[0m13:32:59.140465 [debug] [MainThread]: Command `dbt compile` failed at 13:32:59.140465 after 4.71 seconds
[0m13:32:59.140465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B4E0D15E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B4E0D171D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B4E2C543D0>]}
[0m13:32:59.140465 [debug] [MainThread]: Flushing usage events
[0m13:32:59.796790 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:36:34.125058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A3F626510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A3F677E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A3F626C90>]}


============================== 13:36:34.140677 | ffc2fb01-695f-4c9f-84d1-8f00ef95c40b ==============================
[0m13:36:34.140677 [info ] [MainThread]: Running with dbt=1.10.3
[0m13:36:34.140677 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt compile --target sqlserver', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:36:35.078220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ffc2fb01-695f-4c9f-84d1-8f00ef95c40b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A40DDF150>]}
[0m13:36:35.187591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ffc2fb01-695f-4c9f-84d1-8f00ef95c40b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A3E674310>]}
[0m13:36:35.187591 [info ] [MainThread]: Registered adapter: sqlserver=1.9.0
[0m13:36:35.953177 [debug] [MainThread]: checksum: 136566b3e36444f529acd895ee0315ebaa97f244a4cd6a9860c3053f0a594331, vars: {}, profile: , target: sqlserver, version: 1.10.3
[0m13:36:36.031299 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m13:36:36.031299 [debug] [MainThread]: previous checksum: 136566b3e36444f529acd895ee0315ebaa97f244a4cd6a9860c3053f0a594331, current checksum: 1bfacb87e6914c9942cfa2c4badfc1df19d859fe8ec086286ef238a7d920f97b
[0m13:36:36.031299 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m13:36:36.031299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ffc2fb01-695f-4c9f-84d1-8f00ef95c40b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A3ED38310>]}
[0m13:36:39.234424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ffc2fb01-695f-4c9f-84d1-8f00ef95c40b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A414D4050>]}
[0m13:36:39.406304 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m13:36:39.421936 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m13:36:39.500056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ffc2fb01-695f-4c9f-84d1-8f00ef95c40b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A41650910>]}
[0m13:36:39.500056 [info ] [MainThread]: Found 9 models, 20 data tests, 2 sources, 510 macros
[0m13:36:39.500056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ffc2fb01-695f-4c9f-84d1-8f00ef95c40b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A4122A190>]}
[0m13:36:39.500056 [info ] [MainThread]: 
[0m13:36:39.500056 [info ] [MainThread]: Concurrency: 1 threads (target='sqlserver')
[0m13:36:39.515679 [info ] [MainThread]: 
[0m13:36:39.515679 [debug] [MainThread]: Acquiring new sqlserver connection 'master'
[0m13:36:39.515679 [debug] [ThreadPool]: Acquiring new sqlserver connection 'list_MY_DB_dbo'
[0m13:36:39.546968 [debug] [ThreadPool]: dbt-sqlserver
[0m13:36:39.546968 [debug] [ThreadPool]: Using sqlserver connection "list_MY_DB_dbo"
[0m13:36:39.546968 [debug] [ThreadPool]: On list_MY_DB_dbo: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "connection_name": "list_MY_DB_dbo"} */
USE [MY_DB];
    with base as (
      select
        DB_NAME() as [database],
        t.name as [name],
        SCHEMA_NAME(t.schema_id) as [schema],
        'table' as table_type
      from sys.tables as t with (nolock)
      union all
      select
        DB_NAME() as [database],
        v.name as [name],
        SCHEMA_NAME(v.schema_id) as [schema],
        'view' as table_type
      from sys.views as v with (nolock)
    )
    select * from base
    where [schema] like 'dbo'
    
    OPTION (LABEL = 'dbt-sqlserver');

  
[0m13:36:39.546968 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:36:39.562553 [debug] [ThreadPool]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:36:39.671970 [debug] [ThreadPool]: sqlserver adapter: Connected to db: MY_DB
[0m13:36:39.734425 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:36:39.734425 [debug] [ThreadPool]: On list_MY_DB_dbo: ROLLBACK
[0m13:36:39.734425 [debug] [ThreadPool]: On list_MY_DB_dbo: Close
[0m13:36:39.734425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ffc2fb01-695f-4c9f-84d1-8f00ef95c40b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A41425150>]}
[0m13:36:39.750058 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.src_ecommerce
[0m13:36:39.750058 [debug] [Thread-1 (]: Acquiring new sqlserver connection 'model.dbt_databricks_cicd.src_ecommerce'
[0m13:36:39.750058 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.src_ecommerce
[0m13:36:39.765677 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.src_ecommerce"
[0m13:36:39.765677 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.src_ecommerce
[0m13:36:39.781303 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.src_ecommerce
[0m13:36:39.781303 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.stg_ecommerce
[0m13:36:39.781303 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.src_ecommerce, now model.dbt_databricks_cicd.stg_ecommerce)
[0m13:36:39.781303 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.stg_ecommerce
[0m13:36:39.781303 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.stg_ecommerce"
[0m13:36:39.781303 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.stg_ecommerce
[0m13:36:39.796969 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.stg_ecommerce
[0m13:36:39.796969 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m13:36:39.796969 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.stg_ecommerce, now test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931)
[0m13:36:39.796969 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m13:36:39.796969 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m13:36:39.796969 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m13:36:39.812548 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m13:36:39.812548 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f
[0m13:36:39.812548 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931, now test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f)
[0m13:36:39.812548 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f
[0m13:36:39.812548 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f"
[0m13:36:39.828177 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f
[0m13:36:39.828177 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f
[0m13:36:39.828177 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m13:36:39.828177 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f, now test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93)
[0m13:36:39.828177 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m13:36:39.828177 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m13:36:39.843802 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m13:36:39.843802 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m13:36:39.843802 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4
[0m13:36:39.843802 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93, now test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4)
[0m13:36:39.843802 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4
[0m13:36:39.843802 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4"
[0m13:36:39.859473 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4
[0m13:36:39.859473 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4
[0m13:36:39.859473 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m13:36:39.859473 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023)
[0m13:36:39.859473 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m13:36:39.875096 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m13:36:39.875096 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m13:36:39.875096 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m13:36:39.875096 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m13:36:39.875096 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6)
[0m13:36:39.875096 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m13:36:39.890713 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m13:36:39.890713 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m13:36:39.890713 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m13:36:39.890713 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m13:36:39.890713 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823)
[0m13:36:39.906341 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m13:36:40.000058 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m13:36:40.000058 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m13:36:40.000058 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m13:36:40.015676 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m13:36:40.015676 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32)
[0m13:36:40.015676 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m13:36:40.031303 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m13:36:40.031303 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m13:36:40.031303 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m13:36:40.041589 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m13:36:40.041589 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a)
[0m13:36:40.041589 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m13:36:40.046609 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m13:36:40.046609 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m13:36:40.046609 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m13:36:40.046609 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m13:36:40.046609 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd)
[0m13:36:40.062260 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m13:36:40.062260 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m13:36:40.062260 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m13:36:40.062260 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m13:36:40.062260 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m13:36:40.062260 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23)
[0m13:36:40.062260 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m13:36:40.077883 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m13:36:40.077883 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m13:36:40.077883 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m13:36:40.077883 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m13:36:40.077883 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325)
[0m13:36:40.077883 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m13:36:40.093505 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m13:36:40.093505 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m13:36:40.093505 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m13:36:40.093505 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04
[0m13:36:40.109129 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04)
[0m13:36:40.109129 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04
[0m13:36:40.109129 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04"
[0m13:36:40.109129 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04
[0m13:36:40.109129 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04
[0m13:36:40.109129 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa
[0m13:36:40.124763 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa)
[0m13:36:40.124763 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa
[0m13:36:40.124763 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa"
[0m13:36:40.124763 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa
[0m13:36:40.124763 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa
[0m13:36:40.124763 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7
[0m13:36:40.140376 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7)
[0m13:36:40.140376 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7
[0m13:36:40.140376 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7"
[0m13:36:40.140376 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7
[0m13:36:40.140376 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7
[0m13:36:40.140376 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2
[0m13:36:40.140376 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2)
[0m13:36:40.156042 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2
[0m13:36:40.156042 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2"
[0m13:36:40.156042 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2
[0m13:36:40.156042 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2
[0m13:36:40.156042 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596
[0m13:36:40.156042 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596)
[0m13:36:40.171625 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596
[0m13:36:40.171625 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596"
[0m13:36:40.171625 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596
[0m13:36:40.171625 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596
[0m13:36:40.171625 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34
[0m13:36:40.187259 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34)
[0m13:36:40.187259 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34
[0m13:36:40.187259 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34"
[0m13:36:40.187259 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34
[0m13:36:40.202915 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34
[0m13:36:40.202915 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4
[0m13:36:40.202915 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4)
[0m13:36:40.202915 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4
[0m13:36:40.202915 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4"
[0m13:36:40.202915 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4
[0m13:36:40.218508 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4
[0m13:36:40.218508 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec
[0m13:36:40.218508 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec)
[0m13:36:40.218508 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec
[0m13:36:40.218508 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec"
[0m13:36:40.234234 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec
[0m13:36:40.234234 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec
[0m13:36:40.234234 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.avg_discount_by_category
[0m13:36:40.234234 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec, now model.dbt_databricks_cicd.avg_discount_by_category)
[0m13:36:40.234234 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.avg_discount_by_category
[0m13:36:40.234234 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.avg_discount_by_category"
[0m13:36:40.234234 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.avg_discount_by_category
[0m13:36:40.249753 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.avg_discount_by_category
[0m13:36:40.249753 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.avg_ticket_by_category
[0m13:36:40.249753 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.avg_discount_by_category, now model.dbt_databricks_cicd.avg_ticket_by_category)
[0m13:36:40.249753 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.avg_ticket_by_category
[0m13:36:40.249753 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.avg_ticket_by_category"
[0m13:36:40.249753 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.avg_ticket_by_category
[0m13:36:40.249753 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.avg_ticket_by_category
[0m13:36:40.249753 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.monthly_revenue
[0m13:36:40.265375 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.avg_ticket_by_category, now model.dbt_databricks_cicd.monthly_revenue)
[0m13:36:40.265375 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.monthly_revenue
[0m13:36:40.265375 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.monthly_revenue"
[0m13:36:40.265375 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.monthly_revenue
[0m13:36:40.265375 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.monthly_revenue
[0m13:36:40.265375 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.payment_distribution
[0m13:36:40.265375 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.monthly_revenue, now model.dbt_databricks_cicd.payment_distribution)
[0m13:36:40.265375 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.payment_distribution
[0m13:36:40.281002 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.payment_distribution"
[0m13:36:40.281002 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.payment_distribution
[0m13:36:40.281002 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.payment_distribution
[0m13:36:40.281002 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.sales_by_category
[0m13:36:40.281002 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.payment_distribution, now model.dbt_databricks_cicd.sales_by_category)
[0m13:36:40.281002 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.sales_by_category
[0m13:36:40.296669 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.sales_by_category"
[0m13:36:40.296669 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.sales_by_category
[0m13:36:40.296669 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.sales_by_category
[0m13:36:40.296669 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.top_5_products
[0m13:36:40.296669 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.sales_by_category, now model.dbt_databricks_cicd.top_5_products)
[0m13:36:40.296669 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.top_5_products
[0m13:36:40.296669 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.top_5_products"
[0m13:36:40.312292 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.top_5_products
[0m13:36:40.312292 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.top_5_products
[0m13:36:40.312292 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.top_customers
[0m13:36:40.312292 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.top_5_products, now model.dbt_databricks_cicd.top_customers)
[0m13:36:40.312292 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.top_customers
[0m13:36:40.312292 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.top_customers"
[0m13:36:40.327883 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.top_customers
[0m13:36:40.327883 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.top_customers
[0m13:36:40.327883 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:36:40.327883 [debug] [MainThread]: Connection 'list_MY_DB_dbo' was properly closed.
[0m13:36:40.327883 [debug] [MainThread]: Connection 'model.dbt_databricks_cicd.top_customers' was properly closed.
[0m13:36:40.343540 [debug] [MainThread]: Command end result
[0m13:36:40.390417 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m13:36:40.390417 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m13:36:40.406042 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\run_results.json
[0m13:36:40.421670 [debug] [MainThread]: Command `dbt compile` succeeded at 13:36:40.406042 after 6.50 seconds
[0m13:36:40.421670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A38DB10D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A38DB04D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A38DB0550>]}
[0m13:36:40.421670 [debug] [MainThread]: Flushing usage events
[0m13:36:40.999774 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:42:52.709779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA9088250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA8E44790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA8E44AD0>]}


============================== 13:42:52.709779 | be3f9c1c-7109-44c7-8dde-96b7cdf04aee ==============================
[0m13:42:52.709779 [info ] [MainThread]: Running with dbt=1.10.3
[0m13:42:52.709779 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'invocation_command': 'dbt build --target sqlserver', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:42:53.631696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA87B7B10>]}
[0m13:42:53.741040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAA8F0FD0>]}
[0m13:42:53.741040 [info ] [MainThread]: Registered adapter: sqlserver=1.9.0
[0m13:42:54.522286 [debug] [MainThread]: checksum: 136566b3e36444f529acd895ee0315ebaa97f244a4cd6a9860c3053f0a594331, vars: {}, profile: , target: sqlserver, version: 1.10.3
[0m13:42:54.912914 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:42:54.912914 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:42:55.022282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAAD4B910>]}
[0m13:42:55.256697 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m13:42:55.256697 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m13:42:55.412948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAAD43710>]}
[0m13:42:55.412948 [info ] [MainThread]: Found 9 models, 20 data tests, 2 sources, 510 macros
[0m13:42:55.428528 [info ] [MainThread]: 
[0m13:42:55.428528 [info ] [MainThread]: Concurrency: 1 threads (target='sqlserver')
[0m13:42:55.428528 [info ] [MainThread]: 
[0m13:42:55.428528 [debug] [MainThread]: Acquiring new sqlserver connection 'master'
[0m13:42:55.444194 [debug] [ThreadPool]: Acquiring new sqlserver connection 'list_MY_DB'
[0m13:42:55.475412 [debug] [ThreadPool]: dbt-sqlserver
[0m13:42:55.475412 [debug] [ThreadPool]: Using sqlserver connection "list_MY_DB"
[0m13:42:55.475412 [debug] [ThreadPool]: On list_MY_DB: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "connection_name": "list_MY_DB"} */
USE [MY_DB];
    select  name as [schema]
    from sys.schemas with (nolock) 
    OPTION (LABEL = 'dbt-sqlserver');

  
[0m13:42:55.475412 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:42:55.475412 [debug] [ThreadPool]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:55.584787 [debug] [ThreadPool]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:55.584787 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:42:55.584787 [debug] [ThreadPool]: On list_MY_DB: Close
[0m13:42:55.600405 [debug] [ThreadPool]: Acquiring new sqlserver connection 'list_MY_DB_dbo'
[0m13:42:55.600405 [debug] [ThreadPool]: dbt-sqlserver
[0m13:42:55.616030 [debug] [ThreadPool]: Using sqlserver connection "list_MY_DB_dbo"
[0m13:42:55.616030 [debug] [ThreadPool]: On list_MY_DB_dbo: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "connection_name": "list_MY_DB_dbo"} */
USE [MY_DB];
    with base as (
      select
        DB_NAME() as [database],
        t.name as [name],
        SCHEMA_NAME(t.schema_id) as [schema],
        'table' as table_type
      from sys.tables as t with (nolock)
      union all
      select
        DB_NAME() as [database],
        v.name as [name],
        SCHEMA_NAME(v.schema_id) as [schema],
        'view' as table_type
      from sys.views as v with (nolock)
    )
    select * from base
    where [schema] like 'dbo'
    
    OPTION (LABEL = 'dbt-sqlserver');

  
[0m13:42:55.616030 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:42:55.616030 [debug] [ThreadPool]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:55.616030 [debug] [ThreadPool]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:55.662946 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m13:42:55.662946 [debug] [ThreadPool]: On list_MY_DB_dbo: ROLLBACK
[0m13:42:55.662946 [debug] [ThreadPool]: On list_MY_DB_dbo: Close
[0m13:42:55.662946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAA997910>]}
[0m13:42:55.662946 [debug] [MainThread]: On master: COMMIT
[0m13:42:55.678534 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m13:42:55.678534 [info ] [Thread-1 (]: 1 of 29 START test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery  [RUN]
[0m13:42:55.678534 [debug] [Thread-1 (]: Acquiring new sqlserver connection 'test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931'
[0m13:42:55.678534 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m13:42:55.709780 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m13:42:55.725422 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m13:42:55.756698 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m13:42:55.772285 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m13:42:55.772285 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_7999099e1334cef9eb7ae8ca8d3f9f76_2110]
   as 
    
    
    

with all_values as (

    select
        Payment_Method as value_field,
        count(*) as n_records

    from "my_db"."dbo"."ecommerce"
    group by Payment_Method

)

select *
from all_values
where value_field not in (
    ''Credit Card'',''Net Banking'',''UPI'',''Debit Card'',''Cash on Delivery''
)



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_7999099e1334cef9eb7ae8ca8d3f9f76_2110]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_7999099e1334cef9eb7ae8ca8d3f9f76_2110]
  ;')
[0m13:42:55.772285 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:42:55.772285 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:55.772285 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:55.819159 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:55.819159 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931: ROLLBACK
[0m13:42:55.819159 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931: Close
[0m13:42:55.819159 [info ] [Thread-1 (]: 1 of 29 PASS source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery  [[32mPASS[0m in 0.14s]
[0m13:42:55.834788 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m13:42:55.834788 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f
[0m13:42:55.834788 [info ] [Thread-1 (]: 2 of 29 START test source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery  [RUN]
[0m13:42:55.834788 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931, now test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f)
[0m13:42:55.834788 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f
[0m13:42:55.850405 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f"
[0m13:42:55.850405 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f
[0m13:42:55.850405 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f"
[0m13:42:55.850405 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f"
[0m13:42:55.866035 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_b50dc3e1996b1a7b7983fa1efb770b9d_4118]
   as 
    
    
    

with all_values as (

    select
        Payment_Method as value_field,
        count(*) as n_records

    from "workspace"."default"."sales_ecommerce"
    group by Payment_Method

)

select *
from all_values
where value_field not in (
    ''Credit Card'',''Net Banking'',''UPI'',''Debit Card'',''Cash on Delivery''
)



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_b50dc3e1996b1a7b7983fa1efb770b9d_4118]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_b50dc3e1996b1a7b7983fa1efb770b9d_4118]
  ;')
[0m13:42:55.866035 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:55.866035 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:55.866035 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:55.881760 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:55.881760 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f: ROLLBACK
[0m13:42:55.881760 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f: Close
[0m13:42:55.881760 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_accepted_values_sqlserv_2459675a1bb0ad6a307b59b3a1008a3b.sql[0m
('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:56.053530 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 290, in execute
    test_result = self.execute_data_test(test, manifest)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 165, in execute_data_test
    macro_func()
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 170, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 562, in execute
    if not cursor.nextset():
           ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")

[0m13:42:56.053530 [error] [Thread-1 (]: 2 of 29 ERROR source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery  [[31mERROR[0m in 0.22s]
[0m13:42:56.069155 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f
[0m13:42:56.069155 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m13:42:56.069155 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f' to be skipped because of status 'error'.  Reason: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)").
[0m13:42:56.069155 [info ] [Thread-1 (]: 3 of 29 START test source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date  [RUN]
[0m13:42:56.069155 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.0dadca1b7f, now test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93)
[0m13:42:56.069155 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m13:42:56.084782 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m13:42:56.084782 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m13:42:56.100406 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m13:42:56.100406 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m13:42:56.100406 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_10725fd4f752a6e354b6f1e3cdf8fb3a_16396]
   as 
    
    with actual as (
        select
            column_name,
            data_type
        from INFORMATION_SCHEMA.COLUMNS
        where TABLE_NAME = ''ecommerce''
          and TABLE_SCHEMA = ''dbo''
    ),
    expected as (
        select * from (values
            (''User_ID'', ''varchar''),
            (''Product_ID'', ''varchar''),
            (''Category'', ''varchar''),
            (''Price'', ''float''),
            (''Discount'', ''int''),
            (''Final_Price'', ''float''),
            (''Payment_Method'', ''varchar''),
            (''Purchase_Date'', ''date'')
            
        ) as t(column_name, data_type)
    ),
    diff_expected as (
        select * from expected
        EXCEPT
        select * from actual
    ),
    diff_actual as (
        select * from actual
        EXCEPT
        select * from expected
    )
    select * from diff_expected
    UNION ALL
    select * from diff_actual

  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_10725fd4f752a6e354b6f1e3cdf8fb3a_16396]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_10725fd4f752a6e354b6f1e3cdf8fb3a_16396]
  ;')
[0m13:42:56.100406 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.100406 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.100406 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.178571 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.194162 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93: ROLLBACK
[0m13:42:56.194162 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93: Close
[0m13:42:56.194162 [info ] [Thread-1 (]: 3 of 29 PASS source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date  [[32mPASS[0m in 0.13s]
[0m13:42:56.194162 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m13:42:56.194162 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4
[0m13:42:56.194162 [info ] [Thread-1 (]: 4 of 29 START test source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date  [RUN]
[0m13:42:56.194162 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93, now test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4)
[0m13:42:56.194162 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4
[0m13:42:56.209782 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4"
[0m13:42:56.209782 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4
[0m13:42:56.209782 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4"
[0m13:42:56.225409 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4"
[0m13:42:56.225409 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_00ec5b81e7471cf033246a1127887a0e_13452]
   as 
    
    with actual as (
        select
            column_name,
            data_type
        from INFORMATION_SCHEMA.COLUMNS
        where TABLE_NAME = ''sales_ecommerce''
          and TABLE_SCHEMA = ''default''
    ),
    expected as (
        select * from (values
            (''User_ID'', ''varchar''),
            (''Product_ID'', ''varchar''),
            (''Category'', ''varchar''),
            (''Price'', ''float''),
            (''Discount'', ''int''),
            (''Final_Price'', ''float''),
            (''Payment_Method'', ''varchar''),
            (''Purchase_Date'', ''date'')
            
        ) as t(column_name, data_type)
    ),
    diff_expected as (
        select * from expected
        EXCEPT
        select * from actual
    ),
    diff_actual as (
        select * from actual
        EXCEPT
        select * from expected
    )
    select * from diff_expected
    UNION ALL
    select * from diff_actual

  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_00ec5b81e7471cf033246a1127887a0e_13452]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_00ec5b81e7471cf033246a1127887a0e_13452]
  ;')
[0m13:42:56.225409 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.225409 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.225409 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.287915 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.287915 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4: ROLLBACK
[0m13:42:56.287915 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4: Close
[0m13:42:56.287915 [error] [Thread-1 (]: 4 of 29 FAIL 8 source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date  [[31mFAIL 8[0m in 0.09s]
[0m13:42:56.303538 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4
[0m13:42:56.303538 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m13:42:56.303538 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4' to be skipped because of status 'fail'.  Reason: Got 8 results, configured to fail if != 0.
[0m13:42:56.303538 [info ] [Thread-1 (]: 5 of 29 START test source_not_null_sqlserver_data_ecommerce_Category ........... [RUN]
[0m13:42:56.303538 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.216f82bfa4, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023)
[0m13:42:56.303538 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m13:42:56.319154 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m13:42:56.319154 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m13:42:56.319154 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m13:42:56.334806 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m13:42:56.334806 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_491fe7bb43562776c9b8b25f5b1f0e65_18577]
   as 
    
    
    



select Category
from "my_db"."dbo"."ecommerce"
where Category is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_491fe7bb43562776c9b8b25f5b1f0e65_18577]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_491fe7bb43562776c9b8b25f5b1f0e65_18577]
  ;')
[0m13:42:56.334806 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.334806 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.334806 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.350446 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.350446 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023: ROLLBACK
[0m13:42:56.350446 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023: Close
[0m13:42:56.350446 [info ] [Thread-1 (]: 5 of 29 PASS source_not_null_sqlserver_data_ecommerce_Category ................. [[32mPASS[0m in 0.05s]
[0m13:42:56.350446 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m13:42:56.366033 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m13:42:56.366033 [info ] [Thread-1 (]: 6 of 29 START test source_not_null_sqlserver_data_ecommerce_Discount ........... [RUN]
[0m13:42:56.366033 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6)
[0m13:42:56.366033 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m13:42:56.366033 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m13:42:56.381659 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m13:42:56.381659 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m13:42:56.381659 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m13:42:56.381659 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_5a693b107c6d94cc971f3f52da874160_17625]
   as 
    
    
    



select Discount
from "my_db"."dbo"."ecommerce"
where Discount is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_5a693b107c6d94cc971f3f52da874160_17625]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_5a693b107c6d94cc971f3f52da874160_17625]
  ;')
[0m13:42:56.381659 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.381659 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.397281 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.397281 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.397281 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6: ROLLBACK
[0m13:42:56.412915 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6: Close
[0m13:42:56.412915 [info ] [Thread-1 (]: 6 of 29 PASS source_not_null_sqlserver_data_ecommerce_Discount ................. [[32mPASS[0m in 0.05s]
[0m13:42:56.412915 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m13:42:56.412915 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m13:42:56.412915 [info ] [Thread-1 (]: 7 of 29 START test source_not_null_sqlserver_data_ecommerce_Final_Price ........ [RUN]
[0m13:42:56.412915 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823)
[0m13:42:56.412915 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m13:42:56.428572 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m13:42:56.428572 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m13:42:56.428572 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m13:42:56.428572 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m13:42:56.428572 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_0c828dfdcf5b3aad6aaa994913a3962f_5050]
   as 
    
    
    



select Final_Price
from "my_db"."dbo"."ecommerce"
where Final_Price is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_0c828dfdcf5b3aad6aaa994913a3962f_5050]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_0c828dfdcf5b3aad6aaa994913a3962f_5050]
  ;')
[0m13:42:56.444173 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.444173 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.444173 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.444173 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.459786 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823: ROLLBACK
[0m13:42:56.459786 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823: Close
[0m13:42:56.459786 [info ] [Thread-1 (]: 7 of 29 PASS source_not_null_sqlserver_data_ecommerce_Final_Price .............. [[32mPASS[0m in 0.05s]
[0m13:42:56.459786 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m13:42:56.459786 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m13:42:56.459786 [info ] [Thread-1 (]: 8 of 29 START test source_not_null_sqlserver_data_ecommerce_Payment_Method ..... [RUN]
[0m13:42:56.459786 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32)
[0m13:42:56.459786 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m13:42:56.475412 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m13:42:56.475412 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m13:42:56.491075 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m13:42:56.491075 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m13:42:56.491075 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_21a139b11381849eb19de652fee70f66_12527]
   as 
    
    
    



select Payment_Method
from "my_db"."dbo"."ecommerce"
where Payment_Method is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_21a139b11381849eb19de652fee70f66_12527]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_21a139b11381849eb19de652fee70f66_12527]
  ;')
[0m13:42:56.491075 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.491075 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.491075 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.506698 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.506698 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32: ROLLBACK
[0m13:42:56.506698 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32: Close
[0m13:42:56.506698 [info ] [Thread-1 (]: 8 of 29 PASS source_not_null_sqlserver_data_ecommerce_Payment_Method ........... [[32mPASS[0m in 0.05s]
[0m13:42:56.506698 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m13:42:56.522297 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m13:42:56.522297 [info ] [Thread-1 (]: 9 of 29 START test source_not_null_sqlserver_data_ecommerce_Price .............. [RUN]
[0m13:42:56.522297 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a)
[0m13:42:56.522297 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m13:42:56.522297 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m13:42:56.522297 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m13:42:56.537910 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m13:42:56.537910 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m13:42:56.537910 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_e72d4251eae6aeb393f5f649a06f5ac0_4816]
   as 
    
    
    



select Price
from "my_db"."dbo"."ecommerce"
where Price is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_e72d4251eae6aeb393f5f649a06f5ac0_4816]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_e72d4251eae6aeb393f5f649a06f5ac0_4816]
  ;')
[0m13:42:56.537910 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.537910 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.537910 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.553537 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.553537 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a: ROLLBACK
[0m13:42:56.553537 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a: Close
[0m13:42:56.553537 [info ] [Thread-1 (]: 9 of 29 PASS source_not_null_sqlserver_data_ecommerce_Price .................... [[32mPASS[0m in 0.03s]
[0m13:42:56.569158 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m13:42:56.569158 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m13:42:56.569158 [info ] [Thread-1 (]: 10 of 29 START test source_not_null_sqlserver_data_ecommerce_Product_ID ........ [RUN]
[0m13:42:56.569158 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd)
[0m13:42:56.569158 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m13:42:56.569158 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m13:42:56.584781 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m13:42:56.584781 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m13:42:56.584781 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m13:42:56.584781 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_aa576225f643234fe9ca29dea78b3ee4_3566]
   as 
    
    
    



select Product_ID
from "my_db"."dbo"."ecommerce"
where Product_ID is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_aa576225f643234fe9ca29dea78b3ee4_3566]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_aa576225f643234fe9ca29dea78b3ee4_3566]
  ;')
[0m13:42:56.584781 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.600411 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.600411 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.600411 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.616031 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd: ROLLBACK
[0m13:42:56.616031 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd: Close
[0m13:42:56.616031 [info ] [Thread-1 (]: 10 of 29 PASS source_not_null_sqlserver_data_ecommerce_Product_ID .............. [[32mPASS[0m in 0.05s]
[0m13:42:56.616031 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m13:42:56.616031 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m13:42:56.616031 [info ] [Thread-1 (]: 11 of 29 START test source_not_null_sqlserver_data_ecommerce_Purchase_Date ..... [RUN]
[0m13:42:56.616031 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23)
[0m13:42:56.616031 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m13:42:56.631657 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m13:42:56.631657 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m13:42:56.631657 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m13:42:56.647301 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m13:42:56.647301 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_0f53a46810cccddc285b5a5f2fa86299_14720]
   as 
    
    
    



select Purchase_Date
from "my_db"."dbo"."ecommerce"
where Purchase_Date is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_0f53a46810cccddc285b5a5f2fa86299_14720]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_0f53a46810cccddc285b5a5f2fa86299_14720]
  ;')
[0m13:42:56.647301 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.647301 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.647301 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.663010 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.663010 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23: ROLLBACK
[0m13:42:56.663010 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23: Close
[0m13:42:56.663010 [info ] [Thread-1 (]: 11 of 29 PASS source_not_null_sqlserver_data_ecommerce_Purchase_Date ........... [[32mPASS[0m in 0.05s]
[0m13:42:56.663010 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m13:42:56.663010 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m13:42:56.663010 [info ] [Thread-1 (]: 12 of 29 START test source_not_null_sqlserver_data_ecommerce_User_ID ........... [RUN]
[0m13:42:56.678529 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325)
[0m13:42:56.678529 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m13:42:56.678529 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m13:42:56.678529 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m13:42:56.694160 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m13:42:56.694160 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m13:42:56.694160 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_cf5a2a1ccc69e2afe103181343e9f1f0_3244]
   as 
    
    
    



select User_ID
from "my_db"."dbo"."ecommerce"
where User_ID is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_cf5a2a1ccc69e2afe103181343e9f1f0_3244]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_cf5a2a1ccc69e2afe103181343e9f1f0_3244]
  ;')
[0m13:42:56.694160 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.694160 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.694160 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.709792 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.709792 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325: ROLLBACK
[0m13:42:56.709792 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325: Close
[0m13:42:56.709792 [info ] [Thread-1 (]: 12 of 29 PASS source_not_null_sqlserver_data_ecommerce_User_ID ................. [[32mPASS[0m in 0.03s]
[0m13:42:56.709792 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m13:42:56.709792 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04
[0m13:42:56.725407 [info ] [Thread-1 (]: 13 of 29 START test source_not_null_sqlserver_data_sales_ecommerce_Category .... [RUN]
[0m13:42:56.725407 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04)
[0m13:42:56.725407 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04
[0m13:42:56.725407 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04"
[0m13:42:56.741029 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04
[0m13:42:56.741029 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04"
[0m13:42:56.741029 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04"
[0m13:42:56.741029 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_0d74b39eb8545add87cd446b19d85149_8066]
   as 
    
    
    



select Category
from "workspace"."default"."sales_ecommerce"
where Category is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_0d74b39eb8545add87cd446b19d85149_8066]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_0d74b39eb8545add87cd446b19d85149_8066]
  ;')
[0m13:42:56.741029 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.741029 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.741029 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.756660 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.756660 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04: ROLLBACK
[0m13:42:56.756660 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04: Close
[0m13:42:56.756660 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Category.sql[0m
('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:56.756660 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 290, in execute
    test_result = self.execute_data_test(test, manifest)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 165, in execute_data_test
    macro_func()
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 170, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 562, in execute
    if not cursor.nextset():
           ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")

[0m13:42:56.772285 [error] [Thread-1 (]: 13 of 29 ERROR source_not_null_sqlserver_data_sales_ecommerce_Category ......... [[31mERROR[0m in 0.05s]
[0m13:42:56.772285 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04
[0m13:42:56.772285 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa
[0m13:42:56.772285 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04' to be skipped because of status 'error'.  Reason: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)").
[0m13:42:56.772285 [info ] [Thread-1 (]: 14 of 29 START test source_not_null_sqlserver_data_sales_ecommerce_Discount .... [RUN]
[0m13:42:56.772285 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Category.c17a290e04, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa)
[0m13:42:56.772285 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa
[0m13:42:56.787909 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa"
[0m13:42:56.787909 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa
[0m13:42:56.787909 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa"
[0m13:42:56.787909 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa"
[0m13:42:56.803532 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_740123bfb0fa861927990afc0b5efe4f_5908]
   as 
    
    
    



select Discount
from "workspace"."default"."sales_ecommerce"
where Discount is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_740123bfb0fa861927990afc0b5efe4f_5908]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_740123bfb0fa861927990afc0b5efe4f_5908]
  ;')
[0m13:42:56.803532 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.803532 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.803532 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.803532 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.803532 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa: ROLLBACK
[0m13:42:56.803532 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa: Close
[0m13:42:56.803532 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Discount.sql[0m
('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:56.819170 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 290, in execute
    test_result = self.execute_data_test(test, manifest)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 165, in execute_data_test
    macro_func()
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 170, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 562, in execute
    if not cursor.nextset():
           ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")

[0m13:42:56.819170 [error] [Thread-1 (]: 14 of 29 ERROR source_not_null_sqlserver_data_sales_ecommerce_Discount ......... [[31mERROR[0m in 0.05s]
[0m13:42:56.819170 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa
[0m13:42:56.819170 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7
[0m13:42:56.819170 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa' to be skipped because of status 'error'.  Reason: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)").
[0m13:42:56.819170 [info ] [Thread-1 (]: 15 of 29 START test source_not_null_sqlserver_data_sales_ecommerce_Final_Price . [RUN]
[0m13:42:56.834793 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Discount.4373208bfa, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7)
[0m13:42:56.834793 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7
[0m13:42:56.834793 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7"
[0m13:42:56.834793 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7
[0m13:42:56.850414 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7"
[0m13:42:56.850414 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7"
[0m13:42:56.850414 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_3037c8ece9cb58b14bf62f030dbe2ff8_7369]
   as 
    
    
    



select Final_Price
from "workspace"."default"."sales_ecommerce"
where Final_Price is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_3037c8ece9cb58b14bf62f030dbe2ff8_7369]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_3037c8ece9cb58b14bf62f030dbe2ff8_7369]
  ;')
[0m13:42:56.850414 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.850414 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.850414 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.866041 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.866041 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7: ROLLBACK
[0m13:42:56.866041 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7: Close
[0m13:42:56.866041 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Final_Price.sql[0m
('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:56.866041 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 290, in execute
    test_result = self.execute_data_test(test, manifest)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 165, in execute_data_test
    macro_func()
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 170, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 562, in execute
    if not cursor.nextset():
           ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")

[0m13:42:56.866041 [error] [Thread-1 (]: 15 of 29 ERROR source_not_null_sqlserver_data_sales_ecommerce_Final_Price ...... [[31mERROR[0m in 0.05s]
[0m13:42:56.866041 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7
[0m13:42:56.881659 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2
[0m13:42:56.881659 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7' to be skipped because of status 'error'.  Reason: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)").
[0m13:42:56.881659 [info ] [Thread-1 (]: 16 of 29 START test source_not_null_sqlserver_data_sales_ecommerce_Payment_Method  [RUN]
[0m13:42:56.881659 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Final_Price.9068bfe6a7, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2)
[0m13:42:56.881659 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2
[0m13:42:56.897287 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2"
[0m13:42:56.897287 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2
[0m13:42:56.897287 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2"
[0m13:42:56.897287 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2"
[0m13:42:56.897287 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_3cfe25ddb861ff5e744fd3a0ffd68836_11281]
   as 
    
    
    



select Payment_Method
from "workspace"."default"."sales_ecommerce"
where Payment_Method is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_3cfe25ddb861ff5e744fd3a0ffd68836_11281]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_3cfe25ddb861ff5e744fd3a0ffd68836_11281]
  ;')
[0m13:42:56.897287 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.912908 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.912908 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:56.912908 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:56.928532 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2: ROLLBACK
[0m13:42:56.928532 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2: Close
[0m13:42:56.928532 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.sql[0m
('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:56.944158 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 290, in execute
    test_result = self.execute_data_test(test, manifest)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 165, in execute_data_test
    macro_func()
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 170, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 562, in execute
    if not cursor.nextset():
           ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")

[0m13:42:56.944158 [error] [Thread-1 (]: 16 of 29 ERROR source_not_null_sqlserver_data_sales_ecommerce_Payment_Method ... [[31mERROR[0m in 0.06s]
[0m13:42:56.944158 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2
[0m13:42:56.944158 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596
[0m13:42:56.959792 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2' to be skipped because of status 'error'.  Reason: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)").
[0m13:42:56.959792 [info ] [Thread-1 (]: 17 of 29 START test source_not_null_sqlserver_data_sales_ecommerce_Price ....... [RUN]
[0m13:42:56.959792 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.1102f0b1a2, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596)
[0m13:42:56.959792 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596
[0m13:42:56.975408 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596"
[0m13:42:56.975408 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596
[0m13:42:56.991033 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596"
[0m13:42:56.991033 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596"
[0m13:42:56.991033 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_2b8e9565935d02960cbcc5201d65c6ed_4049]
   as 
    
    
    



select Price
from "workspace"."default"."sales_ecommerce"
where Price is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_2b8e9565935d02960cbcc5201d65c6ed_4049]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_2b8e9565935d02960cbcc5201d65c6ed_4049]
  ;')
[0m13:42:56.991033 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:56.991033 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:56.991033 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:57.006661 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.006661 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596: ROLLBACK
[0m13:42:57.006661 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596: Close
[0m13:42:57.006661 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Price.sql[0m
('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:57.006661 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 290, in execute
    test_result = self.execute_data_test(test, manifest)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 165, in execute_data_test
    macro_func()
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 170, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 562, in execute
    if not cursor.nextset():
           ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")

[0m13:42:57.022295 [error] [Thread-1 (]: 17 of 29 ERROR source_not_null_sqlserver_data_sales_ecommerce_Price ............ [[31mERROR[0m in 0.05s]
[0m13:42:57.022295 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596
[0m13:42:57.022295 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34
[0m13:42:57.022295 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596' to be skipped because of status 'error'.  Reason: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)").
[0m13:42:57.022295 [info ] [Thread-1 (]: 18 of 29 START test source_not_null_sqlserver_data_sales_ecommerce_Product_ID .. [RUN]
[0m13:42:57.022295 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Price.8a1476f596, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34)
[0m13:42:57.022295 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34
[0m13:42:57.037910 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34"
[0m13:42:57.037910 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34
[0m13:42:57.037910 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34"
[0m13:42:57.053557 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34"
[0m13:42:57.053557 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_b0704b9205f22bd5564f588535cee6ce_6551]
   as 
    
    
    



select Product_ID
from "workspace"."default"."sales_ecommerce"
where Product_ID is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_b0704b9205f22bd5564f588535cee6ce_6551]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_b0704b9205f22bd5564f588535cee6ce_6551]
  ;')
[0m13:42:57.053557 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:57.053557 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:57.053557 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:57.053557 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.069156 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34: ROLLBACK
[0m13:42:57.069156 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34: Close
[0m13:42:57.069156 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Product_ID.sql[0m
('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:57.069156 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 290, in execute
    test_result = self.execute_data_test(test, manifest)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 165, in execute_data_test
    macro_func()
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 170, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 562, in execute
    if not cursor.nextset():
           ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")

[0m13:42:57.069156 [error] [Thread-1 (]: 18 of 29 ERROR source_not_null_sqlserver_data_sales_ecommerce_Product_ID ....... [[31mERROR[0m in 0.05s]
[0m13:42:57.069156 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34
[0m13:42:57.069156 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4
[0m13:42:57.069156 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34' to be skipped because of status 'error'.  Reason: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)").
[0m13:42:57.084782 [info ] [Thread-1 (]: 19 of 29 START test source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date  [RUN]
[0m13:42:57.084782 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Product_ID.d70e8aff34, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4)
[0m13:42:57.084782 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4
[0m13:42:57.084782 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4"
[0m13:42:57.100406 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4
[0m13:42:57.100406 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4"
[0m13:42:57.100406 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4"
[0m13:42:57.100406 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_a8bd3adc71b9237a0184b91248f727ab_16862]
   as 
    
    
    



select Purchase_Date
from "workspace"."default"."sales_ecommerce"
where Purchase_Date is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_a8bd3adc71b9237a0184b91248f727ab_16862]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_a8bd3adc71b9237a0184b91248f727ab_16862]
  ;')
[0m13:42:57.100406 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:57.116032 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:57.116032 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:57.116032 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.116032 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4: ROLLBACK
[0m13:42:57.116032 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4: Close
[0m13:42:57.116032 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.sql[0m
('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:57.131662 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 290, in execute
    test_result = self.execute_data_test(test, manifest)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 165, in execute_data_test
    macro_func()
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 170, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 562, in execute
    if not cursor.nextset():
           ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")

[0m13:42:57.131662 [error] [Thread-1 (]: 19 of 29 ERROR source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date .... [[31mERROR[0m in 0.05s]
[0m13:42:57.131662 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4
[0m13:42:57.131662 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec
[0m13:42:57.131662 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4' to be skipped because of status 'error'.  Reason: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)").
[0m13:42:57.131662 [info ] [Thread-1 (]: 20 of 29 START test source_not_null_sqlserver_data_sales_ecommerce_User_ID ..... [RUN]
[0m13:42:57.131662 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.9b6a91aed4, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec)
[0m13:42:57.147297 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec
[0m13:42:57.147297 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec"
[0m13:42:57.147297 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec
[0m13:42:57.162906 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec"
[0m13:42:57.162906 [debug] [Thread-1 (]: Using sqlserver connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec"
[0m13:42:57.162906 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec"} */

    -- Create target schema if it does not
  USE [MY_DB];
  IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = 'dbo')
  BEGIN
    EXEC('CREATE SCHEMA [dbo]')
  END

  

  
  EXEC('create view 
    [dbo].[testview_3bfa3f2ad42467f2cc1e719d22841892_6843]
   as 
    
    
    



select User_ID
from "workspace"."default"."sales_ecommerce"
where User_ID is null



  ;')
  select
    
    count(*) as failures,
    case when count(*) != 0
      then 'true' else 'false' end as should_warn,
    case when count(*) != 0
      then 'true' else 'false' end as should_error
  from (
    select * from 
    [dbo].[testview_3bfa3f2ad42467f2cc1e719d22841892_6843]
  
  ) dbt_internal_test;

  EXEC('drop view 
    [dbo].[testview_3bfa3f2ad42467f2cc1e719d22841892_6843]
  ;')
[0m13:42:57.162906 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:57.162906 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:57.162906 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:57.162906 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.178573 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec: ROLLBACK
[0m13:42:57.178573 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec: Close
[0m13:42:57.178573 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_User_ID.sql[0m
('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:57.178573 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 290, in execute
    test_result = self.execute_data_test(test, manifest)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\test.py", line 165, in execute_data_test
    macro_func()
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 170, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 562, in execute
    if not cursor.nextset():
           ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")

[0m13:42:57.178573 [error] [Thread-1 (]: 20 of 29 ERROR source_not_null_sqlserver_data_sales_ecommerce_User_ID .......... [[31mERROR[0m in 0.05s]
[0m13:42:57.178573 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec
[0m13:42:57.194192 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec' to be skipped because of status 'error'.  Reason: ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)").
[0m13:42:57.178573 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.src_ecommerce
[0m13:42:57.194192 [info ] [Thread-1 (]: 21 of 29 START sql view model dbo.src_ecommerce ................................ [RUN]
[0m13:42:57.194192 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_sales_ecommerce_User_ID.47a66c98ec, now model.dbt_databricks_cicd.src_ecommerce)
[0m13:42:57.194192 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.src_ecommerce
[0m13:42:57.194192 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.src_ecommerce"
[0m13:42:57.209806 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.src_ecommerce
[0m13:42:57.272290 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_databricks_cicd.src_ecommerce"
[0m13:42:57.272290 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.src_ecommerce"
[0m13:42:57.272290 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.src_ecommerce: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.src_ecommerce"} */
USE [MY_DB];
    
    

    

    
    USE [MY_DB];
    EXEC('
        create view "dbo"."src_ecommerce__dbt_tmp" as select * from "my_db"."dbo"."ecommerce";
    ')


[0m13:42:57.272290 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:57.272290 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:57.272290 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:57.287912 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.303535 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.src_ecommerce"
[0m13:42:57.303535 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.src_ecommerce: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.src_ecommerce"} */
USE [MY_DB];
      EXEC sp_rename 'dbo.src_ecommerce__dbt_tmp', 'src_ecommerce'
[0m13:42:57.522297 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.569154 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.src_ecommerce: COMMIT
[0m13:42:57.584781 [debug] [Thread-1 (]: Applying DROP to: "MY_DB"."dbo"."src_ecommerce__dbt_backup"
[0m13:42:57.600404 [debug] [Thread-1 (]: dbt-sqlserver
[0m13:42:57.600404 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.src_ecommerce"
[0m13:42:57.600404 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.src_ecommerce: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.src_ecommerce"} */

        USE [MY_DB];
        select
            sch.name as schema_name,
            obj.name as view_name
        from sys.sql_expression_dependencies refs
        inner join sys.objects obj
        on refs.referencing_id = obj.object_id
        inner join sys.schemas sch
        on obj.schema_id = sch.schema_id
        where refs.referenced_database_name = 'MY_DB'
        and refs.referenced_schema_name = 'dbo'
        and refs.referenced_entity_name = 'src_ecommerce__dbt_backup'
        and refs.referencing_class = 1
        and obj.type = 'V'
        
    OPTION (LABEL = 'dbt-sqlserver');

      
[0m13:42:57.678535 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.678535 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.src_ecommerce"
[0m13:42:57.678535 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.src_ecommerce: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.src_ecommerce"} */

      
      
    
    USE [MY_DB];
    EXEC('DROP view IF EXISTS "dbo"."src_ecommerce__dbt_backup";');

[0m13:42:57.678535 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.694156 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.src_ecommerce: ROLLBACK
[0m13:42:57.694156 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.src_ecommerce: Close
[0m13:42:57.709793 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAB6FBB50>]}
[0m13:42:57.709793 [info ] [Thread-1 (]: 21 of 29 OK created sql view model dbo.src_ecommerce ........................... [[32mOK[0m in 0.50s]
[0m13:42:57.709793 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.src_ecommerce
[0m13:42:57.709793 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.stg_ecommerce
[0m13:42:57.709793 [info ] [Thread-1 (]: 22 of 29 START sql view model dbo.stg_ecommerce ................................ [RUN]
[0m13:42:57.709793 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.src_ecommerce, now model.dbt_databricks_cicd.stg_ecommerce)
[0m13:42:57.709793 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.stg_ecommerce
[0m13:42:57.725410 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.stg_ecommerce"
[0m13:42:57.725410 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.stg_ecommerce
[0m13:42:57.741029 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_databricks_cicd.stg_ecommerce"
[0m13:42:57.741029 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.stg_ecommerce"
[0m13:42:57.741029 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.stg_ecommerce: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.stg_ecommerce"} */
USE [MY_DB];
    
    

    

    
    USE [MY_DB];
    EXEC('
        create view "dbo"."stg_ecommerce__dbt_tmp" as select * from "my_db"."dbo"."ecommerce";
    ')


[0m13:42:57.741029 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:57.741029 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:57.756661 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:57.756661 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.756661 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.stg_ecommerce"
[0m13:42:57.756661 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.stg_ecommerce: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.stg_ecommerce"} */
USE [MY_DB];
      EXEC sp_rename 'dbo.stg_ecommerce__dbt_tmp', 'stg_ecommerce'
[0m13:42:57.772284 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.772284 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.stg_ecommerce: COMMIT
[0m13:42:57.787907 [debug] [Thread-1 (]: Applying DROP to: "MY_DB"."dbo"."stg_ecommerce__dbt_backup"
[0m13:42:57.787907 [debug] [Thread-1 (]: dbt-sqlserver
[0m13:42:57.787907 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.stg_ecommerce"
[0m13:42:57.787907 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.stg_ecommerce: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.stg_ecommerce"} */

        USE [MY_DB];
        select
            sch.name as schema_name,
            obj.name as view_name
        from sys.sql_expression_dependencies refs
        inner join sys.objects obj
        on refs.referencing_id = obj.object_id
        inner join sys.schemas sch
        on obj.schema_id = sch.schema_id
        where refs.referenced_database_name = 'MY_DB'
        and refs.referenced_schema_name = 'dbo'
        and refs.referenced_entity_name = 'stg_ecommerce__dbt_backup'
        and refs.referencing_class = 1
        and obj.type = 'V'
        
    OPTION (LABEL = 'dbt-sqlserver');

      
[0m13:42:57.803533 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.819154 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.stg_ecommerce"
[0m13:42:57.819154 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.stg_ecommerce: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.stg_ecommerce"} */

      
      
    
    USE [MY_DB];
    EXEC('DROP view IF EXISTS "dbo"."stg_ecommerce__dbt_backup";');

[0m13:42:57.819154 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.819154 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.stg_ecommerce: ROLLBACK
[0m13:42:57.819154 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.stg_ecommerce: Close
[0m13:42:57.819154 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAB0084D0>]}
[0m13:42:57.834792 [info ] [Thread-1 (]: 22 of 29 OK created sql view model dbo.stg_ecommerce ........................... [[32mOK[0m in 0.11s]
[0m13:42:57.834792 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.stg_ecommerce
[0m13:42:57.834792 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.avg_discount_by_category
[0m13:42:57.834792 [info ] [Thread-1 (]: 23 of 29 START sql view model dbo.avg_discount_by_category ..................... [RUN]
[0m13:42:57.834792 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.stg_ecommerce, now model.dbt_databricks_cicd.avg_discount_by_category)
[0m13:42:57.834792 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.avg_discount_by_category
[0m13:42:57.866031 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.avg_discount_by_category"
[0m13:42:57.866031 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.avg_discount_by_category
[0m13:42:57.881656 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_databricks_cicd.avg_discount_by_category"
[0m13:42:57.881656 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.avg_discount_by_category"
[0m13:42:57.881656 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.avg_discount_by_category: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.avg_discount_by_category"} */
USE [MY_DB];
    
    

    

    
    USE [MY_DB];
    EXEC('
        create view "dbo"."avg_discount_by_category__dbt_tmp" as SELECT
  Category,
  ROUND(AVG(CAST(`Discount` AS DOUBLE)), 2) AS avg_discount_percent
FROM "MY_DB"."dbo"."stg_ecommerce"
GROUP BY Category
ORDER BY avg_discount_percent DESC;;
    ')


[0m13:42:57.897296 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:57.897296 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:57.897296 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:57.897296 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:57.897296 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.avg_discount_by_category: ROLLBACK
[0m13:42:57.912909 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.avg_discount_by_category: Close
[0m13:42:57.912909 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\avg_discount_by_category.sql[0m
('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")
[0m13:42:57.959782 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 336, in execute
    return self._execute_model(hook_ctx, context_config, model, context, materialization_macro)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 296, in _execute_model
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 81, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 568, in execute
    while cursor.nextset():
          ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")

[0m13:42:57.959782 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAAD4C7D0>]}
[0m13:42:57.959782 [error] [Thread-1 (]: 23 of 29 ERROR creating sql view model dbo.avg_discount_by_category ............ [[31mERROR[0m in 0.12s]
[0m13:42:57.975405 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.avg_discount_by_category
[0m13:42:57.975405 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.avg_ticket_by_category
[0m13:42:57.975405 [debug] [Thread-4 (]: Marking all children of 'model.dbt_databricks_cicd.avg_discount_by_category' to be skipped because of status 'error'.  Reason: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)").
[0m13:42:57.975405 [info ] [Thread-1 (]: 24 of 29 START sql view model dbo.avg_ticket_by_category ....................... [RUN]
[0m13:42:57.975405 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.avg_discount_by_category, now model.dbt_databricks_cicd.avg_ticket_by_category)
[0m13:42:57.975405 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.avg_ticket_by_category
[0m13:42:57.991033 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.avg_ticket_by_category"
[0m13:42:58.006657 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.avg_ticket_by_category
[0m13:42:58.022293 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_databricks_cicd.avg_ticket_by_category"
[0m13:42:58.022293 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.avg_ticket_by_category"
[0m13:42:58.022293 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.avg_ticket_by_category: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.avg_ticket_by_category"} */
USE [MY_DB];
    
    

    

    
    USE [MY_DB];
    EXEC('
        create view "dbo"."avg_ticket_by_category__dbt_tmp" as 

SELECT
    Category,
    ROUND(AVG(CAST(REPLACE(`Final_Price`, '','', ''.'') AS DOUBLE)), 2) AS avg_ticket
FROM "MY_DB"."dbo"."stg_ecommerce"
GROUP BY Category
ORDER BY avg_ticket DESC;
    ')


[0m13:42:58.037923 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:58.037923 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:58.037923 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:58.037923 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:58.037923 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.avg_ticket_by_category: ROLLBACK
[0m13:42:58.037923 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.avg_ticket_by_category: Close
[0m13:42:58.053531 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\avg_ticket_by_category.sql[0m
('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")
[0m13:42:58.053531 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 336, in execute
    return self._execute_model(hook_ctx, context_config, model, context, materialization_macro)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 296, in _execute_model
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 81, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 568, in execute
    while cursor.nextset():
          ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")

[0m13:42:58.069155 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAAD37310>]}
[0m13:42:58.069155 [error] [Thread-1 (]: 24 of 29 ERROR creating sql view model dbo.avg_ticket_by_category .............. [[31mERROR[0m in 0.08s]
[0m13:42:58.069155 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.avg_ticket_by_category
[0m13:42:58.069155 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.monthly_revenue
[0m13:42:58.069155 [debug] [Thread-4 (]: Marking all children of 'model.dbt_databricks_cicd.avg_ticket_by_category' to be skipped because of status 'error'.  Reason: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)").
[0m13:42:58.069155 [info ] [Thread-1 (]: 25 of 29 START sql view model dbo.monthly_revenue .............................. [RUN]
[0m13:42:58.069155 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.avg_ticket_by_category, now model.dbt_databricks_cicd.monthly_revenue)
[0m13:42:58.069155 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.monthly_revenue
[0m13:42:58.084783 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.monthly_revenue"
[0m13:42:58.084783 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.monthly_revenue
[0m13:42:58.084783 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_databricks_cicd.monthly_revenue"
[0m13:42:58.084783 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.monthly_revenue"
[0m13:42:58.100407 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.monthly_revenue: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.monthly_revenue"} */
USE [MY_DB];
    
    

    

    
    USE [MY_DB];
    EXEC('
        create view "dbo"."monthly_revenue__dbt_tmp" as SELECT
  CAST(DATE_TRUNC(''month'', CAST(Purchase_Date AS DATE)) AS DATE) AS month,
  ROUND(SUM(CAST(REPLACE(`Final_Price`, '','', ''.'') AS DOUBLE)), 2) AS monthly_revenue
FROM "MY_DB"."dbo"."stg_ecommerce"
GROUP BY 1
ORDER BY 1;;
    ')


[0m13:42:58.100407 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:58.100407 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:58.100407 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:58.100407 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:58.100407 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.monthly_revenue: ROLLBACK
[0m13:42:58.100407 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.monthly_revenue: Close
[0m13:42:58.116031 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\monthly_revenue.sql[0m
('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]'DATE_TRUNC' não é um nome da função interna reconhecido. (195) (SQLMoreResults)")
[0m13:42:58.116031 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 336, in execute
    return self._execute_model(hook_ctx, context_config, model, context, materialization_macro)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 296, in _execute_model
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 81, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 568, in execute
    while cursor.nextset():
          ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]'DATE_TRUNC' não é um nome da função interna reconhecido. (195) (SQLMoreResults)")

[0m13:42:58.116031 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAB054FD0>]}
[0m13:42:58.116031 [error] [Thread-1 (]: 25 of 29 ERROR creating sql view model dbo.monthly_revenue ..................... [[31mERROR[0m in 0.05s]
[0m13:42:58.116031 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.monthly_revenue
[0m13:42:58.116031 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.payment_distribution
[0m13:42:58.116031 [debug] [Thread-4 (]: Marking all children of 'model.dbt_databricks_cicd.monthly_revenue' to be skipped because of status 'error'.  Reason: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]'DATE_TRUNC' não é um nome da função interna reconhecido. (195) (SQLMoreResults)").
[0m13:42:58.116031 [info ] [Thread-1 (]: 26 of 29 START sql view model dbo.payment_distribution ......................... [RUN]
[0m13:42:58.131674 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.monthly_revenue, now model.dbt_databricks_cicd.payment_distribution)
[0m13:42:58.131674 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.payment_distribution
[0m13:42:58.131674 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.payment_distribution"
[0m13:42:58.131674 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.payment_distribution
[0m13:42:58.147320 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_databricks_cicd.payment_distribution"
[0m13:42:58.147320 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.payment_distribution"
[0m13:42:58.147320 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.payment_distribution: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.payment_distribution"} */
USE [MY_DB];
    
    

    

    
    USE [MY_DB];
    EXEC('
        create view "dbo"."payment_distribution__dbt_tmp" as SELECT
  Payment_Method,
  COUNT(*) AS total_transactions,
  ROUND(SUM(CAST(REPLACE(`Final_Price`, '','', ''.'') AS DOUBLE)), 2) AS total_value
FROM "MY_DB"."dbo"."stg_ecommerce"
GROUP BY Payment_Method
ORDER BY total_value DESC;;
    ')


[0m13:42:58.147320 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:58.147320 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:58.147320 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:58.162909 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:58.162909 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.payment_distribution: ROLLBACK
[0m13:42:58.162909 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.payment_distribution: Close
[0m13:42:58.162909 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\payment_distribution.sql[0m
('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")
[0m13:42:58.162909 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 336, in execute
    return self._execute_model(hook_ctx, context_config, model, context, materialization_macro)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 296, in _execute_model
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 81, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 568, in execute
    while cursor.nextset():
          ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")

[0m13:42:58.162909 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAAFF8610>]}
[0m13:42:58.162909 [error] [Thread-1 (]: 26 of 29 ERROR creating sql view model dbo.payment_distribution ................ [[31mERROR[0m in 0.03s]
[0m13:42:58.178529 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.payment_distribution
[0m13:42:58.178529 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.sales_by_category
[0m13:42:58.178529 [debug] [Thread-4 (]: Marking all children of 'model.dbt_databricks_cicd.payment_distribution' to be skipped because of status 'error'.  Reason: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)").
[0m13:42:58.178529 [info ] [Thread-1 (]: 27 of 29 START sql view model dbo.sales_by_category ............................ [RUN]
[0m13:42:58.178529 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.payment_distribution, now model.dbt_databricks_cicd.sales_by_category)
[0m13:42:58.178529 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.sales_by_category
[0m13:42:58.194195 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.sales_by_category"
[0m13:42:58.194195 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.sales_by_category
[0m13:42:58.194195 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_databricks_cicd.sales_by_category"
[0m13:42:58.194195 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.sales_by_category"
[0m13:42:58.209813 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.sales_by_category: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.sales_by_category"} */
USE [MY_DB];
    
    

    

    
    USE [MY_DB];
    EXEC('
        create view "dbo"."sales_by_category__dbt_tmp" as SELECT
  Category,
  COUNT(*) AS total_orders,
  ROUND(SUM(CAST(REPLACE(`Final_Price`, '','', ''.'') AS DOUBLE)), 2) AS total_revenue
FROM "MY_DB"."dbo"."stg_ecommerce"
GROUP BY Category
ORDER BY total_revenue DESC;;
    ')


[0m13:42:58.209813 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:58.209813 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:58.209813 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:58.209813 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:58.209813 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.sales_by_category: ROLLBACK
[0m13:42:58.209813 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.sales_by_category: Close
[0m13:42:58.209813 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\sales_by_category.sql[0m
('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")
[0m13:42:58.225438 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 336, in execute
    return self._execute_model(hook_ctx, context_config, model, context, materialization_macro)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 296, in _execute_model
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 81, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 568, in execute
    while cursor.nextset():
          ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")

[0m13:42:58.225438 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAAB1FB90>]}
[0m13:42:58.225438 [error] [Thread-1 (]: 27 of 29 ERROR creating sql view model dbo.sales_by_category ................... [[31mERROR[0m in 0.05s]
[0m13:42:58.225438 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.sales_by_category
[0m13:42:58.225438 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.top_5_products
[0m13:42:58.225438 [debug] [Thread-4 (]: Marking all children of 'model.dbt_databricks_cicd.sales_by_category' to be skipped because of status 'error'.  Reason: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)").
[0m13:42:58.225438 [info ] [Thread-1 (]: 28 of 29 START sql view model dbo.top_5_products ............................... [RUN]
[0m13:42:58.225438 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.sales_by_category, now model.dbt_databricks_cicd.top_5_products)
[0m13:42:58.241034 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.top_5_products
[0m13:42:58.241034 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.top_5_products"
[0m13:42:58.241034 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.top_5_products
[0m13:42:58.256698 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_databricks_cicd.top_5_products"
[0m13:42:58.256698 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.top_5_products"
[0m13:42:58.256698 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.top_5_products: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.top_5_products"} */
USE [MY_DB];
    
    

    

    
    USE [MY_DB];
    EXEC('
        create view "dbo"."top_5_products__dbt_tmp" as 

WITH base AS (
    SELECT
        product_id,
        ROUND(SUM(CAST(REPLACE(`Final_Price`, '','', ''.'') AS DOUBLE)), 2)AS total_sales
    FROM "MY_DB"."dbo"."stg_ecommerce"
    GROUP BY product_id
)

SELECT
    product_id,
    total_sales
FROM base
ORDER BY total_sales DESC
LIMIT 5;
    ')


[0m13:42:58.272281 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:58.272281 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:58.272281 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:58.272281 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:58.272281 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.top_5_products: ROLLBACK
[0m13:42:58.287907 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.top_5_products: Close
[0m13:42:58.287907 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\top_5_products.sql[0m
('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults); [42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a 'LIMIT'. (102)")
[0m13:42:58.287907 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 336, in execute
    return self._execute_model(hook_ctx, context_config, model, context, materialization_macro)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 296, in _execute_model
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 81, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 568, in execute
    while cursor.nextset():
          ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults); [42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a 'LIMIT'. (102)")

[0m13:42:58.287907 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAB7D9850>]}
[0m13:42:58.287907 [error] [Thread-1 (]: 28 of 29 ERROR creating sql view model dbo.top_5_products ...................... [[31mERROR[0m in 0.06s]
[0m13:42:58.303529 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.top_5_products
[0m13:42:58.303529 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.top_customers
[0m13:42:58.303529 [debug] [Thread-4 (]: Marking all children of 'model.dbt_databricks_cicd.top_5_products' to be skipped because of status 'error'.  Reason: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults); [42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a 'LIMIT'. (102)").
[0m13:42:58.303529 [info ] [Thread-1 (]: 29 of 29 START sql view model dbo.top_customers ................................ [RUN]
[0m13:42:58.303529 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.top_5_products, now model.dbt_databricks_cicd.top_customers)
[0m13:42:58.303529 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.top_customers
[0m13:42:58.319156 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.top_customers"
[0m13:42:58.319156 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.top_customers
[0m13:42:58.334795 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_databricks_cicd.top_customers"
[0m13:42:58.334795 [debug] [Thread-1 (]: Using sqlserver connection "model.dbt_databricks_cicd.top_customers"
[0m13:42:58.350406 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.top_customers: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "node_id": "model.dbt_databricks_cicd.top_customers"} */
USE [MY_DB];
    
    

    

    
    USE [MY_DB];
    EXEC('
        create view "dbo"."top_customers__dbt_tmp" as 

SELECT
  User_ID,
  COUNT(*) AS total_orders,
  ROUND(SUM(CAST(REPLACE(`Final_Price`, '','', ''.'') AS DOUBLE)), 2) AS total_spent
FROM "MY_DB"."dbo"."stg_ecommerce"
GROUP BY User_ID
ORDER BY total_spent DESC
LIMIT 10;;
    ')


[0m13:42:58.350406 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:42:58.350406 [debug] [Thread-1 (]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=MY_DB;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m13:42:58.350406 [debug] [Thread-1 (]: sqlserver adapter: Connected to db: MY_DB
[0m13:42:58.350406 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m13:42:58.350406 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.top_customers: ROLLBACK
[0m13:42:58.350406 [debug] [Thread-1 (]: On model.dbt_databricks_cicd.top_customers: Close
[0m13:42:58.366030 [error] [Thread-1 (]: [31mUnhandled error while executing target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\top_customers.sql[0m
('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")
[0m13:42:58.366030 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 363, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 310, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\base.py", line 410, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 336, in execute
    return self._execute_model(hook_ctx, context_config, model, context, materialization_macro)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\task\run.py", line 296, in _execute_model
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 81, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\clients\jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\clients\jinja.py", line 395, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\jinja2\runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\base\impl.py", line 437, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\diniz\Python\Lib\site-packages\dbt\adapters\fabric\fabric_connection_manager.py", line 568, in execute
    while cursor.nextset():
          ^^^^^^^^^^^^^^^^
pyodbc.ProgrammingError: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")

[0m13:42:58.366030 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be3f9c1c-7109-44c7-8dde-96b7cdf04aee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAAB037B10>]}
[0m13:42:58.381657 [error] [Thread-1 (]: 29 of 29 ERROR creating sql view model dbo.top_customers ....................... [[31mERROR[0m in 0.06s]
[0m13:42:58.381657 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.top_customers
[0m13:42:58.381657 [debug] [Thread-4 (]: Marking all children of 'model.dbt_databricks_cicd.top_customers' to be skipped because of status 'error'.  Reason: ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)").
[0m13:42:58.381657 [debug] [MainThread]: On master: COMMIT
[0m13:42:58.381657 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:42:58.381657 [debug] [MainThread]: Connection 'list_MY_DB' was properly closed.
[0m13:42:58.381657 [debug] [MainThread]: Connection 'list_MY_DB_dbo' was properly closed.
[0m13:42:58.397290 [debug] [MainThread]: Connection 'model.dbt_databricks_cicd.top_customers' was properly closed.
[0m13:42:58.397290 [info ] [MainThread]: 
[0m13:42:58.397290 [info ] [MainThread]: Finished running 20 data tests, 9 view models in 0 hours 0 minutes and 2.97 seconds (2.97s).
[0m13:42:58.412905 [debug] [MainThread]: Command end result
[0m13:42:58.475403 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m13:42:58.475403 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m13:42:58.491029 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\run_results.json
[0m13:42:58.491029 [info ] [MainThread]: 
[0m13:42:58.491029 [info ] [MainThread]: [31mCompleted with 17 errors, 0 partial successes, and 0 warnings:[0m
[0m13:42:58.491029 [info ] [MainThread]: 
[0m13:42:58.491029 [error] [MainThread]: [31mFailure in test source_accepted_values_sqlserver_data_sales_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m13:42:58.506695 [error] [MainThread]:   ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:58.506695 [info ] [MainThread]: 
[0m13:42:58.506695 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_accepted_values_sqlserv_2459675a1bb0ad6a307b59b3a1008a3b.sql
[0m13:42:58.506695 [info ] [MainThread]: 
[0m13:42:58.506695 [error] [MainThread]: [31mFailure in test source_assert_schema_matches_sqlserver_data_sales_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m13:42:58.506695 [error] [MainThread]:   Got 8 results, configured to fail if != 0
[0m13:42:58.506695 [info ] [MainThread]: 
[0m13:42:58.506695 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_assert_schema_matches_s_a3c180804a3eda59a19801a4d38a627b.sql
[0m13:42:58.522297 [info ] [MainThread]: 
[0m13:42:58.522297 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_sales_ecommerce_Category (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m13:42:58.522297 [error] [MainThread]:   ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:58.522297 [info ] [MainThread]: 
[0m13:42:58.522297 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Category.sql
[0m13:42:58.522297 [info ] [MainThread]: 
[0m13:42:58.522297 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_sales_ecommerce_Discount (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m13:42:58.537944 [error] [MainThread]:   ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:58.537944 [info ] [MainThread]: 
[0m13:42:58.537944 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Discount.sql
[0m13:42:58.537944 [info ] [MainThread]: 
[0m13:42:58.537944 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_sales_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m13:42:58.537944 [error] [MainThread]:   ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:58.537944 [info ] [MainThread]: 
[0m13:42:58.553574 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Final_Price.sql
[0m13:42:58.553574 [info ] [MainThread]: 
[0m13:42:58.553574 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_sales_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m13:42:58.553574 [error] [MainThread]:   ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:58.553574 [info ] [MainThread]: 
[0m13:42:58.553574 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Payment_Method.sql
[0m13:42:58.553574 [info ] [MainThread]: 
[0m13:42:58.569197 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_sales_ecommerce_Price (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m13:42:58.569197 [error] [MainThread]:   ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:58.569197 [info ] [MainThread]: 
[0m13:42:58.569197 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Price.sql
[0m13:42:58.569197 [info ] [MainThread]: 
[0m13:42:58.569197 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_sales_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m13:42:58.584778 [error] [MainThread]:   ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:58.584778 [info ] [MainThread]: 
[0m13:42:58.584778 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Product_ID.sql
[0m13:42:58.584778 [info ] [MainThread]: 
[0m13:42:58.584778 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m13:42:58.584778 [error] [MainThread]:   ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:58.584778 [info ] [MainThread]: 
[0m13:42:58.584778 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_Purchase_Date.sql
[0m13:42:58.600444 [info ] [MainThread]: 
[0m13:42:58.600444 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_sales_ecommerce_User_ID (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m13:42:58.600444 [error] [MainThread]:   ('42S02', "[42S02] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Nome de objeto 'workspace.default.sales_ecommerce' inválido. (208) (SQLMoreResults)")
[0m13:42:58.600444 [info ] [MainThread]: 
[0m13:42:58.600444 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_sales_ecommerce_User_ID.sql
[0m13:42:58.600444 [info ] [MainThread]: 
[0m13:42:58.600444 [error] [MainThread]: [31mFailure in model avg_discount_by_category (dbt_databricks_cicd/models\mart\avg_discount_by_category.sql)[0m
[0m13:42:58.616070 [error] [MainThread]:   ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")
[0m13:42:58.616070 [info ] [MainThread]: 
[0m13:42:58.616070 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\avg_discount_by_category.sql
[0m13:42:58.616070 [info ] [MainThread]: 
[0m13:42:58.616070 [error] [MainThread]: [31mFailure in model avg_ticket_by_category (dbt_databricks_cicd/models\mart\avg_ticket_by_category.sql)[0m
[0m13:42:58.616070 [error] [MainThread]:   ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")
[0m13:42:58.616070 [info ] [MainThread]: 
[0m13:42:58.631677 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\avg_ticket_by_category.sql
[0m13:42:58.631677 [info ] [MainThread]: 
[0m13:42:58.631677 [error] [MainThread]: [31mFailure in model monthly_revenue (dbt_databricks_cicd/models\mart\monthly_revenue.sql)[0m
[0m13:42:58.631677 [error] [MainThread]:   ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]'DATE_TRUNC' não é um nome da função interna reconhecido. (195) (SQLMoreResults)")
[0m13:42:58.631677 [info ] [MainThread]: 
[0m13:42:58.631677 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\monthly_revenue.sql
[0m13:42:58.631677 [info ] [MainThread]: 
[0m13:42:58.631677 [error] [MainThread]: [31mFailure in model payment_distribution (dbt_databricks_cicd/models\mart\payment_distribution.sql)[0m
[0m13:42:58.647302 [error] [MainThread]:   ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")
[0m13:42:58.647302 [info ] [MainThread]: 
[0m13:42:58.647302 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\payment_distribution.sql
[0m13:42:58.647302 [info ] [MainThread]: 
[0m13:42:58.647302 [error] [MainThread]: [31mFailure in model sales_by_category (dbt_databricks_cicd/models\mart\sales_by_category.sql)[0m
[0m13:42:58.647302 [error] [MainThread]:   ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")
[0m13:42:58.647302 [info ] [MainThread]: 
[0m13:42:58.662906 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\sales_by_category.sql
[0m13:42:58.662906 [info ] [MainThread]: 
[0m13:42:58.662906 [error] [MainThread]: [31mFailure in model top_5_products (dbt_databricks_cicd/models\mart\top_5_products.sql)[0m
[0m13:42:58.662906 [error] [MainThread]:   ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults); [42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a 'LIMIT'. (102)")
[0m13:42:58.662906 [info ] [MainThread]: 
[0m13:42:58.662906 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\top_5_products.sql
[0m13:42:58.662906 [info ] [MainThread]: 
[0m13:42:58.678532 [error] [MainThread]: [31mFailure in model top_customers (dbt_databricks_cicd/models\mart\top_customers.sql)[0m
[0m13:42:58.678532 [error] [MainThread]:   ('42000', "[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Sintaxe incorreta próxima a '`'. (102) (SQLMoreResults)")
[0m13:42:58.678532 [info ] [MainThread]: 
[0m13:42:58.678532 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\mart\top_customers.sql
[0m13:42:58.678532 [info ] [MainThread]: 
[0m13:42:58.678532 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=17 SKIP=0 NO-OP=0 TOTAL=29
[0m13:42:58.678532 [debug] [MainThread]: Command `dbt build` failed at 13:42:58.678532 after 6.12 seconds
[0m13:42:58.694161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA9053890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA9050810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA8D443D0>]}
[0m13:42:58.694161 [debug] [MainThread]: Flushing usage events
[0m13:42:59.413023 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:14:32.699028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E64309710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E6430B0D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E643096D0>]}


============================== 09:14:32.714651 | fb22409a-3a5a-4d55-a9d0-da747ef10c29 ==============================
[0m09:14:32.714651 [info ] [MainThread]: Running with dbt=1.10.3
[0m09:14:32.714651 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt test --target databricks', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True'}
[0m09:14:34.949028 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:14:34.964652 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:14:34.964652 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:14:41.382982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fb22409a-3a5a-4d55-a9d0-da747ef10c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E6D6E7AD0>]}
[0m09:14:41.492391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fb22409a-3a5a-4d55-a9d0-da747ef10c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E63A79D90>]}
[0m09:14:41.507975 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m09:14:42.601726 [debug] [MainThread]: checksum: 3fbc73d786818920aecb1b2190495121ce0b60768fe54c46d521f1e82da30af9, vars: {}, profile: , target: databricks, version: 1.10.3
[0m09:14:42.789267 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m09:14:42.789267 [debug] [MainThread]: previous checksum: 3fbc73d786818920aecb1b2190495121ce0b60768fe54c46d521f1e82da30af9, current checksum: 136566b3e36444f529acd895ee0315ebaa97f244a4cd6a9860c3053f0a594331
[0m09:14:42.789267 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m09:14:42.789267 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m09:14:42.789267 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m09:14:42.789267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fb22409a-3a5a-4d55-a9d0-da747ef10c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E63A18410>]}
[0m09:14:47.929857 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_databricks_cicd.staging.sqlserver
[0m09:14:47.945536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fb22409a-3a5a-4d55-a9d0-da747ef10c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E707E6550>]}
[0m09:14:48.133023 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m09:14:48.133023 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m09:14:48.242351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fb22409a-3a5a-4d55-a9d0-da747ef10c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E70F12D90>]}
[0m09:14:48.242351 [info ] [MainThread]: Found 9 models, 27 data tests, 1 source, 682 macros
[0m09:14:48.242351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb22409a-3a5a-4d55-a9d0-da747ef10c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E70C16B10>]}
[0m09:14:48.257979 [info ] [MainThread]: 
[0m09:14:48.257979 [info ] [MainThread]: Concurrency: 1 threads (target='databricks')
[0m09:14:48.257979 [info ] [MainThread]: 
[0m09:14:48.257979 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m09:14:48.257979 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:14:48.273600 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_default, idle-time=0s, language=None, compute-name=) - Creating connection
[0m09:14:48.273600 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m09:14:48.304867 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m09:14:48.304867 [debug] [ThreadPool]: On list_workspace_default: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "connection_name": "list_workspace_default"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'default'

  
[0m09:14:48.304867 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:14:49.664227 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f06ed1-1edc-1518-9304-afdee1fba535) - Created
[0m09:15:09.571919 [debug] [ThreadPool]: SQL status: OK in 21.270 seconds
[0m09:15:09.603175 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f06ed1-1edc-1518-9304-afdee1fba535, command-id=01f06ed1-1f26-19ec-a7e0-383cbed7e374) - Closing
[0m09:15:10.025054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb22409a-3a5a-4d55-a9d0-da747ef10c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E70C41A10>]}
[0m09:15:10.040713 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:15:10.040713 [info ] [Thread-1 (]: 1 of 27 START test not_null_avg_discount_by_category_Category .................. [RUN]
[0m09:15:10.040713 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b, idle-time=0s, language=None, compute-name=) - Creating connection
[0m09:15:10.040713 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b'
[0m09:15:10.040713 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:15:10.071936 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b"
[0m09:15:10.087548 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:15:10.118805 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b"
[0m09:15:10.134430 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b"
[0m09:15:10.134430 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `workspace`.`default`.`avg_discount_by_category`
where Category is null



  
  
      
    ) dbt_internal_test
[0m09:15:10.134430 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:15:10.900122 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887) - Created
[0m09:15:15.743804 [debug] [Thread-1 (]: SQL status: OK in 5.610 seconds
[0m09:15:15.743804 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-2bc6-1052-9a9d-08fdd3d91aff) - Closing
[0m09:15:15.759423 [info ] [Thread-1 (]: 1 of 27 PASS not_null_avg_discount_by_category_Category ........................ [[32mPASS[0m in 5.72s]
[0m09:15:15.759423 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:15:15.759423 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:15:15.759423 [info ] [Thread-1 (]: 2 of 27 START test not_null_avg_discount_by_category_avg_discount_percent ...... [RUN]
[0m09:15:15.759423 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b, now test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101)
[0m09:15:15.759423 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:15:15.759423 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:15:15.775084 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101"
[0m09:15:15.775084 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:15:15.790678 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101"
[0m09:15:15.790678 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101"
[0m09:15:15.790678 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select avg_discount_percent
from `workspace`.`default`.`avg_discount_by_category`
where avg_discount_percent is null



  
  
      
    ) dbt_internal_test
[0m09:15:19.681376 [debug] [Thread-1 (]: SQL status: OK in 3.890 seconds
[0m09:15:19.696937 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-2eaf-1a0e-b4e0-e0be84b0418b) - Closing
[0m09:15:19.696937 [info ] [Thread-1 (]: 2 of 27 PASS not_null_avg_discount_by_category_avg_discount_percent ............ [[32mPASS[0m in 3.94s]
[0m09:15:19.696937 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:15:19.712554 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:15:19.712554 [info ] [Thread-1 (]: 3 of 27 START test not_null_avg_ticket_by_category_Category .................... [RUN]
[0m09:15:19.712554 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101, now test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f)
[0m09:15:19.712554 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f, idle-time=0.015616893768310547s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:15:19.712554 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:15:19.728178 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f"
[0m09:15:19.728178 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:15:19.743807 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f"
[0m09:15:19.743807 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f"
[0m09:15:19.743807 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `workspace`.`default`.`avg_ticket_by_category`
where Category is null



  
  
      
    ) dbt_internal_test
[0m09:15:20.884459 [debug] [Thread-1 (]: SQL status: OK in 1.140 seconds
[0m09:15:20.884459 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-310a-1445-9614-c24b09887f84) - Closing
[0m09:15:20.884459 [info ] [Thread-1 (]: 3 of 27 PASS not_null_avg_ticket_by_category_Category .......................... [[32mPASS[0m in 1.17s]
[0m09:15:20.884459 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:15:20.884459 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:15:20.884459 [info ] [Thread-1 (]: 4 of 27 START test not_null_avg_ticket_by_category_avg_ticket .................. [RUN]
[0m09:15:20.900046 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f, now test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41)
[0m09:15:20.900046 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41, idle-time=0.015587329864501953s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:15:20.900046 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:15:20.900046 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41"
[0m09:15:20.915674 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:15:20.915674 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41"
[0m09:15:20.915674 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41"
[0m09:15:20.915674 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select avg_ticket
from `workspace`.`default`.`avg_ticket_by_category`
where avg_ticket is null



  
  
      
    ) dbt_internal_test
[0m09:15:21.853224 [debug] [Thread-1 (]: SQL status: OK in 0.940 seconds
[0m09:15:21.853224 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-31bd-1fcf-a540-656aba5cf66c) - Closing
[0m09:15:21.853224 [info ] [Thread-1 (]: 4 of 27 PASS not_null_avg_ticket_by_category_avg_ticket ........................ [[32mPASS[0m in 0.97s]
[0m09:15:21.853224 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:15:21.868843 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:15:21.868843 [info ] [Thread-1 (]: 5 of 27 START test not_null_monthly_revenue_month .............................. [RUN]
[0m09:15:21.868843 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41, now test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b)
[0m09:15:21.868843 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b, idle-time=0.015619039535522461s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:15:21.868843 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:15:21.868843 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b"
[0m09:15:21.884433 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:15:21.884433 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b"
[0m09:15:21.884433 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b"
[0m09:15:21.884433 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select month
from `workspace`.`default`.`monthly_revenue`
where month is null



  
  
      
    ) dbt_internal_test
[0m09:15:22.962560 [debug] [Thread-1 (]: SQL status: OK in 1.080 seconds
[0m09:15:22.962560 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-3252-1b4e-8c18-42839b70fdcf) - Closing
[0m09:15:22.962560 [info ] [Thread-1 (]: 5 of 27 PASS not_null_monthly_revenue_month .................................... [[32mPASS[0m in 1.09s]
[0m09:15:22.978175 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:15:22.978175 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:15:22.978175 [info ] [Thread-1 (]: 6 of 27 START test not_null_monthly_revenue_monthly_revenue .................... [RUN]
[0m09:15:22.978175 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b, now test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b)
[0m09:15:22.978175 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b, idle-time=0.015614986419677734s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:15:22.978175 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:15:22.993797 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b"
[0m09:15:22.993797 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:15:22.993797 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b"
[0m09:15:23.009422 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b"
[0m09:15:23.009422 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select monthly_revenue
from `workspace`.`default`.`monthly_revenue`
where monthly_revenue is null



  
  
      
    ) dbt_internal_test
[0m09:15:24.056356 [debug] [Thread-1 (]: SQL status: OK in 1.050 seconds
[0m09:15:24.056356 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-32fc-16c8-8982-7158a4d7ba99) - Closing
[0m09:15:24.056356 [info ] [Thread-1 (]: 6 of 27 PASS not_null_monthly_revenue_monthly_revenue .......................... [[32mPASS[0m in 1.08s]
[0m09:15:24.056356 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:15:24.071963 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:15:24.071963 [info ] [Thread-1 (]: 7 of 27 START test not_null_payment_distribution_Payment_Method ................ [RUN]
[0m09:15:24.071963 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b, now test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae)
[0m09:15:24.071963 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae, idle-time=0.015606403350830078s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:15:24.071963 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:15:24.071963 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae"
[0m09:15:24.087548 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:15:24.087548 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae"
[0m09:15:24.087548 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae"
[0m09:15:24.087548 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Payment_Method
from `workspace`.`default`.`payment_distribution`
where Payment_Method is null



  
  
      
    ) dbt_internal_test
[0m09:15:25.040746 [debug] [Thread-1 (]: SQL status: OK in 0.950 seconds
[0m09:15:25.056338 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-33a2-1cc5-bab9-a18560e4f99a) - Closing
[0m09:15:25.056338 [info ] [Thread-1 (]: 7 of 27 PASS not_null_payment_distribution_Payment_Method ...................... [[32mPASS[0m in 0.98s]
[0m09:15:25.056338 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:15:25.056338 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:15:25.056338 [info ] [Thread-1 (]: 8 of 27 START test not_null_payment_distribution_total_transactions ............ [RUN]
[0m09:15:25.056338 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae, now test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8)
[0m09:15:25.056338 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:15:25.071922 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:15:25.071922 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8"
[0m09:15:25.071922 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:15:25.087562 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8"
[0m09:15:25.087562 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8"
[0m09:15:25.087562 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_transactions
from `workspace`.`default`.`payment_distribution`
where total_transactions is null



  
  
      
    ) dbt_internal_test
[0m09:15:25.962613 [debug] [Thread-1 (]: SQL status: OK in 0.860 seconds
[0m09:15:25.962613 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-343b-1b9a-8596-1c00eda081f0) - Closing
[0m09:15:25.962613 [info ] [Thread-1 (]: 8 of 27 PASS not_null_payment_distribution_total_transactions .................. [[32mPASS[0m in 0.91s]
[0m09:15:25.962613 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:15:25.962613 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:15:25.962613 [info ] [Thread-1 (]: 9 of 27 START test not_null_payment_distribution_total_value ................... [RUN]
[0m09:15:25.978176 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8, now test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03)
[0m09:15:25.978176 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03, idle-time=0.015563011169433594s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:15:25.978176 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:15:25.978176 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03"
[0m09:15:25.978176 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:15:25.993803 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03"
[0m09:15:25.993803 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03"
[0m09:15:25.993803 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_value
from `workspace`.`default`.`payment_distribution`
where total_value is null



  
  
      
    ) dbt_internal_test
[0m09:15:26.962625 [debug] [Thread-1 (]: SQL status: OK in 0.970 seconds
[0m09:15:26.978212 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-34c5-1a26-a086-32e211856195) - Closing
[0m09:15:26.978212 [info ] [Thread-1 (]: 9 of 27 PASS not_null_payment_distribution_total_value ......................... [[32mPASS[0m in 1.00s]
[0m09:15:26.978212 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:15:26.978212 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:15:26.978212 [info ] [Thread-1 (]: 10 of 27 START test not_null_sales_by_category_Category ........................ [RUN]
[0m09:15:26.978212 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03, now test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507)
[0m09:15:26.978212 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:15:26.978212 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:15:26.993800 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507"
[0m09:15:26.993800 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:15:26.993800 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507"
[0m09:15:27.009421 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507"
[0m09:15:27.009421 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `workspace`.`default`.`sales_by_category`
where Category is null



  
  
      
    ) dbt_internal_test
[0m09:15:27.915720 [debug] [Thread-1 (]: SQL status: OK in 0.910 seconds
[0m09:15:27.931359 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-355f-1243-94dd-1faeb75fa2eb) - Closing
[0m09:15:27.931359 [info ] [Thread-1 (]: 10 of 27 PASS not_null_sales_by_category_Category .............................. [[32mPASS[0m in 0.95s]
[0m09:15:27.931359 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:15:27.931359 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:15:27.946940 [info ] [Thread-1 (]: 11 of 27 START test not_null_sales_by_category_total_orders .................... [RUN]
[0m09:15:27.946940 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507, now test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e)
[0m09:15:27.946940 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e, idle-time=0.015581369400024414s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:15:27.946940 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:15:27.962555 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e"
[0m09:15:27.962555 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:15:27.978176 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e"
[0m09:15:27.978176 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e"
[0m09:15:27.978176 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_orders
from `workspace`.`default`.`sales_by_category`
where total_orders is null



  
  
      
    ) dbt_internal_test
[0m09:15:28.634486 [debug] [Thread-1 (]: SQL status: OK in 0.660 seconds
[0m09:15:28.634486 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-35f2-1bf3-a92d-c302d3170d6c) - Closing
[0m09:15:28.650059 [info ] [Thread-1 (]: 11 of 27 PASS not_null_sales_by_category_total_orders .......................... [[32mPASS[0m in 0.70s]
[0m09:15:28.650059 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:15:28.650059 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:15:28.650059 [info ] [Thread-1 (]: 12 of 27 START test not_null_sales_by_category_total_revenue ................... [RUN]
[0m09:15:28.650059 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e, now test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd)
[0m09:15:28.665682 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd, idle-time=0.015624046325683594s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:15:28.665682 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:15:28.681303 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd"
[0m09:15:28.681303 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:15:28.681303 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd"
[0m09:15:28.681303 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd"
[0m09:15:28.696947 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_revenue
from `workspace`.`default`.`sales_by_category`
where total_revenue is null



  
  
      
    ) dbt_internal_test
[0m09:15:29.603220 [debug] [Thread-1 (]: SQL status: OK in 0.910 seconds
[0m09:15:29.603220 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-365f-1c38-a47d-60e4fd580f11) - Closing
[0m09:15:29.603220 [info ] [Thread-1 (]: 12 of 27 PASS not_null_sales_by_category_total_revenue ......................... [[32mPASS[0m in 0.95s]
[0m09:15:29.603220 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:15:29.603220 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:15:29.603220 [info ] [Thread-1 (]: 13 of 27 START test not_null_top_5_products_product_id ......................... [RUN]
[0m09:15:29.618831 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd, now test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe)
[0m09:15:29.618831 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe, idle-time=0.015610694885253906s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:15:29.618831 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:15:29.618831 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe"
[0m09:15:29.618831 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:15:29.634428 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe"
[0m09:15:29.634428 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe"
[0m09:15:29.634428 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from `workspace`.`default`.`top_5_products`
where product_id is null



  
  
      
    ) dbt_internal_test
[0m09:15:30.790748 [debug] [Thread-1 (]: SQL status: OK in 1.160 seconds
[0m09:15:30.806362 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-36f0-1bb2-9161-42a26c92ff0e) - Closing
[0m09:15:30.806362 [info ] [Thread-1 (]: 13 of 27 PASS not_null_top_5_products_product_id ............................... [[32mPASS[0m in 1.19s]
[0m09:15:30.806362 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:15:30.806362 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:15:30.821923 [info ] [Thread-1 (]: 14 of 27 START test not_null_top_5_products_total_sales ........................ [RUN]
[0m09:15:30.821923 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe, now test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818)
[0m09:15:30.821923 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818, idle-time=0.01556086540222168s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:15:30.821923 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:15:30.821923 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818"
[0m09:15:30.821923 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:15:30.837586 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818"
[0m09:15:30.853170 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818"
[0m09:15:30.853170 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_sales
from `workspace`.`default`.`top_5_products`
where total_sales is null



  
  
      
    ) dbt_internal_test
[0m09:15:31.775117 [debug] [Thread-1 (]: SQL status: OK in 0.920 seconds
[0m09:15:31.775117 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-37a9-140b-b3f5-2a4cc33515cf) - Closing
[0m09:15:31.775117 [info ] [Thread-1 (]: 14 of 27 PASS not_null_top_5_products_total_sales .............................. [[32mPASS[0m in 0.95s]
[0m09:15:31.775117 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:15:31.775117 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:15:31.790710 [info ] [Thread-1 (]: 15 of 27 START test not_null_top_customers_User_ID ............................. [RUN]
[0m09:15:31.790710 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818, now test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0)
[0m09:15:31.790710 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0, idle-time=0.015593290328979492s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:15:31.790710 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:15:31.790710 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0"
[0m09:15:31.806302 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:15:31.806302 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0"
[0m09:15:31.806302 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0"
[0m09:15:31.806302 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select User_ID
from `workspace`.`default`.`top_customers`
where User_ID is null



  
  
      
    ) dbt_internal_test
[0m09:15:32.759460 [debug] [Thread-1 (]: SQL status: OK in 0.950 seconds
[0m09:15:32.759460 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-383b-1b17-87b0-78f8ec7bce78) - Closing
[0m09:15:32.759460 [info ] [Thread-1 (]: 15 of 27 PASS not_null_top_customers_User_ID ................................... [[32mPASS[0m in 0.97s]
[0m09:15:32.759460 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:15:32.759460 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:15:32.759460 [info ] [Thread-1 (]: 16 of 27 START test not_null_top_customers_total_orders ........................ [RUN]
[0m09:15:32.759460 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0, now test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746)
[0m09:15:32.759460 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:15:32.775047 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:15:32.775047 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746"
[0m09:15:32.775047 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:15:32.790679 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746"
[0m09:15:32.790679 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746"
[0m09:15:32.790679 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_orders
from `workspace`.`default`.`top_customers`
where total_orders is null



  
  
      
    ) dbt_internal_test
[0m09:15:33.493868 [debug] [Thread-1 (]: SQL status: OK in 0.700 seconds
[0m09:15:33.509496 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-38d1-1929-8473-4937553b6134) - Closing
[0m09:15:33.509496 [info ] [Thread-1 (]: 16 of 27 PASS not_null_top_customers_total_orders .............................. [[32mPASS[0m in 0.75s]
[0m09:15:33.509496 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:15:33.509496 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:15:33.525099 [info ] [Thread-1 (]: 17 of 27 START test not_null_top_customers_total_spent ......................... [RUN]
[0m09:15:33.525099 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746, now test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73)
[0m09:15:33.525099 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73, idle-time=0.01560354232788086s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:15:33.525099 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:15:33.525099 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73"
[0m09:15:33.540680 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:15:33.540680 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73"
[0m09:15:33.540680 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73"
[0m09:15:33.540680 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_spent
from `workspace`.`default`.`top_customers`
where total_spent is null



  
  
      
    ) dbt_internal_test
[0m09:15:34.571987 [debug] [Thread-1 (]: SQL status: OK in 1.030 seconds
[0m09:15:34.571987 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-3944-1562-bd7e-de0c9e9977d7) - Closing
[0m09:15:34.571987 [info ] [Thread-1 (]: 17 of 27 PASS not_null_top_customers_total_spent ............................... [[32mPASS[0m in 1.05s]
[0m09:15:34.571987 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:15:34.571987 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:15:34.587557 [info ] [Thread-1 (]: 18 of 27 START test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery  [RUN]
[0m09:15:34.587557 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73, now test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931)
[0m09:15:34.587557 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931, idle-time=0.01557016372680664s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:15:34.587557 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:15:34.587557 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m09:15:34.603180 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:15:34.603180 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m09:15:34.603180 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m09:15:34.603180 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        Payment_Method as value_field,
        count(*) as n_records

    from `my_db`.`dbo`.`ecommerce`
    group by Payment_Method

)

select *
from all_values
where value_field not in (
    'Credit Card','Net Banking','UPI','Debit Card','Cash on Delivery'
)



  
  
      
    ) dbt_internal_test
[0m09:15:35.150106 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        Payment_Method as value_field,
        count(*) as n_records

    from `my_db`.`dbo`.`ecommerce`
    group by Payment_Method

)

select *
from all_values
where value_field not in (
    'Credit Card','Net Banking','UPI','Debit Card','Cash on Delivery'
)



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-39e6-1b2a-9143-1b1576aa3ce7
[0m09:15:35.306298 [debug] [Thread-1 (]: Database Error in test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_accepted_values_sqlserv_de5401a33d073d59e860da01ddb6837b.sql
[0m09:15:35.306298 [error] [Thread-1 (]: 18 of 27 ERROR source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery  [[31mERROR[0m in 0.72s]
[0m09:15:35.306298 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:15:35.306298 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:15:35.306298 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931' to be skipped because of status 'error'.  Reason: Database Error in test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_accepted_values_sqlserv_de5401a33d073d59e860da01ddb6837b.sql.
[0m09:15:35.306298 [info ] [Thread-1 (]: 19 of 27 START test source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date  [RUN]
[0m09:15:35.306298 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931, now test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93)
[0m09:15:35.306298 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:15:35.306298 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:15:35.321931 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m09:15:35.321931 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:15:35.337545 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m09:15:35.337545 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m09:15:35.337545 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    with actual as (
        select
            column_name,
            data_type
        from INFORMATION_SCHEMA.COLUMNS
        where TABLE_NAME = 'ecommerce'
          and TABLE_SCHEMA = 'dbo'
    ),
    expected as (
        select * from (values
            ('User_ID', 'varchar'),
            ('Product_ID', 'varchar'),
            ('Category', 'varchar'),
            ('Price', 'float'),
            ('Discount', 'int'),
            ('Final_Price', 'float'),
            ('Payment_Method', 'varchar'),
            ('Purchase_Date', 'date')
            
        ) as t(column_name, data_type)
    ),
    diff_expected as (
        select * from expected
        EXCEPT
        select * from actual
    ),
    diff_actual as (
        select * from actual
        EXCEPT
        select * from expected
    )
    select * from diff_expected
    UNION ALL
    select * from diff_actual

  
  
      
    ) dbt_internal_test
[0m09:15:37.306308 [debug] [Thread-1 (]: SQL status: OK in 1.970 seconds
[0m09:15:37.306308 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, command-id=01f06ed1-3a57-111e-8453-7b6ec2c55b6c) - Closing
[0m09:15:37.306308 [error] [Thread-1 (]: 19 of 27 FAIL 8 source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date  [[31mFAIL 8[0m in 2.00s]
[0m09:15:37.321936 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:15:37.321936 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:15:37.321936 [info ] [Thread-1 (]: 20 of 27 START test source_not_null_sqlserver_data_ecommerce_Category .......... [RUN]
[0m09:15:37.321936 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023)
[0m09:15:37.321936 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023, idle-time=0.01562786102294922s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:15:37.337606 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:15:37.353183 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m09:15:37.353183 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:15:37.368871 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m09:15:37.368871 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m09:15:37.368871 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `my_db`.`dbo`.`ecommerce`
where Category is null



  
  
      
    ) dbt_internal_test
[0m09:15:37.835182 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `my_db`.`dbo`.`ecommerce`
where Category is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-3b8c-1578-809b-221fa79a1f09
[0m09:15:37.835182 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Category (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Category.sql
[0m09:15:37.835182 [error] [Thread-1 (]: 20 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Category ............... [[31mERROR[0m in 0.51s]
[0m09:15:37.835182 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:15:37.850748 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Category (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Category.sql.
[0m09:15:37.835182 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:15:37.850748 [info ] [Thread-1 (]: 21 of 27 START test source_not_null_sqlserver_data_ecommerce_Discount .......... [RUN]
[0m09:15:37.850748 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6)
[0m09:15:37.850748 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6, idle-time=0.01556539535522461s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:15:37.850748 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:15:37.850748 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m09:15:37.866408 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:15:37.866408 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m09:15:37.866408 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m09:15:37.866408 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Discount
from `my_db`.`dbo`.`ecommerce`
where Discount is null



  
  
      
    ) dbt_internal_test
[0m09:15:38.225885 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Discount
from `my_db`.`dbo`.`ecommerce`
where Discount is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-3bd8-1e6e-8b29-e99f3a2fcdf0
[0m09:15:38.241406 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Discount (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Discount.sql
[0m09:15:38.241406 [error] [Thread-1 (]: 21 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Discount ............... [[31mERROR[0m in 0.39s]
[0m09:15:38.241406 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:15:38.241406 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:15:38.241406 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Discount (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Discount.sql.
[0m09:15:38.241406 [info ] [Thread-1 (]: 22 of 27 START test source_not_null_sqlserver_data_ecommerce_Final_Price ....... [RUN]
[0m09:15:38.241406 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823)
[0m09:15:38.257032 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:15:38.257032 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:15:38.257032 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m09:15:38.257032 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:15:38.272613 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m09:15:38.272613 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m09:15:38.272613 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Final_Price
from `my_db`.`dbo`.`ecommerce`
where Final_Price is null



  
  
      
    ) dbt_internal_test
[0m09:15:38.678868 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Final_Price
from `my_db`.`dbo`.`ecommerce`
where Final_Price is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-3c17-163e-b60e-454aa16bc772
[0m09:15:38.694496 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Final_Price.sql
[0m09:15:38.694496 [error] [Thread-1 (]: 22 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Final_Price ............ [[31mERROR[0m in 0.45s]
[0m09:15:38.694496 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:15:38.694496 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:15:38.694496 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Final_Price.sql.
[0m09:15:38.694496 [info ] [Thread-1 (]: 23 of 27 START test source_not_null_sqlserver_data_ecommerce_Payment_Method .... [RUN]
[0m09:15:38.694496 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32)
[0m09:15:38.710130 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32, idle-time=0.015633106231689453s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:15:38.710130 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:15:38.710130 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m09:15:38.725743 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:15:38.725743 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m09:15:38.725743 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m09:15:38.725743 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Payment_Method
from `my_db`.`dbo`.`ecommerce`
where Payment_Method is null



  
  
      
    ) dbt_internal_test
[0m09:15:39.257042 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Payment_Method
from `my_db`.`dbo`.`ecommerce`
where Payment_Method is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-3c5d-11fb-a4ad-c6f27025e8c5
[0m09:15:39.272623 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Payment_Method.sql
[0m09:15:39.272623 [error] [Thread-1 (]: 23 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Payment_Method ......... [[31mERROR[0m in 0.58s]
[0m09:15:39.272623 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:15:39.288253 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:15:39.288253 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Payment_Method.sql.
[0m09:15:39.288253 [info ] [Thread-1 (]: 24 of 27 START test source_not_null_sqlserver_data_ecommerce_Price ............. [RUN]
[0m09:15:39.288253 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a)
[0m09:15:39.288253 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a, idle-time=0.01562976837158203s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:15:39.288253 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:15:39.303904 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m09:15:39.303904 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:15:39.303904 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m09:15:39.319494 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m09:15:39.319494 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Price
from `my_db`.`dbo`.`ecommerce`
where Price is null



  
  
      
    ) dbt_internal_test
[0m09:15:39.725815 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Price
from `my_db`.`dbo`.`ecommerce`
where Price is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-3cb5-108e-a26c-7c8ac04945d5
[0m09:15:39.741434 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Price (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Price.sql
[0m09:15:39.741434 [error] [Thread-1 (]: 24 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Price .................. [[31mERROR[0m in 0.45s]
[0m09:15:39.756994 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:15:39.756994 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:15:39.756994 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Price (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Price.sql.
[0m09:15:39.756994 [info ] [Thread-1 (]: 25 of 27 START test source_not_null_sqlserver_data_ecommerce_Product_ID ........ [RUN]
[0m09:15:39.756994 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd)
[0m09:15:39.756994 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd, idle-time=0.015559911727905273s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:15:39.756994 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:15:39.772615 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m09:15:39.772615 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:15:39.788279 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m09:15:39.788279 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m09:15:39.788279 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Product_ID
from `my_db`.`dbo`.`ecommerce`
where Product_ID is null



  
  
      
    ) dbt_internal_test
[0m09:15:40.147691 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Product_ID
from `my_db`.`dbo`.`ecommerce`
where Product_ID is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-3cfd-1876-9362-e0d6104555d1
[0m09:15:40.163281 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Product_ID.sql
[0m09:15:40.163281 [error] [Thread-1 (]: 25 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Product_ID ............. [[31mERROR[0m in 0.41s]
[0m09:15:40.163281 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:15:40.163281 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:15:40.163281 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Product_ID.sql.
[0m09:15:40.163281 [info ] [Thread-1 (]: 26 of 27 START test source_not_null_sqlserver_data_ecommerce_Purchase_Date ..... [RUN]
[0m09:15:40.178867 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23)
[0m09:15:40.178867 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23, idle-time=0.015586137771606445s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:15:40.178867 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:15:40.194494 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m09:15:40.194494 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:15:40.210116 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m09:15:40.210116 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m09:15:40.225741 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Purchase_Date
from `my_db`.`dbo`.`ecommerce`
where Purchase_Date is null



  
  
      
    ) dbt_internal_test
[0m09:15:40.585120 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Purchase_Date
from `my_db`.`dbo`.`ecommerce`
where Purchase_Date is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-3d3f-124e-b282-d37a37fa4f9c
[0m09:15:40.600741 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Purchase_Date.sql
[0m09:15:40.600741 [error] [Thread-1 (]: 26 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Purchase_Date .......... [[31mERROR[0m in 0.42s]
[0m09:15:40.600741 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:15:40.600741 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m09:15:40.616364 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Purchase_Date.sql.
[0m09:15:40.616364 [info ] [Thread-1 (]: 27 of 27 START test source_not_null_sqlserver_data_ecommerce_User_ID ........... [RUN]
[0m09:15:40.616364 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325)
[0m09:15:40.616364 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325, idle-time=0.015623331069946289s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:15:40.616364 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m09:15:40.631990 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m09:15:40.631990 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m09:15:40.647625 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m09:15:40.647625 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m09:15:40.647625 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select User_ID
from `my_db`.`dbo`.`ecommerce`
where User_ID is null



  
  
      
    ) dbt_internal_test
[0m09:15:41.053871 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select User_ID
from `my_db`.`dbo`.`ecommerce`
where User_ID is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-3d80-1f23-ba07-af4d5f41ab20
[0m09:15:41.069554 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_User_ID (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_User_ID.sql
[0m09:15:41.069554 [error] [Thread-1 (]: 27 of 27 ERROR source_not_null_sqlserver_data_ecommerce_User_ID ................ [[31mERROR[0m in 0.45s]
[0m09:15:41.069554 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m09:15:41.069554 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_User_ID (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_User_ID.sql.
[0m09:15:41.069554 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=31.044500827789307s, language=None, compute-name=) - Reusing connection previously named master
[0m09:15:41.085122 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:15:41.085122 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m09:15:41.085122 [debug] [MainThread]: On list_workspace_default: Close
[0m09:15:41.085122 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f06ed1-1edc-1518-9304-afdee1fba535) - Closing
[0m09:15:41.303926 [debug] [MainThread]: Connection 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325' was properly closed.
[0m09:15:41.303926 [debug] [MainThread]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325: Close
[0m09:15:41.303926 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f06ed1-2ba3-1e1e-99e3-4e4c0642c887) - Closing
[0m09:15:41.522701 [info ] [MainThread]: 
[0m09:15:41.538275 [info ] [MainThread]: Finished running 27 data tests in 0 hours 0 minutes and 53.26 seconds (53.26s).
[0m09:15:41.553926 [debug] [MainThread]: Command end result
[0m09:15:41.631989 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m09:15:41.631989 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m09:15:41.647617 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\run_results.json
[0m09:15:41.647617 [info ] [MainThread]: 
[0m09:15:41.647617 [info ] [MainThread]: [31mCompleted with 10 errors, 0 partial successes, and 0 warnings:[0m
[0m09:15:41.663262 [info ] [MainThread]: 
[0m09:15:41.663262 [error] [MainThread]: [31mFailure in test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:15:41.663262 [error] [MainThread]:   Database Error in test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_accepted_values_sqlserv_de5401a33d073d59e860da01ddb6837b.sql
[0m09:15:41.663262 [info ] [MainThread]: 
[0m09:15:41.663262 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_accepted_values_sqlserv_de5401a33d073d59e860da01ddb6837b.sql
[0m09:15:41.663262 [info ] [MainThread]: 
[0m09:15:41.678904 [error] [MainThread]: [31mFailure in test source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:15:41.678904 [error] [MainThread]:   Got 8 results, configured to fail if != 0
[0m09:15:41.678904 [info ] [MainThread]: 
[0m09:15:41.678904 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_assert_schema_matches_s_a76a1d3a3c37dcddf2ac3a6e53d8459a.sql
[0m09:15:41.678904 [info ] [MainThread]: 
[0m09:15:41.678904 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Category (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:15:41.678904 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Category (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Category.sql
[0m09:15:41.694490 [info ] [MainThread]: 
[0m09:15:41.694490 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Category.sql
[0m09:15:41.694490 [info ] [MainThread]: 
[0m09:15:41.694490 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Discount (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:15:41.694490 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Discount (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Discount.sql
[0m09:15:41.694490 [info ] [MainThread]: 
[0m09:15:41.710114 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Discount.sql
[0m09:15:41.710114 [info ] [MainThread]: 
[0m09:15:41.710114 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:15:41.710114 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Final_Price.sql
[0m09:15:41.710114 [info ] [MainThread]: 
[0m09:15:41.710114 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Final_Price.sql
[0m09:15:41.725739 [info ] [MainThread]: 
[0m09:15:41.725739 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:15:41.725739 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Payment_Method.sql
[0m09:15:41.725739 [info ] [MainThread]: 
[0m09:15:41.725739 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Payment_Method.sql
[0m09:15:41.725739 [info ] [MainThread]: 
[0m09:15:41.725739 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Price (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:15:41.741365 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Price (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Price.sql
[0m09:15:41.741365 [info ] [MainThread]: 
[0m09:15:41.741365 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Price.sql
[0m09:15:41.741365 [info ] [MainThread]: 
[0m09:15:41.741365 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:15:41.756988 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Product_ID.sql
[0m09:15:41.756988 [info ] [MainThread]: 
[0m09:15:41.756988 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Product_ID.sql
[0m09:15:41.756988 [info ] [MainThread]: 
[0m09:15:41.756988 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:15:41.756988 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Purchase_Date.sql
[0m09:15:41.772654 [info ] [MainThread]: 
[0m09:15:41.772654 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Purchase_Date.sql
[0m09:15:41.772654 [info ] [MainThread]: 
[0m09:15:41.772654 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_User_ID (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:15:41.772654 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_User_ID (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_User_ID.sql
[0m09:15:41.788298 [info ] [MainThread]: 
[0m09:15:41.788298 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_User_ID.sql
[0m09:15:41.788298 [info ] [MainThread]: 
[0m09:15:41.788298 [info ] [MainThread]: Done. PASS=17 WARN=0 ERROR=10 SKIP=0 NO-OP=0 TOTAL=27
[0m09:15:41.788298 [debug] [MainThread]: Command `dbt test` failed at 09:15:41.788298 after 69.24 seconds
[0m09:15:41.788298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E64356E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E643570D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E70687610>]}
[0m09:15:41.803907 [debug] [MainThread]: Flushing usage events
[0m09:15:42.491446 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:18:48.415377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA48C4850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA48E6FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA48E6950>]}


============================== 09:18:48.415377 | ba9fa600-f4e2-4997-9456-25c58fccaf99 ==============================
[0m09:18:48.415377 [info ] [MainThread]: Running with dbt=1.10.3
[0m09:18:48.431000 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt test --target databricks', 'send_anonymous_usage_stats': 'True'}
[0m09:18:50.305996 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:18:50.305996 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:18:50.305996 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:18:52.102912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ba9fa600-f4e2-4997-9456-25c58fccaf99', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA489AE50>]}
[0m09:18:52.243505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ba9fa600-f4e2-4997-9456-25c58fccaf99', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA4009B90>]}
[0m09:18:52.259125 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m09:18:53.352876 [debug] [MainThread]: checksum: 3fbc73d786818920aecb1b2190495121ce0b60768fe54c46d521f1e82da30af9, vars: {}, profile: , target: databricks, version: 1.10.3
[0m09:18:53.540375 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m09:18:53.556000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ba9fa600-f4e2-4997-9456-25c58fccaf99', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA3FA0110>]}
[0m09:18:57.259123 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.dbt_databricks_cicd.raw.sqlserver
- models.dbt_databricks_cicd.staging.sqlserver
[0m09:18:57.290380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ba9fa600-f4e2-4997-9456-25c58fccaf99', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBB11CF850>]}
[0m09:18:57.462248 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m09:18:57.462248 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m09:18:57.540371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ba9fa600-f4e2-4997-9456-25c58fccaf99', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBB14EFF10>]}
[0m09:18:57.540371 [info ] [MainThread]: Found 9 models, 27 data tests, 1 source, 682 macros
[0m09:18:57.556000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ba9fa600-f4e2-4997-9456-25c58fccaf99', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBB1207E90>]}
[0m09:18:57.556000 [info ] [MainThread]: 
[0m09:18:57.556000 [info ] [MainThread]: Concurrency: 1 threads (target='databricks')
[0m09:18:57.556000 [info ] [MainThread]: 
[0m09:18:57.571629 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m09:18:57.571629 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:18:57.587295 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_default, idle-time=0s, language=None, compute-name=) - Creating connection
[0m09:18:57.587295 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m09:18:57.602899 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m09:18:57.602899 [debug] [ThreadPool]: On list_workspace_default: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "connection_name": "list_workspace_default"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'default'

  
[0m09:18:57.602899 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:18:58.384131 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f06ed1-b33d-1e80-a952-dcc52fafbee1) - Created
[0m09:18:59.134191 [debug] [ThreadPool]: SQL status: OK in 1.530 seconds
[0m09:18:59.134191 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f06ed1-b33d-1e80-a952-dcc52fafbee1, command-id=01f06ed1-b361-1030-bcc3-82c29e8d1af6) - Closing
[0m09:18:59.149750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ba9fa600-f4e2-4997-9456-25c58fccaf99', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBB1199610>]}
[0m09:18:59.149750 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:18:59.149750 [info ] [Thread-1 (]: 1 of 27 START test not_null_avg_discount_by_category_Category .................. [RUN]
[0m09:18:59.165373 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b, idle-time=0s, language=None, compute-name=) - Creating connection
[0m09:18:59.165373 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b'
[0m09:18:59.165373 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:18:59.212247 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b"
[0m09:18:59.212247 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:18:59.243504 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b"
[0m09:18:59.259134 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b"
[0m09:18:59.259134 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `workspace`.`default`.`avg_discount_by_category`
where Category is null



  
  
      
    ) dbt_internal_test
[0m09:18:59.259134 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:18:59.993569 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e) - Created
[0m09:19:01.321637 [debug] [Thread-1 (]: SQL status: OK in 2.060 seconds
[0m09:19:01.337266 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b452-155a-931c-8a16a7173d59) - Closing
[0m09:19:01.352885 [info ] [Thread-1 (]: 1 of 27 PASS not_null_avg_discount_by_category_Category ........................ [[32mPASS[0m in 2.19s]
[0m09:19:01.352885 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:19:01.352885 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:19:01.352885 [info ] [Thread-1 (]: 2 of 27 START test not_null_avg_discount_by_category_avg_discount_percent ...... [RUN]
[0m09:19:01.352885 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b, now test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101)
[0m09:19:01.352885 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:19:01.352885 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:19:01.368537 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101"
[0m09:19:01.368537 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:19:01.384170 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101"
[0m09:19:01.384170 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101"
[0m09:19:01.384170 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select avg_discount_percent
from `workspace`.`default`.`avg_discount_by_category`
where avg_discount_percent is null



  
  
      
    ) dbt_internal_test
[0m09:19:01.806037 [debug] [Thread-1 (]: SQL status: OK in 0.420 seconds
[0m09:19:01.806037 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b526-145d-9cc3-773c423dfc04) - Closing
[0m09:19:01.806037 [info ] [Thread-1 (]: 2 of 27 PASS not_null_avg_discount_by_category_avg_discount_percent ............ [[32mPASS[0m in 0.45s]
[0m09:19:01.806037 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:19:01.806037 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:19:01.806037 [info ] [Thread-1 (]: 3 of 27 START test not_null_avg_ticket_by_category_Category .................... [RUN]
[0m09:19:01.821621 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101, now test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f)
[0m09:19:01.821621 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f, idle-time=0.01558375358581543s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:19:01.821621 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:19:01.821621 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f"
[0m09:19:01.821621 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:19:01.837248 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f"
[0m09:19:01.837248 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f"
[0m09:19:01.837248 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `workspace`.`default`.`avg_ticket_by_category`
where Category is null



  
  
      
    ) dbt_internal_test
[0m09:19:02.274765 [debug] [Thread-1 (]: SQL status: OK in 0.440 seconds
[0m09:19:02.290439 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b56c-153f-9956-7bbfdbf7ee1f) - Closing
[0m09:19:02.290439 [info ] [Thread-1 (]: 3 of 27 PASS not_null_avg_ticket_by_category_Category .......................... [[32mPASS[0m in 0.47s]
[0m09:19:02.290439 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:19:02.290439 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:19:02.306021 [info ] [Thread-1 (]: 4 of 27 START test not_null_avg_ticket_by_category_avg_ticket .................. [RUN]
[0m09:19:02.306021 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f, now test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41)
[0m09:19:02.306021 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41, idle-time=0.015581846237182617s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:19:02.306021 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:19:02.321667 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41"
[0m09:19:02.321667 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:19:02.321667 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41"
[0m09:19:02.337248 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41"
[0m09:19:02.337248 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select avg_ticket
from `workspace`.`default`.`avg_ticket_by_category`
where avg_ticket is null



  
  
      
    ) dbt_internal_test
[0m09:19:02.790461 [debug] [Thread-1 (]: SQL status: OK in 0.450 seconds
[0m09:19:02.806008 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b5b6-1ac3-aa41-296ad1f07870) - Closing
[0m09:19:02.806008 [info ] [Thread-1 (]: 4 of 27 PASS not_null_avg_ticket_by_category_avg_ticket ........................ [[32mPASS[0m in 0.50s]
[0m09:19:02.806008 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:19:02.806008 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:19:02.806008 [info ] [Thread-1 (]: 5 of 27 START test not_null_monthly_revenue_month .............................. [RUN]
[0m09:19:02.806008 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41, now test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b)
[0m09:19:02.806008 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:19:02.821621 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:19:02.821621 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b"
[0m09:19:02.821621 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:19:02.837323 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b"
[0m09:19:02.837323 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b"
[0m09:19:02.837323 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select month
from `workspace`.`default`.`monthly_revenue`
where month is null



  
  
      
    ) dbt_internal_test
[0m09:19:03.212251 [debug] [Thread-1 (]: SQL status: OK in 0.370 seconds
[0m09:19:03.227872 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b604-1747-9538-9d1d6858f3c1) - Closing
[0m09:19:03.227872 [info ] [Thread-1 (]: 5 of 27 PASS not_null_monthly_revenue_month .................................... [[32mPASS[0m in 0.42s]
[0m09:19:03.227872 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:19:03.227872 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:19:03.243498 [info ] [Thread-1 (]: 6 of 27 START test not_null_monthly_revenue_monthly_revenue .................... [RUN]
[0m09:19:03.243498 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b, now test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b)
[0m09:19:03.243498 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b, idle-time=0.015625715255737305s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:19:03.243498 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:19:03.259134 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b"
[0m09:19:03.259134 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:19:03.259134 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b"
[0m09:19:03.259134 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b"
[0m09:19:03.274768 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select monthly_revenue
from `workspace`.`default`.`monthly_revenue`
where monthly_revenue is null



  
  
      
    ) dbt_internal_test
[0m09:19:03.696626 [debug] [Thread-1 (]: SQL status: OK in 0.420 seconds
[0m09:19:03.712249 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b645-1511-b555-ad79e272236c) - Closing
[0m09:19:03.712249 [info ] [Thread-1 (]: 6 of 27 PASS not_null_monthly_revenue_monthly_revenue .......................... [[32mPASS[0m in 0.47s]
[0m09:19:03.712249 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:19:03.712249 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:19:03.727880 [info ] [Thread-1 (]: 7 of 27 START test not_null_payment_distribution_Payment_Method ................ [RUN]
[0m09:19:03.727880 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b, now test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae)
[0m09:19:03.727880 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae, idle-time=0.015631437301635742s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:19:03.727880 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:19:03.743512 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae"
[0m09:19:03.759128 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:19:03.759128 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae"
[0m09:19:03.774754 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae"
[0m09:19:03.774754 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Payment_Method
from `workspace`.`default`.`payment_distribution`
where Payment_Method is null



  
  
      
    ) dbt_internal_test
[0m09:19:04.149772 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m09:19:04.149772 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b693-18ab-abe5-ca3caaf5f7f4) - Closing
[0m09:19:04.149772 [info ] [Thread-1 (]: 7 of 27 PASS not_null_payment_distribution_Payment_Method ...................... [[32mPASS[0m in 0.42s]
[0m09:19:04.165379 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:19:04.165379 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:19:04.165379 [info ] [Thread-1 (]: 8 of 27 START test not_null_payment_distribution_total_transactions ............ [RUN]
[0m09:19:04.181061 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae, now test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8)
[0m09:19:04.181061 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8, idle-time=0.03128862380981445s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:19:04.181061 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:19:04.212251 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8"
[0m09:19:04.212251 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:19:04.227874 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8"
[0m09:19:04.227874 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8"
[0m09:19:04.227874 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_transactions
from `workspace`.`default`.`payment_distribution`
where total_transactions is null



  
  
      
    ) dbt_internal_test
[0m09:19:04.649772 [debug] [Thread-1 (]: SQL status: OK in 0.420 seconds
[0m09:19:04.649772 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b6d9-13a3-bf52-b72120133b9d) - Closing
[0m09:19:04.649772 [info ] [Thread-1 (]: 8 of 27 PASS not_null_payment_distribution_total_transactions .................. [[32mPASS[0m in 0.48s]
[0m09:19:04.649772 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:19:04.665373 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:19:04.665373 [info ] [Thread-1 (]: 9 of 27 START test not_null_payment_distribution_total_value ................... [RUN]
[0m09:19:04.665373 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8, now test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03)
[0m09:19:04.665373 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03, idle-time=0.015600919723510742s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:19:04.665373 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:19:04.680998 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03"
[0m09:19:04.680998 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:19:04.696685 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03"
[0m09:19:04.696685 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03"
[0m09:19:04.696685 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_value
from `workspace`.`default`.`payment_distribution`
where total_value is null



  
  
      
    ) dbt_internal_test
[0m09:19:05.071709 [debug] [Thread-1 (]: SQL status: OK in 0.380 seconds
[0m09:19:05.071709 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b71f-142e-aa14-68d77085d3b7) - Closing
[0m09:19:05.087255 [info ] [Thread-1 (]: 9 of 27 PASS not_null_payment_distribution_total_value ......................... [[32mPASS[0m in 0.41s]
[0m09:19:05.087255 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:19:05.087255 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:19:05.087255 [info ] [Thread-1 (]: 10 of 27 START test not_null_sales_by_category_Category ........................ [RUN]
[0m09:19:05.087255 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03, now test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507)
[0m09:19:05.087255 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507, idle-time=0.015546321868896484s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:19:05.087255 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:19:05.102873 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507"
[0m09:19:05.102873 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:19:05.118542 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507"
[0m09:19:05.118542 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507"
[0m09:19:05.118542 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `workspace`.`default`.`sales_by_category`
where Category is null



  
  
      
    ) dbt_internal_test
[0m09:19:05.509165 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m09:19:05.524811 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b760-155b-ab9f-59646509300f) - Closing
[0m09:19:05.524811 [info ] [Thread-1 (]: 10 of 27 PASS not_null_sales_by_category_Category .............................. [[32mPASS[0m in 0.44s]
[0m09:19:05.524811 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:19:05.540412 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:19:05.540412 [info ] [Thread-1 (]: 11 of 27 START test not_null_sales_by_category_total_orders .................... [RUN]
[0m09:19:05.540412 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507, now test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e)
[0m09:19:05.540412 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e, idle-time=0.015601158142089844s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:19:05.540412 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:19:05.556044 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e"
[0m09:19:05.571623 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:19:05.571623 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e"
[0m09:19:05.571623 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e"
[0m09:19:05.571623 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_orders
from `workspace`.`default`.`sales_by_category`
where total_orders is null



  
  
      
    ) dbt_internal_test
[0m09:19:05.962269 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m09:19:05.962269 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b7a5-1663-8b31-7733ceee8ce1) - Closing
[0m09:19:05.962269 [info ] [Thread-1 (]: 11 of 27 PASS not_null_sales_by_category_total_orders .......................... [[32mPASS[0m in 0.42s]
[0m09:19:05.977883 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:19:05.977883 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:19:05.977883 [info ] [Thread-1 (]: 12 of 27 START test not_null_sales_by_category_total_revenue ................... [RUN]
[0m09:19:05.977883 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e, now test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd)
[0m09:19:05.977883 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd, idle-time=0.015614032745361328s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:19:05.977883 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:19:05.993502 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd"
[0m09:19:05.993502 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:19:05.993502 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd"
[0m09:19:06.009171 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd"
[0m09:19:06.009171 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_revenue
from `workspace`.`default`.`sales_by_category`
where total_revenue is null



  
  
      
    ) dbt_internal_test
[0m09:19:06.634197 [debug] [Thread-1 (]: SQL status: OK in 0.630 seconds
[0m09:19:06.649749 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b7e7-1e6c-aef9-7196818cfb22) - Closing
[0m09:19:06.649749 [info ] [Thread-1 (]: 12 of 27 PASS not_null_sales_by_category_total_revenue ......................... [[32mPASS[0m in 0.67s]
[0m09:19:06.649749 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:19:06.649749 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:19:06.649749 [info ] [Thread-1 (]: 13 of 27 START test not_null_top_5_products_product_id ......................... [RUN]
[0m09:19:06.649749 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd, now test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe)
[0m09:19:06.665380 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe, idle-time=0.015630245208740234s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:19:06.665380 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:19:06.665380 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe"
[0m09:19:06.665380 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:19:06.681041 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe"
[0m09:19:06.681041 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe"
[0m09:19:06.681041 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from `workspace`.`default`.`top_5_products`
where product_id is null



  
  
      
    ) dbt_internal_test
[0m09:19:07.071629 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m09:19:07.071629 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b84f-1419-abe7-9f44fbf97f47) - Closing
[0m09:19:07.071629 [info ] [Thread-1 (]: 13 of 27 PASS not_null_top_5_products_product_id ............................... [[32mPASS[0m in 0.42s]
[0m09:19:07.071629 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:19:07.071629 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:19:07.071629 [info ] [Thread-1 (]: 14 of 27 START test not_null_top_5_products_total_sales ........................ [RUN]
[0m09:19:07.071629 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe, now test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818)
[0m09:19:07.087249 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818, idle-time=0.015620231628417969s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:19:07.087249 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:19:07.087249 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818"
[0m09:19:07.102913 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:19:07.102913 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818"
[0m09:19:07.102913 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818"
[0m09:19:07.102913 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_sales
from `workspace`.`default`.`top_5_products`
where total_sales is null



  
  
      
    ) dbt_internal_test
[0m09:19:07.509198 [debug] [Thread-1 (]: SQL status: OK in 0.410 seconds
[0m09:19:07.524748 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b88f-1c3b-86cd-24e9515426e6) - Closing
[0m09:19:07.524748 [info ] [Thread-1 (]: 14 of 27 PASS not_null_top_5_products_total_sales .............................. [[32mPASS[0m in 0.45s]
[0m09:19:07.524748 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:19:07.524748 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:19:07.524748 [info ] [Thread-1 (]: 15 of 27 START test not_null_top_customers_User_ID ............................. [RUN]
[0m09:19:07.524748 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818, now test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0)
[0m09:19:07.524748 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:19:07.540414 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:19:07.540414 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0"
[0m09:19:07.540414 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:19:07.556043 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0"
[0m09:19:07.556043 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0"
[0m09:19:07.556043 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select User_ID
from `workspace`.`default`.`top_customers`
where User_ID is null



  
  
      
    ) dbt_internal_test
[0m09:19:07.946689 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m09:19:07.946689 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b8d4-1a7b-9d4b-407ca20a413e) - Closing
[0m09:19:07.946689 [info ] [Thread-1 (]: 15 of 27 PASS not_null_top_customers_User_ID ................................... [[32mPASS[0m in 0.42s]
[0m09:19:07.962250 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:19:07.962250 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:19:07.962250 [info ] [Thread-1 (]: 16 of 27 START test not_null_top_customers_total_orders ........................ [RUN]
[0m09:19:07.962250 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0, now test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746)
[0m09:19:07.962250 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746, idle-time=0.015561103820800781s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:19:07.962250 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:19:07.977915 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746"
[0m09:19:07.977915 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:19:07.977915 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746"
[0m09:19:07.993549 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746"
[0m09:19:07.993549 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_orders
from `workspace`.`default`.`top_customers`
where total_orders is null



  
  
      
    ) dbt_internal_test
[0m09:19:08.352948 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m09:19:08.368523 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b916-11f2-9aba-dff706efb7c6) - Closing
[0m09:19:08.368523 [info ] [Thread-1 (]: 16 of 27 PASS not_null_top_customers_total_orders .............................. [[32mPASS[0m in 0.41s]
[0m09:19:08.384135 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:19:08.384135 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:19:08.384135 [info ] [Thread-1 (]: 17 of 27 START test not_null_top_customers_total_spent ......................... [RUN]
[0m09:19:08.384135 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746, now test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73)
[0m09:19:08.399776 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73, idle-time=0.03125286102294922s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:19:08.399776 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:19:08.415434 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73"
[0m09:19:08.415434 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:19:08.415434 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73"
[0m09:19:08.430996 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73"
[0m09:19:08.430996 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_spent
from `workspace`.`default`.`top_customers`
where total_spent is null



  
  
      
    ) dbt_internal_test
[0m09:19:08.821726 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m09:19:08.837270 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b959-1f09-b4c2-c8efadf42c7c) - Closing
[0m09:19:08.837270 [info ] [Thread-1 (]: 17 of 27 PASS not_null_top_customers_total_spent ............................... [[32mPASS[0m in 0.45s]
[0m09:19:08.852878 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:19:08.852878 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:19:08.852878 [info ] [Thread-1 (]: 18 of 27 START test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery  [RUN]
[0m09:19:08.852878 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73, now test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931)
[0m09:19:08.852878 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931, idle-time=0.01560831069946289s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:19:08.852878 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:19:08.868505 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m09:19:08.868505 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:19:08.884170 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m09:19:08.884170 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m09:19:08.884170 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        Payment_Method as value_field,
        count(*) as n_records

    from `my_db`.`dbo`.`ecommerce`
    group by Payment_Method

)

select *
from all_values
where value_field not in (
    'Credit Card','Net Banking','UPI','Debit Card','Cash on Delivery'
)



  
  
      
    ) dbt_internal_test
[0m09:19:09.243541 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        Payment_Method as value_field,
        count(*) as n_records

    from `my_db`.`dbo`.`ecommerce`
    group by Payment_Method

)

select *
from all_values
where value_field not in (
    'Credit Card','Net Banking','UPI','Debit Card','Cash on Delivery'
)



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-b99f-172f-b880-e068afafd314
[0m09:19:09.259145 [debug] [Thread-1 (]: Database Error in test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_accepted_values_sqlserv_de5401a33d073d59e860da01ddb6837b.sql
[0m09:19:09.259145 [error] [Thread-1 (]: 18 of 27 ERROR source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery  [[31mERROR[0m in 0.41s]
[0m09:19:09.274747 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:19:09.274747 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:19:09.274747 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931' to be skipped because of status 'error'.  Reason: Database Error in test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_accepted_values_sqlserv_de5401a33d073d59e860da01ddb6837b.sql.
[0m09:19:09.274747 [info ] [Thread-1 (]: 19 of 27 START test source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date  [RUN]
[0m09:19:09.274747 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931, now test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93)
[0m09:19:09.274747 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93, idle-time=0.01560211181640625s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:19:09.274747 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:19:09.290415 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m09:19:09.290415 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:19:09.290415 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m09:19:09.306000 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m09:19:09.306000 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    with actual as (
        select
            column_name,
            data_type
        from INFORMATION_SCHEMA.COLUMNS
        where TABLE_NAME = 'ecommerce'
          and TABLE_SCHEMA = 'dbo'
    ),
    expected as (
        select * from (values
            ('User_ID', 'varchar'),
            ('Product_ID', 'varchar'),
            ('Category', 'varchar'),
            ('Price', 'float'),
            ('Discount', 'int'),
            ('Final_Price', 'float'),
            ('Payment_Method', 'varchar'),
            ('Purchase_Date', 'date')
            
        ) as t(column_name, data_type)
    ),
    diff_expected as (
        select * from expected
        EXCEPT
        select * from actual
    ),
    diff_actual as (
        select * from actual
        EXCEPT
        select * from expected
    )
    select * from diff_expected
    UNION ALL
    select * from diff_actual

  
  
      
    ) dbt_internal_test
[0m09:19:10.337327 [debug] [Thread-1 (]: SQL status: OK in 1.030 seconds
[0m09:19:10.352934 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, command-id=01f06ed1-b9de-1812-8b8b-7add540d1dcd) - Closing
[0m09:19:10.352934 [error] [Thread-1 (]: 19 of 27 FAIL 8 source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date  [[31mFAIL 8[0m in 1.08s]
[0m09:19:10.352934 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:19:10.352934 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:19:10.352934 [info ] [Thread-1 (]: 20 of 27 START test source_not_null_sqlserver_data_ecommerce_Category .......... [RUN]
[0m09:19:10.368508 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023)
[0m09:19:10.368508 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023, idle-time=0.015574932098388672s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:19:10.368508 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:19:10.368508 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m09:19:10.384122 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:19:10.384122 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m09:19:10.384122 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m09:19:10.384122 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `my_db`.`dbo`.`ecommerce`
where Category is null



  
  
      
    ) dbt_internal_test
[0m09:19:10.743572 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `my_db`.`dbo`.`ecommerce`
where Category is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-ba84-1687-b6c6-f1bdd9067688
[0m09:19:10.743572 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Category (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Category.sql
[0m09:19:10.759129 [error] [Thread-1 (]: 20 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Category ............... [[31mERROR[0m in 0.38s]
[0m09:19:10.759129 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:19:10.759129 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:19:10.759129 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Category (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Category.sql.
[0m09:19:10.759129 [info ] [Thread-1 (]: 21 of 27 START test source_not_null_sqlserver_data_ecommerce_Discount .......... [RUN]
[0m09:19:10.759129 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6)
[0m09:19:10.759129 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6, idle-time=0.015557050704956055s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:19:10.759129 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:19:10.774748 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m09:19:10.774748 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:19:10.790441 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m09:19:10.790441 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m09:19:10.790441 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Discount
from `my_db`.`dbo`.`ecommerce`
where Discount is null



  
  
      
    ) dbt_internal_test
[0m09:19:11.134186 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Discount
from `my_db`.`dbo`.`ecommerce`
where Discount is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-bac1-11d6-95ab-f63a27c5ec84
[0m09:19:11.149774 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Discount (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Discount.sql
[0m09:19:11.149774 [error] [Thread-1 (]: 21 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Discount ............... [[31mERROR[0m in 0.39s]
[0m09:19:11.149774 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:19:11.165386 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:19:11.165386 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Discount (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Discount.sql.
[0m09:19:11.165386 [info ] [Thread-1 (]: 22 of 27 START test source_not_null_sqlserver_data_ecommerce_Final_Price ....... [RUN]
[0m09:19:11.165386 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823)
[0m09:19:11.165386 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823, idle-time=0.015611886978149414s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:19:11.165386 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:19:11.181041 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m09:19:11.181041 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:19:11.196622 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m09:19:11.196622 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m09:19:11.196622 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Final_Price
from `my_db`.`dbo`.`ecommerce`
where Final_Price is null



  
  
      
    ) dbt_internal_test
[0m09:19:11.556074 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Final_Price
from `my_db`.`dbo`.`ecommerce`
where Final_Price is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-bb00-153d-8f9d-0b99e52b757a
[0m09:19:11.556074 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Final_Price.sql
[0m09:19:11.571629 [error] [Thread-1 (]: 22 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Final_Price ............ [[31mERROR[0m in 0.39s]
[0m09:19:11.571629 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:19:11.571629 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:19:11.571629 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Final_Price.sql.
[0m09:19:11.571629 [info ] [Thread-1 (]: 23 of 27 START test source_not_null_sqlserver_data_ecommerce_Payment_Method .... [RUN]
[0m09:19:11.571629 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32)
[0m09:19:11.571629 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32, idle-time=0.015554428100585938s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:19:11.571629 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:19:11.587249 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m09:19:11.587249 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:19:11.602918 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m09:19:11.602918 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m09:19:11.602918 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Payment_Method
from `my_db`.`dbo`.`ecommerce`
where Payment_Method is null



  
  
      
    ) dbt_internal_test
[0m09:19:12.056018 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Payment_Method
from `my_db`.`dbo`.`ecommerce`
where Payment_Method is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-bb3e-14dd-b214-351b04eebb63
[0m09:19:12.071642 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Payment_Method.sql
[0m09:19:12.071642 [error] [Thread-1 (]: 23 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Payment_Method ......... [[31mERROR[0m in 0.50s]
[0m09:19:12.071642 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:19:12.071642 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:19:12.071642 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Payment_Method.sql.
[0m09:19:12.087255 [info ] [Thread-1 (]: 24 of 27 START test source_not_null_sqlserver_data_ecommerce_Price ............. [RUN]
[0m09:19:12.087255 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a)
[0m09:19:12.087255 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a, idle-time=0.015612602233886719s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:19:12.087255 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:19:12.087255 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m09:19:12.102888 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:19:12.102888 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m09:19:12.102888 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m09:19:12.102888 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Price
from `my_db`.`dbo`.`ecommerce`
where Price is null



  
  
      
    ) dbt_internal_test
[0m09:19:12.446696 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Price
from `my_db`.`dbo`.`ecommerce`
where Price is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-bb8a-1e90-bdf2-6e81bc2fb894
[0m09:19:12.462273 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Price (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Price.sql
[0m09:19:12.462273 [error] [Thread-1 (]: 24 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Price .................. [[31mERROR[0m in 0.38s]
[0m09:19:12.477876 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:19:12.477876 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:19:12.477876 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Price (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Price.sql.
[0m09:19:12.477876 [info ] [Thread-1 (]: 25 of 27 START test source_not_null_sqlserver_data_ecommerce_Product_ID ........ [RUN]
[0m09:19:12.477876 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd)
[0m09:19:12.477876 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd, idle-time=0.015602350234985352s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:19:12.477876 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:19:12.493503 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m09:19:12.493503 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:19:12.509163 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m09:19:12.509163 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m09:19:12.509163 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Product_ID
from `my_db`.`dbo`.`ecommerce`
where Product_ID is null



  
  
      
    ) dbt_internal_test
[0m09:19:12.852879 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Product_ID
from `my_db`.`dbo`.`ecommerce`
where Product_ID is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-bbc7-1211-8645-852b02fcbc15
[0m09:19:12.868519 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Product_ID.sql
[0m09:19:12.868519 [error] [Thread-1 (]: 25 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Product_ID ............. [[31mERROR[0m in 0.39s]
[0m09:19:12.868519 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:19:12.868519 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:19:12.868519 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Product_ID.sql.
[0m09:19:12.884160 [info ] [Thread-1 (]: 26 of 27 START test source_not_null_sqlserver_data_ecommerce_Purchase_Date ..... [RUN]
[0m09:19:12.884160 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23)
[0m09:19:12.884160 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23, idle-time=0.015641212463378906s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:19:12.884160 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:19:12.884160 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m09:19:12.899760 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:19:12.899760 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m09:19:12.899760 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m09:19:12.899760 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Purchase_Date
from `my_db`.`dbo`.`ecommerce`
where Purchase_Date is null



  
  
      
    ) dbt_internal_test
[0m09:19:13.259221 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Purchase_Date
from `my_db`.`dbo`.`ecommerce`
where Purchase_Date is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-bc04-1e70-b700-d7323002b018
[0m09:19:13.274775 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Purchase_Date.sql
[0m09:19:13.274775 [error] [Thread-1 (]: 26 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Purchase_Date .......... [[31mERROR[0m in 0.39s]
[0m09:19:13.274775 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:19:13.290379 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Purchase_Date.sql.
[0m09:19:13.290379 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m09:19:13.290379 [info ] [Thread-1 (]: 27 of 27 START test source_not_null_sqlserver_data_ecommerce_User_ID ........... [RUN]
[0m09:19:13.290379 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325)
[0m09:19:13.290379 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325, idle-time=0.015603780746459961s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:19:13.290379 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m09:19:13.306006 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m09:19:13.306006 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m09:19:13.306006 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m09:19:13.306006 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m09:19:13.321626 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select User_ID
from `my_db`.`dbo`.`ecommerce`
where User_ID is null



  
  
      
    ) dbt_internal_test
[0m09:19:13.665390 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select User_ID
from `my_db`.`dbo`.`ecommerce`
where User_ID is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed1-bc43-1373-bcdd-7b8db9769f83
[0m09:19:13.665390 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_User_ID (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_User_ID.sql
[0m09:19:13.665390 [error] [Thread-1 (]: 27 of 27 ERROR source_not_null_sqlserver_data_ecommerce_User_ID ................ [[31mERROR[0m in 0.38s]
[0m09:19:13.681010 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m09:19:13.681010 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_User_ID (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_User_ID.sql.
[0m09:19:13.681010 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=14.53126072883606s, language=None, compute-name=) - Reusing connection previously named master
[0m09:19:13.681010 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:19:13.681010 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m09:19:13.681010 [debug] [MainThread]: On list_workspace_default: Close
[0m09:19:13.681010 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f06ed1-b33d-1e80-a952-dcc52fafbee1) - Closing
[0m09:19:13.899753 [debug] [MainThread]: Connection 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325' was properly closed.
[0m09:19:13.899753 [debug] [MainThread]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325: Close
[0m09:19:13.899753 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f06ed1-b434-1433-a8f9-ee8faa0baf0e) - Closing
[0m09:19:14.102951 [info ] [MainThread]: 
[0m09:19:14.118504 [info ] [MainThread]: Finished running 27 data tests in 0 hours 0 minutes and 16.55 seconds (16.55s).
[0m09:19:14.134125 [debug] [MainThread]: Command end result
[0m09:19:14.243496 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m09:19:14.259125 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m09:19:14.274753 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\run_results.json
[0m09:19:14.274753 [info ] [MainThread]: 
[0m09:19:14.274753 [info ] [MainThread]: [31mCompleted with 10 errors, 0 partial successes, and 0 warnings:[0m
[0m09:19:14.274753 [info ] [MainThread]: 
[0m09:19:14.274753 [error] [MainThread]: [31mFailure in test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:19:14.290377 [error] [MainThread]:   Database Error in test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_accepted_values_sqlserv_de5401a33d073d59e860da01ddb6837b.sql
[0m09:19:14.290377 [info ] [MainThread]: 
[0m09:19:14.290377 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_accepted_values_sqlserv_de5401a33d073d59e860da01ddb6837b.sql
[0m09:19:14.290377 [info ] [MainThread]: 
[0m09:19:14.290377 [error] [MainThread]: [31mFailure in test source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:19:14.290377 [error] [MainThread]:   Got 8 results, configured to fail if != 0
[0m09:19:14.290377 [info ] [MainThread]: 
[0m09:19:14.305996 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_assert_schema_matches_s_a76a1d3a3c37dcddf2ac3a6e53d8459a.sql
[0m09:19:14.305996 [info ] [MainThread]: 
[0m09:19:14.305996 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Category (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:19:14.305996 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Category (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Category.sql
[0m09:19:14.305996 [info ] [MainThread]: 
[0m09:19:14.305996 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Category.sql
[0m09:19:14.321621 [info ] [MainThread]: 
[0m09:19:14.321621 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Discount (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:19:14.321621 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Discount (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Discount.sql
[0m09:19:14.321621 [info ] [MainThread]: 
[0m09:19:14.321621 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Discount.sql
[0m09:19:14.321621 [info ] [MainThread]: 
[0m09:19:14.337248 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:19:14.337248 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Final_Price.sql
[0m09:19:14.337248 [info ] [MainThread]: 
[0m09:19:14.337248 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Final_Price.sql
[0m09:19:14.337248 [info ] [MainThread]: 
[0m09:19:14.337248 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:19:14.352883 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Payment_Method.sql
[0m09:19:14.352883 [info ] [MainThread]: 
[0m09:19:14.352883 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Payment_Method.sql
[0m09:19:14.352883 [info ] [MainThread]: 
[0m09:19:14.352883 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Price (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:19:14.352883 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Price (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Price.sql
[0m09:19:14.352883 [info ] [MainThread]: 
[0m09:19:14.368498 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Price.sql
[0m09:19:14.368498 [info ] [MainThread]: 
[0m09:19:14.368498 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:19:14.368498 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Product_ID.sql
[0m09:19:14.368498 [info ] [MainThread]: 
[0m09:19:14.368498 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Product_ID.sql
[0m09:19:14.368498 [info ] [MainThread]: 
[0m09:19:14.384130 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:19:14.384130 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Purchase_Date.sql
[0m09:19:14.384130 [info ] [MainThread]: 
[0m09:19:14.384130 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_Purchase_Date.sql
[0m09:19:14.384130 [info ] [MainThread]: 
[0m09:19:14.384130 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_User_ID (dbt_databricks_cicd/models\raw\schema.yml)[0m
[0m09:19:14.399755 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_User_ID (dbt_databricks_cicd/models\raw\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_User_ID.sql
[0m09:19:14.399755 [info ] [MainThread]: 
[0m09:19:14.399755 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\schema.yml\source_not_null_sqlserver_data_ecommerce_User_ID.sql
[0m09:19:14.399755 [info ] [MainThread]: 
[0m09:19:14.399755 [info ] [MainThread]: Done. PASS=17 WARN=0 ERROR=10 SKIP=0 NO-OP=0 TOTAL=27
[0m09:19:14.415380 [debug] [MainThread]: Command `dbt test` failed at 09:19:14.415380 after 26.14 seconds
[0m09:19:14.415380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA14A57D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FB9E180550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA48837D0>]}
[0m09:19:14.415380 [debug] [MainThread]: Flushing usage events
[0m09:19:15.009183 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:24:42.326892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261C58C0C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261C58C0210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261C58C1290>]}


============================== 09:24:42.342514 | 331f5dc6-e9ff-4705-aabd-8ddb4f72b6ec ==============================
[0m09:24:42.342514 [info ] [MainThread]: Running with dbt=1.10.3
[0m09:24:42.342514 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt test --target databricks', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:24:44.155015 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:24:44.155015 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:24:44.155015 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:24:45.951890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '331f5dc6-e9ff-4705-aabd-8ddb4f72b6ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261D01C7AD0>]}
[0m09:24:46.076889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '331f5dc6-e9ff-4705-aabd-8ddb4f72b6ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261C5039C50>]}
[0m09:24:46.076889 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m09:24:47.201891 [debug] [MainThread]: checksum: 3fbc73d786818920aecb1b2190495121ce0b60768fe54c46d521f1e82da30af9, vars: {}, profile: , target: databricks, version: 1.10.3
[0m09:24:47.686263 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m09:24:47.686263 [debug] [MainThread]: Partial parsing: added file: dbt_databricks_cicd://dbt_databricks_cicd/models\raw\sqlserver\schema.yml
[0m09:24:47.686263 [debug] [MainThread]: Partial parsing: updated file: dbt_databricks_cicd://dbt_databricks_cicd/models\raw\schema.yml
[0m09:24:48.701887 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "sqlserver_data_ecommerce".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("sqlserver_data", "ecommerce").
  
  To fix this, change the name of one of these resources:
  - source.dbt_databricks_cicd.sqlserver_data.ecommerce (dbt_databricks_cicd/models\raw\schema.yml)
  - source.dbt_databricks_cicd.sqlserver_data.ecommerce (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
[0m09:24:48.717555 [debug] [MainThread]: Command `dbt test` failed at 09:24:48.701887 after 6.54 seconds
[0m09:24:48.717555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261BF100510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261C5917FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261C59171D0>]}
[0m09:24:48.717555 [debug] [MainThread]: Flushing usage events
[0m09:24:49.311305 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:31:41.524110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291C1B97790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291C1B40350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291C1B41CD0>]}


============================== 09:31:41.524110 | dc00555c-bcf7-4d01-a76c-7ba7868fa0fc ==============================
[0m09:31:41.524110 [info ] [MainThread]: Running with dbt=1.10.3
[0m09:31:41.524110 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt test --target databricks', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:31:43.319126 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:31:43.319126 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:31:43.319126 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:31:45.147249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dc00555c-bcf7-4d01-a76c-7ba7868fa0fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291C1B55150>]}
[0m09:31:45.287879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dc00555c-bcf7-4d01-a76c-7ba7868fa0fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291C12B9C90>]}
[0m09:31:45.287879 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m09:31:46.350334 [debug] [MainThread]: checksum: 3fbc73d786818920aecb1b2190495121ce0b60768fe54c46d521f1e82da30af9, vars: {}, profile: , target: databricks, version: 1.10.3
[0m09:31:46.787873 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m09:31:46.787873 [debug] [MainThread]: Partial parsing: added file: dbt_databricks_cicd://dbt_databricks_cicd/models\raw\sqlserver\schema.yml
[0m09:31:48.084750 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.dbt_databricks_cicd.raw.sqlserver
- models.dbt_databricks_cicd.staging.sqlserver
[0m09:31:48.100374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dc00555c-bcf7-4d01-a76c-7ba7868fa0fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291CCD05710>]}
[0m09:31:48.319124 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m09:31:48.319124 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m09:31:48.412834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dc00555c-bcf7-4d01-a76c-7ba7868fa0fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291CE766850>]}
[0m09:31:48.412834 [info ] [MainThread]: Found 9 models, 27 data tests, 1 source, 682 macros
[0m09:31:48.412834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dc00555c-bcf7-4d01-a76c-7ba7868fa0fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291CE333110>]}
[0m09:31:48.428466 [info ] [MainThread]: 
[0m09:31:48.428466 [info ] [MainThread]: Concurrency: 1 threads (target='databricks')
[0m09:31:48.428466 [info ] [MainThread]: 
[0m09:31:48.428466 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m09:31:48.428466 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:31:48.444089 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_workspace_default, idle-time=0s, language=None, compute-name=) - Creating connection
[0m09:31:48.444089 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m09:31:48.475429 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m09:31:48.475429 [debug] [ThreadPool]: On list_workspace_default: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "connection_name": "list_workspace_default"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'workspace' 
  AND table_schema = 'default'

  
[0m09:31:48.475429 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:31:49.756644 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f06ed3-7eee-1bbe-b910-5065cf86bb17) - Created
[0m09:32:07.303535 [debug] [ThreadPool]: SQL status: OK in 18.830 seconds
[0m09:32:07.350335 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f06ed3-7eee-1bbe-b910-5065cf86bb17, command-id=01f06ed3-7f2d-103c-8c1e-ae2a56d128d2) - Closing
[0m09:32:07.694158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dc00555c-bcf7-4d01-a76c-7ba7868fa0fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291CE202190>]}
[0m09:32:07.725346 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:32:07.725346 [info ] [Thread-1 (]: 1 of 27 START test not_null_avg_discount_by_category_Category .................. [RUN]
[0m09:32:07.725346 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b, idle-time=0s, language=None, compute-name=) - Creating connection
[0m09:32:07.725346 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b'
[0m09:32:07.725346 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:32:07.756593 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b"
[0m09:32:07.756593 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:32:07.787886 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b"
[0m09:32:07.787886 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b"
[0m09:32:07.787886 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `workspace`.`default`.`avg_discount_by_category`
where Category is null



  
  
      
    ) dbt_internal_test
[0m09:32:07.803462 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:32:08.569161 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc) - Created
[0m09:32:12.272219 [debug] [Thread-1 (]: SQL status: OK in 4.470 seconds
[0m09:32:12.272219 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-8a5a-13b7-a2be-c8402ecb2a0e) - Closing
[0m09:32:12.287899 [info ] [Thread-1 (]: 1 of 27 PASS not_null_avg_discount_by_category_Category ........................ [[32mPASS[0m in 4.56s]
[0m09:32:12.303465 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:32:12.303465 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:32:12.303465 [info ] [Thread-1 (]: 2 of 27 START test not_null_avg_discount_by_category_avg_discount_percent ...... [RUN]
[0m09:32:12.303465 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b, now test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101)
[0m09:32:12.303465 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101, idle-time=0.015566825866699219s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b
[0m09:32:12.303465 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:32:12.319088 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101"
[0m09:32:12.319088 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:32:12.319088 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101"
[0m09:32:12.334709 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101"
[0m09:32:12.334709 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select avg_discount_percent
from `workspace`.`default`.`avg_discount_by_category`
where avg_discount_percent is null



  
  
      
    ) dbt_internal_test
[0m09:32:13.397274 [debug] [Thread-1 (]: SQL status: OK in 1.060 seconds
[0m09:32:13.397274 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-8c98-18f9-bc5f-2633bb2c719d) - Closing
[0m09:32:13.397274 [info ] [Thread-1 (]: 2 of 27 PASS not_null_avg_discount_by_category_avg_discount_percent ............ [[32mPASS[0m in 1.09s]
[0m09:32:13.412836 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:32:13.412836 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:32:13.412836 [info ] [Thread-1 (]: 3 of 27 START test not_null_avg_ticket_by_category_Category .................... [RUN]
[0m09:32:13.412836 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101, now test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f)
[0m09:32:13.428533 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f, idle-time=0.03125905990600586s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101
[0m09:32:13.428533 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:32:13.444092 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f"
[0m09:32:13.444092 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:32:13.444092 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f"
[0m09:32:13.444092 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f"
[0m09:32:13.444092 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `workspace`.`default`.`avg_ticket_by_category`
where Category is null



  
  
      
    ) dbt_internal_test
[0m09:32:14.256701 [debug] [Thread-1 (]: SQL status: OK in 0.810 seconds
[0m09:32:14.256701 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-8d42-1532-a215-c32980f2daa0) - Closing
[0m09:32:14.256701 [info ] [Thread-1 (]: 3 of 27 PASS not_null_avg_ticket_by_category_Category .......................... [[32mPASS[0m in 0.84s]
[0m09:32:14.272214 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:32:14.272214 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:32:14.272214 [info ] [Thread-1 (]: 4 of 27 START test not_null_avg_ticket_by_category_avg_ticket .................. [RUN]
[0m09:32:14.272214 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f, now test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41)
[0m09:32:14.272214 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41, idle-time=0.015512943267822266s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f
[0m09:32:14.272214 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:32:14.287846 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41"
[0m09:32:14.287846 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:32:14.303460 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41"
[0m09:32:14.303460 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41"
[0m09:32:14.303460 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select avg_ticket
from `workspace`.`default`.`avg_ticket_by_category`
where avg_ticket is null



  
  
      
    ) dbt_internal_test
[0m09:32:15.162918 [debug] [Thread-1 (]: SQL status: OK in 0.860 seconds
[0m09:32:15.162918 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-8dc4-1455-adac-daf0561d4f9d) - Closing
[0m09:32:15.162918 [info ] [Thread-1 (]: 4 of 27 PASS not_null_avg_ticket_by_category_avg_ticket ........................ [[32mPASS[0m in 0.89s]
[0m09:32:15.178457 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:32:15.178457 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:32:15.178457 [info ] [Thread-1 (]: 5 of 27 START test not_null_monthly_revenue_month .............................. [RUN]
[0m09:32:15.178457 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41, now test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b)
[0m09:32:15.178457 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b, idle-time=0.015538215637207031s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41
[0m09:32:15.178457 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:32:15.209715 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b"
[0m09:32:15.209715 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:32:15.225334 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b"
[0m09:32:15.225334 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b"
[0m09:32:15.225334 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select month
from `workspace`.`default`.`monthly_revenue`
where month is null



  
  
      
    ) dbt_internal_test
[0m09:32:16.162839 [debug] [Thread-1 (]: SQL status: OK in 0.940 seconds
[0m09:32:16.162839 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-8e52-100f-ae14-539c19f42d4b) - Closing
[0m09:32:16.162839 [info ] [Thread-1 (]: 5 of 27 PASS not_null_monthly_revenue_month .................................... [[32mPASS[0m in 0.98s]
[0m09:32:16.162839 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:32:16.162839 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:32:16.162839 [info ] [Thread-1 (]: 6 of 27 START test not_null_monthly_revenue_monthly_revenue .................... [RUN]
[0m09:32:16.178458 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b, now test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b)
[0m09:32:16.178458 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b, idle-time=0.015619277954101562s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b
[0m09:32:16.178458 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:32:16.209706 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b"
[0m09:32:16.209706 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:32:16.225334 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b"
[0m09:32:16.225334 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b"
[0m09:32:16.225334 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select monthly_revenue
from `workspace`.`default`.`monthly_revenue`
where monthly_revenue is null



  
  
      
    ) dbt_internal_test
[0m09:32:17.131660 [debug] [Thread-1 (]: SQL status: OK in 0.890 seconds
[0m09:32:17.131660 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-8ee9-1e16-92d1-c242102721d6) - Closing
[0m09:32:17.131660 [info ] [Thread-1 (]: 6 of 27 PASS not_null_monthly_revenue_monthly_revenue .......................... [[32mPASS[0m in 0.95s]
[0m09:32:17.147207 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:32:17.147207 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:32:17.147207 [info ] [Thread-1 (]: 7 of 27 START test not_null_payment_distribution_Payment_Method ................ [RUN]
[0m09:32:17.147207 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b, now test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae)
[0m09:32:17.147207 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae, idle-time=0.015546798706054688s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b
[0m09:32:17.147207 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:32:17.162833 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae"
[0m09:32:17.162833 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:32:17.162833 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae"
[0m09:32:17.162833 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae"
[0m09:32:17.178459 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Payment_Method
from `workspace`.`default`.`payment_distribution`
where Payment_Method is null



  
  
      
    ) dbt_internal_test
[0m09:32:18.014764 [debug] [Thread-1 (]: SQL status: OK in 0.840 seconds
[0m09:32:18.030393 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-8f79-1a29-941d-004bcd588c49) - Closing
[0m09:32:18.030393 [info ] [Thread-1 (]: 7 of 27 PASS not_null_payment_distribution_Payment_Method ...................... [[32mPASS[0m in 0.88s]
[0m09:32:18.030393 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:32:18.046028 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:32:18.046028 [info ] [Thread-1 (]: 8 of 27 START test not_null_payment_distribution_total_transactions ............ [RUN]
[0m09:32:18.046028 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae, now test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8)
[0m09:32:18.046028 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8, idle-time=0.015635013580322266s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae
[0m09:32:18.046028 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:32:18.073748 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8"
[0m09:32:18.073748 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:32:18.089387 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8"
[0m09:32:18.104997 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8"
[0m09:32:18.104997 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_transactions
from `workspace`.`default`.`payment_distribution`
where total_transactions is null



  
  
      
    ) dbt_internal_test
[0m09:32:18.799171 [debug] [Thread-1 (]: SQL status: OK in 0.690 seconds
[0m09:32:18.814713 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-9008-1562-80fd-0d8d383bc9f4) - Closing
[0m09:32:18.814713 [info ] [Thread-1 (]: 8 of 27 PASS not_null_payment_distribution_total_transactions .................. [[32mPASS[0m in 0.77s]
[0m09:32:18.814713 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:32:18.814713 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:32:18.814713 [info ] [Thread-1 (]: 9 of 27 START test not_null_payment_distribution_total_value ................... [RUN]
[0m09:32:18.814713 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8, now test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03)
[0m09:32:18.814713 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8
[0m09:32:18.830340 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:32:18.845975 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03"
[0m09:32:18.845975 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:32:18.845975 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03"
[0m09:32:18.845975 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03"
[0m09:32:18.845975 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_value
from `workspace`.`default`.`payment_distribution`
where total_value is null



  
  
      
    ) dbt_internal_test
[0m09:32:19.549150 [debug] [Thread-1 (]: SQL status: OK in 0.700 seconds
[0m09:32:19.564770 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-9079-1de7-96f3-83e41b29267a) - Closing
[0m09:32:19.564770 [info ] [Thread-1 (]: 9 of 27 PASS not_null_payment_distribution_total_value ......................... [[32mPASS[0m in 0.75s]
[0m09:32:19.564770 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:32:19.580361 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:32:19.580361 [info ] [Thread-1 (]: 10 of 27 START test not_null_sales_by_category_Category ........................ [RUN]
[0m09:32:19.580361 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03, now test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507)
[0m09:32:19.580361 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507, idle-time=0.015591859817504883s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03
[0m09:32:19.595984 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:32:19.595984 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507"
[0m09:32:19.611594 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:32:19.611594 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507"
[0m09:32:19.611594 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507"
[0m09:32:19.611594 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `workspace`.`default`.`sales_by_category`
where Category is null



  
  
      
    ) dbt_internal_test
[0m09:32:20.517931 [debug] [Thread-1 (]: SQL status: OK in 0.910 seconds
[0m09:32:20.533465 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-90ef-1a2e-aa15-5ed6890843e8) - Closing
[0m09:32:20.533465 [info ] [Thread-1 (]: 10 of 27 PASS not_null_sales_by_category_Category .............................. [[32mPASS[0m in 0.95s]
[0m09:32:20.533465 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:32:20.533465 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:32:20.533465 [info ] [Thread-1 (]: 11 of 27 START test not_null_sales_by_category_total_orders .................... [RUN]
[0m09:32:20.533465 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507, now test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e)
[0m09:32:20.533465 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507
[0m09:32:20.549126 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:32:20.549126 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e"
[0m09:32:20.549126 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:32:20.564721 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e"
[0m09:32:20.564721 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e"
[0m09:32:20.564721 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_orders
from `workspace`.`default`.`sales_by_category`
where total_orders is null



  
  
      
    ) dbt_internal_test
[0m09:32:21.314867 [debug] [Thread-1 (]: SQL status: OK in 0.750 seconds
[0m09:32:21.314867 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-9180-1814-824f-a10605a16208) - Closing
[0m09:32:21.330409 [info ] [Thread-1 (]: 11 of 27 PASS not_null_sales_by_category_total_orders .......................... [[32mPASS[0m in 0.80s]
[0m09:32:21.330409 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:32:21.330409 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:32:21.345978 [info ] [Thread-1 (]: 12 of 27 START test not_null_sales_by_category_total_revenue ................... [RUN]
[0m09:32:21.345978 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e, now test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd)
[0m09:32:21.345978 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd, idle-time=0.015568733215332031s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e
[0m09:32:21.345978 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:32:21.361594 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd"
[0m09:32:21.361594 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:32:21.361594 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd"
[0m09:32:21.361594 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd"
[0m09:32:21.361594 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_revenue
from `workspace`.`default`.`sales_by_category`
where total_revenue is null



  
  
      
    ) dbt_internal_test
[0m09:32:22.236710 [debug] [Thread-1 (]: SQL status: OK in 0.880 seconds
[0m09:32:22.236710 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-91fc-154d-86ae-8ae0c310708c) - Closing
[0m09:32:22.236710 [info ] [Thread-1 (]: 12 of 27 PASS not_null_sales_by_category_total_revenue ......................... [[32mPASS[0m in 0.89s]
[0m09:32:22.252251 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:32:22.252251 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:32:22.252251 [info ] [Thread-1 (]: 13 of 27 START test not_null_top_5_products_product_id ......................... [RUN]
[0m09:32:22.252251 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd, now test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe)
[0m09:32:22.252251 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe, idle-time=0.015541315078735352s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd
[0m09:32:22.252251 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:32:22.267879 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe"
[0m09:32:22.267879 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:32:22.267879 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe"
[0m09:32:22.283466 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe"
[0m09:32:22.283466 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from `workspace`.`default`.`top_5_products`
where product_id is null



  
  
      
    ) dbt_internal_test
[0m09:32:23.049165 [debug] [Thread-1 (]: SQL status: OK in 0.770 seconds
[0m09:32:23.064749 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-9285-15aa-a243-5332af367570) - Closing
[0m09:32:23.064749 [info ] [Thread-1 (]: 13 of 27 PASS not_null_top_5_products_product_id ............................... [[32mPASS[0m in 0.81s]
[0m09:32:23.064749 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:32:23.064749 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:32:23.064749 [info ] [Thread-1 (]: 14 of 27 START test not_null_top_5_products_total_sales ........................ [RUN]
[0m09:32:23.064749 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe, now test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818)
[0m09:32:23.064749 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe
[0m09:32:23.064749 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:32:23.080376 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818"
[0m09:32:23.080376 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:32:23.095966 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818"
[0m09:32:23.095966 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818"
[0m09:32:23.095966 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_sales
from `workspace`.`default`.`top_5_products`
where total_sales is null



  
  
      
    ) dbt_internal_test
[0m09:32:23.767890 [debug] [Thread-1 (]: SQL status: OK in 0.670 seconds
[0m09:32:23.767890 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-9301-1de4-9239-31251748e661) - Closing
[0m09:32:23.767890 [info ] [Thread-1 (]: 14 of 27 PASS not_null_top_5_products_total_sales .............................. [[32mPASS[0m in 0.70s]
[0m09:32:23.783472 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:32:23.783472 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:32:23.783472 [info ] [Thread-1 (]: 15 of 27 START test not_null_top_customers_User_ID ............................. [RUN]
[0m09:32:23.783472 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818, now test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0)
[0m09:32:23.783472 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0, idle-time=0.015581846237182617s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818
[0m09:32:23.783472 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:32:23.799130 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0"
[0m09:32:23.799130 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:32:23.799130 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0"
[0m09:32:23.814710 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0"
[0m09:32:23.814710 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select User_ID
from `workspace`.`default`.`top_customers`
where User_ID is null



  
  
      
    ) dbt_internal_test
[0m09:32:24.549163 [debug] [Thread-1 (]: SQL status: OK in 0.730 seconds
[0m09:32:24.564750 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-936f-1153-98a5-b8f928060200) - Closing
[0m09:32:24.564750 [info ] [Thread-1 (]: 15 of 27 PASS not_null_top_customers_User_ID ................................... [[32mPASS[0m in 0.78s]
[0m09:32:24.564750 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:32:24.564750 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:32:24.564750 [info ] [Thread-1 (]: 16 of 27 START test not_null_top_customers_total_orders ........................ [RUN]
[0m09:32:24.564750 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0, now test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746)
[0m09:32:24.580374 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746, idle-time=0.0s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0
[0m09:32:24.580374 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:32:24.580374 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746"
[0m09:32:24.580374 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:32:24.595966 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746"
[0m09:32:24.595966 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746"
[0m09:32:24.595966 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_orders
from `workspace`.`default`.`top_customers`
where total_orders is null



  
  
      
    ) dbt_internal_test
[0m09:32:25.330351 [debug] [Thread-1 (]: SQL status: OK in 0.730 seconds
[0m09:32:25.330351 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-93e6-1922-bf34-6d73b5f9c56e) - Closing
[0m09:32:25.330351 [info ] [Thread-1 (]: 16 of 27 PASS not_null_top_customers_total_orders .............................. [[32mPASS[0m in 0.77s]
[0m09:32:25.345965 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:32:25.345965 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:32:25.345965 [info ] [Thread-1 (]: 17 of 27 START test not_null_top_customers_total_spent ......................... [RUN]
[0m09:32:25.345965 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746, now test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73)
[0m09:32:25.345965 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73, idle-time=0.01561427116394043s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746
[0m09:32:25.361602 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:32:25.361602 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73"
[0m09:32:25.377211 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:32:25.377211 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73"
[0m09:32:25.377211 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73"
[0m09:32:25.377211 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_spent
from `workspace`.`default`.`top_customers`
where total_spent is null



  
  
      
    ) dbt_internal_test
[0m09:32:26.049083 [debug] [Thread-1 (]: SQL status: OK in 0.670 seconds
[0m09:32:26.049083 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-945e-168c-aed2-612363310a3b) - Closing
[0m09:32:26.049083 [info ] [Thread-1 (]: 17 of 27 PASS not_null_top_customers_total_spent ............................... [[32mPASS[0m in 0.70s]
[0m09:32:26.049083 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:32:26.049083 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:32:26.049083 [info ] [Thread-1 (]: 18 of 27 START test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery  [RUN]
[0m09:32:26.064709 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73, now test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931)
[0m09:32:26.064709 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931, idle-time=0.015626192092895508s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73
[0m09:32:26.064709 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:32:26.064709 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m09:32:26.080337 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:32:26.080337 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m09:32:26.095963 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"
[0m09:32:26.095963 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        Payment_Method as value_field,
        count(*) as n_records

    from `my_db`.`dbo`.`ecommerce`
    group by Payment_Method

)

select *
from all_values
where value_field not in (
    'Credit Card','Net Banking','UPI','Debit Card','Cash on Delivery'
)



  
  
      
    ) dbt_internal_test
[0m09:32:26.564787 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        Payment_Method as value_field,
        count(*) as n_records

    from `my_db`.`dbo`.`ecommerce`
    group by Payment_Method

)

select *
from all_values
where value_field not in (
    'Credit Card','Net Banking','UPI','Debit Card','Cash on Delivery'
)



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed3-94cb-1293-b804-399832ea7216
[0m09:32:26.596002 [debug] [Thread-1 (]: Database Error in test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_accepted_values_sqlserv_de5401a33d073d59e860da01ddb6837b.sql
[0m09:32:26.596002 [error] [Thread-1 (]: 18 of 27 ERROR source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery  [[31mERROR[0m in 0.53s]
[0m09:32:26.596002 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:32:26.596002 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:32:26.596002 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931' to be skipped because of status 'error'.  Reason: Database Error in test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_accepted_values_sqlserv_de5401a33d073d59e860da01ddb6837b.sql.
[0m09:32:26.596002 [info ] [Thread-1 (]: 19 of 27 START test source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date  [RUN]
[0m09:32:26.596002 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931, now test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93)
[0m09:32:26.611626 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93, idle-time=0.015624284744262695s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.380ed0b931
[0m09:32:26.611626 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:32:26.611626 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m09:32:26.627220 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:32:26.627220 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m09:32:26.627220 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"
[0m09:32:26.642839 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    with actual as (
        select
            column_name,
            data_type
        from INFORMATION_SCHEMA.COLUMNS
        where TABLE_NAME = 'ecommerce'
          and TABLE_SCHEMA = 'dbo'
    ),
    expected as (
        select * from (values
            ('User_ID', 'varchar'),
            ('Product_ID', 'varchar'),
            ('Category', 'varchar'),
            ('Price', 'float'),
            ('Discount', 'int'),
            ('Final_Price', 'float'),
            ('Payment_Method', 'varchar'),
            ('Purchase_Date', 'date')
            
        ) as t(column_name, data_type)
    ),
    diff_expected as (
        select * from expected
        EXCEPT
        select * from actual
    ),
    diff_actual as (
        select * from actual
        EXCEPT
        select * from expected
    )
    select * from diff_expected
    UNION ALL
    select * from diff_actual

  
  
      
    ) dbt_internal_test
[0m09:32:28.299142 [debug] [Thread-1 (]: SQL status: OK in 1.660 seconds
[0m09:32:28.299142 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, command-id=01f06ed3-951e-17d5-955e-af43d6e7b9b6) - Closing
[0m09:32:28.314718 [error] [Thread-1 (]: 19 of 27 FAIL 8 source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date  [[31mFAIL 8[0m in 1.72s]
[0m09:32:28.314718 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:32:28.314718 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:32:28.330401 [info ] [Thread-1 (]: 20 of 27 START test source_not_null_sqlserver_data_ecommerce_Category .......... [RUN]
[0m09:32:28.330401 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023)
[0m09:32:28.330401 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023, idle-time=0.015682220458984375s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date.478b2c7f93
[0m09:32:28.330401 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:32:28.346001 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m09:32:28.346001 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:32:28.361601 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m09:32:28.361601 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"
[0m09:32:28.361601 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `my_db`.`dbo`.`ecommerce`
where Category is null



  
  
      
    ) dbt_internal_test
[0m09:32:28.908517 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Category
from `my_db`.`dbo`.`ecommerce`
where Category is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed3-9626-151c-8fbf-851e8da487e8
[0m09:32:28.908517 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Category (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Category.sql
[0m09:32:28.924144 [error] [Thread-1 (]: 20 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Category ............... [[31mERROR[0m in 0.59s]
[0m09:32:28.924144 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:32:28.924144 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:32:28.924144 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Category (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Category.sql.
[0m09:32:28.924144 [info ] [Thread-1 (]: 21 of 27 START test source_not_null_sqlserver_data_ecommerce_Discount .......... [RUN]
[0m09:32:28.924144 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6)
[0m09:32:28.939755 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6, idle-time=0.015610218048095703s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Category.9952791023
[0m09:32:28.939755 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:32:28.939755 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m09:32:28.955437 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:32:28.955437 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m09:32:28.955437 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"
[0m09:32:28.955437 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Discount
from `my_db`.`dbo`.`ecommerce`
where Discount is null



  
  
      
    ) dbt_internal_test
[0m09:32:29.314767 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Discount
from `my_db`.`dbo`.`ecommerce`
where Discount is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed3-9680-1d90-b837-d36f1aa0663b
[0m09:32:29.330406 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Discount (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Discount.sql
[0m09:32:29.330406 [error] [Thread-1 (]: 21 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Discount ............... [[31mERROR[0m in 0.41s]
[0m09:32:29.330406 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:32:29.330406 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:32:29.330406 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Discount (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Discount.sql.
[0m09:32:29.330406 [info ] [Thread-1 (]: 22 of 27 START test source_not_null_sqlserver_data_ecommerce_Final_Price ....... [RUN]
[0m09:32:29.345958 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823)
[0m09:32:29.345958 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823, idle-time=0.015552520751953125s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Discount.325e0946d6
[0m09:32:29.345958 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:32:29.345958 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m09:32:29.361625 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:32:29.361625 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m09:32:29.361625 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"
[0m09:32:29.361625 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Final_Price
from `my_db`.`dbo`.`ecommerce`
where Final_Price is null



  
  
      
    ) dbt_internal_test
[0m09:32:29.877211 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Final_Price
from `my_db`.`dbo`.`ecommerce`
where Final_Price is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed3-96be-1a6f-a953-122d33d7e68a
[0m09:32:29.892836 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Final_Price.sql
[0m09:32:29.892836 [error] [Thread-1 (]: 22 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Final_Price ............ [[31mERROR[0m in 0.55s]
[0m09:32:29.892836 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:32:29.892836 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:32:29.908461 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Final_Price.sql.
[0m09:32:29.908461 [info ] [Thread-1 (]: 23 of 27 START test source_not_null_sqlserver_data_ecommerce_Payment_Method .... [RUN]
[0m09:32:29.908461 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32)
[0m09:32:29.908461 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32, idle-time=0.015624761581420898s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Final_Price.02a9691823
[0m09:32:29.908461 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:32:29.924084 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m09:32:29.939710 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:32:29.955336 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m09:32:29.955336 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"
[0m09:32:29.955336 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Payment_Method
from `my_db`.`dbo`.`ecommerce`
where Payment_Method is null



  
  
      
    ) dbt_internal_test
[0m09:32:30.314724 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Payment_Method
from `my_db`.`dbo`.`ecommerce`
where Payment_Method is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed3-9718-103d-a34a-ec4e767d6d14
[0m09:32:30.330408 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Payment_Method.sql
[0m09:32:30.330408 [error] [Thread-1 (]: 23 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Payment_Method ......... [[31mERROR[0m in 0.42s]
[0m09:32:30.345980 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:32:30.345980 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:32:30.345980 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Payment_Method.sql.
[0m09:32:30.345980 [info ] [Thread-1 (]: 24 of 27 START test source_not_null_sqlserver_data_ecommerce_Price ............. [RUN]
[0m09:32:30.361590 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a)
[0m09:32:30.361590 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a, idle-time=0.031181812286376953s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Payment_Method.5fc9e11b32
[0m09:32:30.361590 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:32:30.392836 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m09:32:30.392836 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:32:30.392836 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m09:32:30.408463 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"
[0m09:32:30.408463 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Price
from `my_db`.`dbo`.`ecommerce`
where Price is null



  
  
      
    ) dbt_internal_test
[0m09:32:30.767836 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Price
from `my_db`.`dbo`.`ecommerce`
where Price is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed3-975e-1cd5-bb08-74d50dbfb657
[0m09:32:30.783464 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Price (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Price.sql
[0m09:32:30.783464 [error] [Thread-1 (]: 24 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Price .................. [[31mERROR[0m in 0.42s]
[0m09:32:30.799086 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:32:30.799086 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:32:30.799086 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Price (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Price.sql.
[0m09:32:30.799086 [info ] [Thread-1 (]: 25 of 27 START test source_not_null_sqlserver_data_ecommerce_Product_ID ........ [RUN]
[0m09:32:30.799086 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd)
[0m09:32:30.814709 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd, idle-time=0.03124547004699707s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Price.ffd98ee55a
[0m09:32:30.814709 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:32:30.830338 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m09:32:30.830338 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:32:30.845961 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m09:32:30.845961 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"
[0m09:32:30.845961 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Product_ID
from `my_db`.`dbo`.`ecommerce`
where Product_ID is null



  
  
      
    ) dbt_internal_test
[0m09:32:31.220994 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Product_ID
from `my_db`.`dbo`.`ecommerce`
where Product_ID is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed3-97a0-112b-9daf-63d961c5675d
[0m09:32:31.236586 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Product_ID.sql
[0m09:32:31.236586 [error] [Thread-1 (]: 25 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Product_ID ............. [[31mERROR[0m in 0.44s]
[0m09:32:31.236586 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:32:31.236586 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:32:31.236586 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Product_ID.sql.
[0m09:32:31.236586 [info ] [Thread-1 (]: 26 of 27 START test source_not_null_sqlserver_data_ecommerce_Purchase_Date ..... [RUN]
[0m09:32:31.236586 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23)
[0m09:32:31.252210 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23, idle-time=0.015624284744262695s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Product_ID.1a04fc4acd
[0m09:32:31.252210 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:32:31.252210 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m09:32:31.252210 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:32:31.267848 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m09:32:31.267848 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"
[0m09:32:31.267848 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Purchase_Date
from `my_db`.`dbo`.`ecommerce`
where Purchase_Date is null



  
  
      
    ) dbt_internal_test
[0m09:32:31.689786 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select Purchase_Date
from `my_db`.`dbo`.`ecommerce`
where Purchase_Date is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed3-97e0-1cc0-9cc6-6911e9dfde76
[0m09:32:31.705395 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Purchase_Date.sql
[0m09:32:31.720999 [error] [Thread-1 (]: 26 of 27 ERROR source_not_null_sqlserver_data_ecommerce_Purchase_Date .......... [[31mERROR[0m in 0.47s]
[0m09:32:31.720999 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:32:31.720999 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m09:32:31.720999 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Purchase_Date.sql.
[0m09:32:31.720999 [info ] [Thread-1 (]: 27 of 27 START test source_not_null_sqlserver_data_ecommerce_User_ID ........... [RUN]
[0m09:32:31.720999 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23, now test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325)
[0m09:32:31.720999 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc, name=test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325, idle-time=0.015603780746459961s, language=sql, compute-name=) - Reusing connection previously named test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_Purchase_Date.ff0ecd3a23
[0m09:32:31.720999 [debug] [Thread-1 (]: Began compiling node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m09:32:31.736586 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m09:32:31.736586 [debug] [Thread-1 (]: Began executing node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m09:32:31.752215 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m09:32:31.752215 [debug] [Thread-1 (]: Using databricks connection "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"
[0m09:32:31.752215 [debug] [Thread-1 (]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325: /* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select User_ID
from `my_db`.`dbo`.`ecommerce`
where User_ID is null



  
  
      
    ) dbt_internal_test
[0m09:32:32.095963 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.3", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "dbt_databricks_cicd", "target_name": "databricks", "node_id": "test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select User_ID
from `my_db`.`dbo`.`ecommerce`
where User_ID is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=01f06ed3-982a-104d-8fff-0e0b296006c9
[0m09:32:32.111585 [debug] [Thread-1 (]: Database Error in test source_not_null_sqlserver_data_ecommerce_User_ID (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_User_ID.sql
[0m09:32:32.111585 [error] [Thread-1 (]: 27 of 27 ERROR source_not_null_sqlserver_data_ecommerce_User_ID ................ [[31mERROR[0m in 0.39s]
[0m09:32:32.111585 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325
[0m09:32:32.111585 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_sqlserver_data_ecommerce_User_ID (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_User_ID.sql.
[0m09:32:32.127210 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=24.417439937591553s, language=None, compute-name=) - Reusing connection previously named master
[0m09:32:32.127210 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:32:32.127210 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m09:32:32.127210 [debug] [MainThread]: On list_workspace_default: Close
[0m09:32:32.127210 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f06ed3-7eee-1bbe-b910-5065cf86bb17) - Closing
[0m09:32:32.346042 [debug] [MainThread]: Connection 'test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325' was properly closed.
[0m09:32:32.346042 [debug] [MainThread]: On test.dbt_databricks_cicd.source_not_null_sqlserver_data_ecommerce_User_ID.6485382325: Close
[0m09:32:32.361650 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f06ed3-8a35-121a-a826-51fefcf65abc) - Closing
[0m09:32:32.564815 [info ] [MainThread]: 
[0m09:32:32.564815 [info ] [MainThread]: Finished running 27 data tests in 0 hours 0 minutes and 44.14 seconds (44.14s).
[0m09:32:32.580360 [debug] [MainThread]: Command end result
[0m09:32:32.674087 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m09:32:32.674087 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m09:32:32.689750 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\run_results.json
[0m09:32:32.689750 [info ] [MainThread]: 
[0m09:32:32.689750 [info ] [MainThread]: [31mCompleted with 10 errors, 0 partial successes, and 0 warnings:[0m
[0m09:32:32.689750 [info ] [MainThread]: 
[0m09:32:32.705337 [error] [MainThread]: [31mFailure in test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)[0m
[0m09:32:32.705337 [error] [MainThread]:   Database Error in test source_accepted_values_sqlserver_data_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 20 pos 9
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_accepted_values_sqlserv_de5401a33d073d59e860da01ddb6837b.sql
[0m09:32:32.705337 [info ] [MainThread]: 
[0m09:32:32.705337 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_accepted_values_sqlserv_de5401a33d073d59e860da01ddb6837b.sql
[0m09:32:32.705337 [info ] [MainThread]: 
[0m09:32:32.705337 [error] [MainThread]: [31mFailure in test source_assert_schema_matches_sqlserver_data_ecommerce_varchar__varchar__varchar__float__int__float__varchar__date (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)[0m
[0m09:32:32.705337 [error] [MainThread]:   Got 8 results, configured to fail if != 0
[0m09:32:32.705337 [info ] [MainThread]: 
[0m09:32:32.720960 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_assert_schema_matches_s_a76a1d3a3c37dcddf2ac3a6e53d8459a.sql
[0m09:32:32.720960 [info ] [MainThread]: 
[0m09:32:32.720960 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Category (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)[0m
[0m09:32:32.720960 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Category (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Category.sql
[0m09:32:32.720960 [info ] [MainThread]: 
[0m09:32:32.720960 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Category.sql
[0m09:32:32.736624 [info ] [MainThread]: 
[0m09:32:32.736624 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Discount (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)[0m
[0m09:32:32.736624 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Discount (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Discount.sql
[0m09:32:32.736624 [info ] [MainThread]: 
[0m09:32:32.736624 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Discount.sql
[0m09:32:32.752211 [info ] [MainThread]: 
[0m09:32:32.752211 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)[0m
[0m09:32:32.752211 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Final_Price (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Final_Price.sql
[0m09:32:32.752211 [info ] [MainThread]: 
[0m09:32:32.752211 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Final_Price.sql
[0m09:32:32.752211 [info ] [MainThread]: 
[0m09:32:32.767837 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)[0m
[0m09:32:32.767837 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Payment_Method (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Payment_Method.sql
[0m09:32:32.767837 [info ] [MainThread]: 
[0m09:32:32.767837 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Payment_Method.sql
[0m09:32:32.767837 [info ] [MainThread]: 
[0m09:32:32.767837 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Price (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)[0m
[0m09:32:32.783464 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Price (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Price.sql
[0m09:32:32.783464 [info ] [MainThread]: 
[0m09:32:32.783464 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Price.sql
[0m09:32:32.783464 [info ] [MainThread]: 
[0m09:32:32.783464 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)[0m
[0m09:32:32.783464 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Product_ID (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Product_ID.sql
[0m09:32:32.799085 [info ] [MainThread]: 
[0m09:32:32.799085 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Product_ID.sql
[0m09:32:32.799085 [info ] [MainThread]: 
[0m09:32:32.799085 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)[0m
[0m09:32:32.799085 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_Purchase_Date (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Purchase_Date.sql
[0m09:32:32.799085 [info ] [MainThread]: 
[0m09:32:32.814735 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_Purchase_Date.sql
[0m09:32:32.814735 [info ] [MainThread]: 
[0m09:32:32.814735 [error] [MainThread]: [31mFailure in test source_not_null_sqlserver_data_ecommerce_User_ID (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)[0m
[0m09:32:32.814735 [error] [MainThread]:   Database Error in test source_not_null_sqlserver_data_ecommerce_User_ID (dbt_databricks_cicd/models\raw\sqlserver\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `my_db`.`dbo`.`ecommerce` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target\run\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_User_ID.sql
[0m09:32:32.814735 [info ] [MainThread]: 
[0m09:32:32.814735 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\schema.yml\source_not_null_sqlserver_data_ecommerce_User_ID.sql
[0m09:32:32.830339 [info ] [MainThread]: 
[0m09:32:32.830339 [info ] [MainThread]: Done. PASS=17 WARN=0 ERROR=10 SKIP=0 NO-OP=0 TOTAL=27
[0m09:32:32.830339 [debug] [MainThread]: Command `dbt test` failed at 09:32:32.830339 after 51.45 seconds
[0m09:32:32.830339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291C1B9C3D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291C1B971D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291CCDE4610>]}
[0m09:32:32.830339 [debug] [MainThread]: Flushing usage events
[0m09:32:33.939778 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:33:47.746749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FF8194310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FF8195D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FF8196D90>]}


============================== 11:33:47.755753 | 8284fc79-3519-4bbd-b8e6-dfae4a2c8289 ==============================
[0m11:33:47.755753 [info ] [MainThread]: Running with dbt=1.10.3
[0m11:33:47.757752 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt deps', 'send_anonymous_usage_stats': 'True'}
[0m11:33:48.370316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8284fc79-3519-4bbd-b8e6-dfae4a2c8289', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FF82875D0>]}
[0m11:33:48.503598 [debug] [MainThread]: Set downloads directory='C:\Users\diniz\AppData\Local\Temp\dbt-downloads-aq4f63qe'
[0m11:33:48.506729 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m11:33:49.616565 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m11:33:49.620576 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m11:33:49.794349 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m11:33:49.820900 [info ] [MainThread]: Updating lock file in file path: D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD/package-lock.yml
[0m11:33:49.838512 [debug] [MainThread]: Set downloads directory='C:\Users\diniz\AppData\Local\Temp\dbt-downloads-z1zcikw_'
[0m11:33:49.845538 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m11:33:50.638398 [info ] [MainThread]: Installed from version 1.3.0
[0m11:33:50.639385 [info ] [MainThread]: Up to date!
[0m11:33:50.641384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '8284fc79-3519-4bbd-b8e6-dfae4a2c8289', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FF8194550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FF82ADD90>]}
[0m11:33:50.646384 [debug] [MainThread]: Command `dbt deps` succeeded at 11:33:50.646384 after 3.07 seconds
[0m11:33:50.648385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FF1A46290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FF19C1290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FF19C0210>]}
[0m11:33:50.649384 [debug] [MainThread]: Flushing usage events
[0m11:33:51.248922 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:34:17.481569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF800C9690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF800CA610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF800C8510>]}


============================== 11:34:17.489502 | 0eac1926-bd53-4456-b98a-681bde0fc2a0 ==============================
[0m11:34:17.489502 [info ] [MainThread]: Running with dbt=1.10.3
[0m11:34:17.492504 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Documentos\\Python-SQL\\Projetos\\Projeto DBT Databricks CICD\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\diniz\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt build', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:34:23.412692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0eac1926-bd53-4456-b98a-681bde0fc2a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF8181DA50>]}
[0m11:34:23.537720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0eac1926-bd53-4456-b98a-681bde0fc2a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFEF51E50>]}
[0m11:34:23.539726 [info ] [MainThread]: Registered adapter: sqlserver=1.9.0
[0m11:34:24.711551 [debug] [MainThread]: checksum: 1bfacb87e6914c9942cfa2c4badfc1df19d859fe8ec086286ef238a7d920f97b, vars: {}, profile: , target: , version: 1.10.3
[0m11:34:24.946100 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m11:34:24.948096 [debug] [MainThread]: previous checksum: 1bfacb87e6914c9942cfa2c4badfc1df19d859fe8ec086286ef238a7d920f97b, current checksum: 3fbc73d786818920aecb1b2190495121ce0b60768fe54c46d521f1e82da30af9
[0m11:34:24.949112 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m11:34:24.951100 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m11:34:24.953744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0eac1926-bd53-4456-b98a-681bde0fc2a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFF6F4250>]}
[0m11:34:31.695858 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_top_customers_User_ID.7df96963c0' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'top_customers' which is disabled
[0m11:34:31.697856 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_top_customers_total_orders.32b439c746' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'top_customers' which is disabled
[0m11:34:31.698861 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.dbt_utils_expression_is_true_top_customers_total_orders__total_orders_0.08d7bb49d4' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'top_customers' which is disabled
[0m11:34:31.699862 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_top_customers_total_spent.426b0eae73' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'top_customers' which is disabled
[0m11:34:31.701859 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.dbt_utils_expression_is_true_top_customers_total_spent__total_spent_0.9290dc4343' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'top_customers' which is disabled
[0m11:34:31.703857 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_top_5_products_product_id.c6ac160bfe' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'top_5_products' which is disabled
[0m11:34:31.704855 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_top_5_products_total_sales.e20b869818' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'top_5_products' which is disabled
[0m11:34:31.706852 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.dbt_utils_expression_is_true_top_5_products_total_sales__total_sales_0.d3462b3b57' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'top_5_products' which is disabled
[0m11:34:31.708853 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_monthly_revenue_month.ee51ac609b' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'monthly_revenue' which is disabled
[0m11:34:31.711861 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_monthly_revenue_monthly_revenue.02fcba701b' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'monthly_revenue' which is disabled
[0m11:34:31.712857 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.dbt_utils_expression_is_true_monthly_revenue_monthly_revenue__monthly_revenue_0.f3c39d0a14' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'monthly_revenue' which is disabled
[0m11:34:31.714857 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_avg_discount_by_category_Category.237c8c010b' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'avg_discount_by_category' which is disabled
[0m11:34:31.715857 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_avg_discount_by_category_avg_discount_percent.622396a101' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'avg_discount_by_category' which is disabled
[0m11:34:31.716856 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.dbt_utils_expression_is_true_avg_discount_by_category_avg_discount_percent__avg_discount_percent_0.6056ce52e6' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'avg_discount_by_category' which is disabled
[0m11:34:31.719859 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_avg_ticket_by_category_Category.e0cd76451f' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'avg_ticket_by_category' which is disabled
[0m11:34:31.721860 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_avg_ticket_by_category_avg_ticket.68d6992a41' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'avg_ticket_by_category' which is disabled
[0m11:34:31.723856 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.dbt_utils_expression_is_true_avg_ticket_by_category_avg_ticket__avg_ticket_0.dc5a5b1301' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'avg_ticket_by_category' which is disabled
[0m11:34:31.727858 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_payment_distribution_Payment_Method.bd80df3cae' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'payment_distribution' which is disabled
[0m11:34:31.729855 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_payment_distribution_total_transactions.928411e2e8' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'payment_distribution' which is disabled
[0m11:34:31.730861 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.dbt_utils_expression_is_true_payment_distribution_total_transactions__total_transactions_0.0c63b16147' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'payment_distribution' which is disabled
[0m11:34:31.732853 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_payment_distribution_total_value.58135f5f03' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'payment_distribution' which is disabled
[0m11:34:31.734854 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.dbt_utils_expression_is_true_payment_distribution_total_value__total_value_0.a082f6ba97' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'payment_distribution' which is disabled
[0m11:34:31.736853 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_sales_by_category_Category.e69cdf4507' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'sales_by_category' which is disabled
[0m11:34:31.738854 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_sales_by_category_total_orders.024c423e1e' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'sales_by_category' which is disabled
[0m11:34:31.740854 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.dbt_utils_expression_is_true_sales_by_category_total_orders__total_orders_0.89435423b7' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'sales_by_category' which is disabled
[0m11:34:31.742858 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.not_null_sales_by_category_total_revenue.fb1162affd' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'sales_by_category' which is disabled
[0m11:34:31.746858 [debug] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_databricks_cicd.dbt_utils_expression_is_true_sales_by_category_total_revenue__total_revenue_0.c600e04fdd' (dbt_databricks_cicd/models\mart\databricks\schema.yml) depends on a node named 'sales_by_category' which is disabled
[0m11:34:31.962207 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_databricks_cicd.mart.sqlserver
[0m11:34:31.989697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0eac1926-bd53-4456-b98a-681bde0fc2a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF820DAB10>]}
[0m11:34:32.392995 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m11:34:32.428992 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m11:34:32.662446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0eac1926-bd53-4456-b98a-681bde0fc2a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF81F4E7D0>]}
[0m11:34:32.664445 [info ] [MainThread]: Found 2 models, 11 data tests, 1 source, 626 macros
[0m11:34:32.671440 [info ] [MainThread]: 
[0m11:34:32.673441 [info ] [MainThread]: Concurrency: 1 threads (target='sqlserver')
[0m11:34:32.674447 [info ] [MainThread]: 
[0m11:34:32.678446 [debug] [MainThread]: Acquiring new sqlserver connection 'master'
[0m11:34:32.699443 [debug] [ThreadPool]: Acquiring new sqlserver connection 'list_my_db'
[0m11:34:32.764444 [debug] [ThreadPool]: dbt-sqlserver
[0m11:34:32.767446 [debug] [ThreadPool]: Using sqlserver connection "list_my_db"
[0m11:34:32.769443 [debug] [ThreadPool]: On list_my_db: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "connection_name": "list_my_db"} */
USE [my_db];
    select  name as [schema]
    from sys.schemas with (nolock) 
    OPTION (LABEL = 'dbt-sqlserver');

  
[0m11:34:32.771443 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:34:32.774441 [debug] [ThreadPool]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=my_db;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m11:34:33.165432 [debug] [ThreadPool]: sqlserver adapter: Connected to db: my_db
[0m11:34:33.266541 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:34:33.270541 [debug] [ThreadPool]: On list_my_db: Close
[0m11:34:33.274541 [debug] [ThreadPool]: Acquiring new sqlserver connection 'list_my_db_dbo'
[0m11:34:33.293243 [debug] [ThreadPool]: dbt-sqlserver
[0m11:34:33.295915 [debug] [ThreadPool]: Using sqlserver connection "list_my_db_dbo"
[0m11:34:33.297136 [debug] [ThreadPool]: On list_my_db_dbo: /* {"app": "dbt", "dbt_version": "1.10.3", "profile_name": "dbt_databricks_cicd", "target_name": "sqlserver", "connection_name": "list_my_db_dbo"} */
USE [my_db];
    with base as (
      select
        DB_NAME() as [database],
        t.name as [name],
        SCHEMA_NAME(t.schema_id) as [schema],
        'table' as table_type
      from sys.tables as t with (nolock)
      union all
      select
        DB_NAME() as [database],
        v.name as [name],
        SCHEMA_NAME(v.schema_id) as [schema],
        'view' as table_type
      from sys.views as v with (nolock)
    )
    select * from base
    where [schema] like 'dbo'
    
    OPTION (LABEL = 'dbt-sqlserver');

  
[0m11:34:33.299142 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:34:33.300143 [debug] [ThreadPool]: sqlserver adapter: Using connection string: DRIVER={ODBC Driver 17 for SQL Server};SERVER=LAPTOP-NV4PR600,1433;Database=my_db;UID={dbt_user};PWD=***;encrypt=Yes;TrustServerCertificate=Yes;APP=dbt-sqlserver/1.9.0
[0m11:34:33.302138 [debug] [ThreadPool]: sqlserver adapter: Connected to db: my_db
[0m11:34:33.665828 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:34:33.669835 [debug] [ThreadPool]: On list_my_db_dbo: ROLLBACK
[0m11:34:33.670834 [debug] [ThreadPool]: On list_my_db_dbo: Close
[0m11:34:33.673868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0eac1926-bd53-4456-b98a-681bde0fc2a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF81F428D0>]}
[0m11:34:33.676836 [debug] [MainThread]: On master: COMMIT
[0m11:34:33.687836 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.src_ecommerce
[0m11:34:33.689839 [info ] [Thread-1 (]: 1 of 13 START sql view model dbo.src_ecommerce ................................. [RUN]
[0m11:34:33.692836 [debug] [Thread-1 (]: Acquiring new sqlserver connection 'model.dbt_databricks_cicd.src_ecommerce'
[0m11:34:33.694836 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.src_ecommerce
[0m11:34:33.735835 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.src_ecommerce"
[0m11:34:33.744840 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.src_ecommerce
[0m11:34:34.123406 [debug] [Thread-1 (]: Compilation Error in model src_ecommerce (dbt_databricks_cicd/models\raw\sqlserver\src_ecommerce.sql)
  When searching for a relation, dbt found an approximate match. Instead of guessing 
  which relation to use, dbt will move on. Please delete "MY_DB"."dbo"."src_ecommerce", or rename it to be less ambiguous.
  Searched for: "my_db"."dbo"."src_ecommerce"
  Found: "MY_DB"."dbo"."src_ecommerce"
  
  > in macro load_cached_relation (macros\adapters\relation.sql)
  > called by macro materialization_view_sqlserver (macros\materializations\models\view\view.sql)
  > called by model src_ecommerce (dbt_databricks_cicd/models\raw\sqlserver\src_ecommerce.sql)
[0m11:34:34.137397 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0eac1926-bd53-4456-b98a-681bde0fc2a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF81CF59D0>]}
[0m11:34:34.143401 [error] [Thread-1 (]: 1 of 13 ERROR creating sql view model dbo.src_ecommerce ........................ [[31mERROR[0m in 0.44s]
[0m11:34:34.149410 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.src_ecommerce
[0m11:34:34.151396 [debug] [Thread-1 (]: Began running node model.dbt_databricks_cicd.stg_ecommerce
[0m11:34:34.153395 [debug] [Thread-4 (]: Marking all children of 'model.dbt_databricks_cicd.src_ecommerce' to be skipped because of status 'error'.  Reason: Compilation Error in model src_ecommerce (dbt_databricks_cicd/models\raw\sqlserver\src_ecommerce.sql)
  When searching for a relation, dbt found an approximate match. Instead of guessing 
  which relation to use, dbt will move on. Please delete "MY_DB"."dbo"."src_ecommerce", or rename it to be less ambiguous.
  Searched for: "my_db"."dbo"."src_ecommerce"
  Found: "MY_DB"."dbo"."src_ecommerce"
  
  > in macro load_cached_relation (macros\adapters\relation.sql)
  > called by macro materialization_view_sqlserver (macros\materializations\models\view\view.sql)
  > called by model src_ecommerce (dbt_databricks_cicd/models\raw\sqlserver\src_ecommerce.sql).
[0m11:34:34.155395 [info ] [Thread-1 (]: 2 of 13 START sql view model dbo.stg_ecommerce ................................. [RUN]
[0m11:34:34.162411 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_databricks_cicd.src_ecommerce, now model.dbt_databricks_cicd.stg_ecommerce)
[0m11:34:34.165402 [debug] [Thread-1 (]: Began compiling node model.dbt_databricks_cicd.stg_ecommerce
[0m11:34:34.238410 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_databricks_cicd.stg_ecommerce"
[0m11:34:34.247419 [debug] [Thread-1 (]: Began executing node model.dbt_databricks_cicd.stg_ecommerce
[0m11:34:34.296396 [debug] [Thread-1 (]: Compilation Error in model stg_ecommerce (dbt_databricks_cicd/models\staging\sqlserver\stg_ecommerce.sql)
  When searching for a relation, dbt found an approximate match. Instead of guessing 
  which relation to use, dbt will move on. Please delete "MY_DB"."dbo"."stg_ecommerce", or rename it to be less ambiguous.
  Searched for: "my_db"."dbo"."stg_ecommerce"
  Found: "MY_DB"."dbo"."stg_ecommerce"
  
  > in macro load_cached_relation (macros\adapters\relation.sql)
  > called by macro materialization_view_sqlserver (macros\materializations\models\view\view.sql)
  > called by model stg_ecommerce (dbt_databricks_cicd/models\staging\sqlserver\stg_ecommerce.sql)
[0m11:34:34.299400 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0eac1926-bd53-4456-b98a-681bde0fc2a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF801250D0>]}
[0m11:34:34.303399 [error] [Thread-1 (]: 2 of 13 ERROR creating sql view model dbo.stg_ecommerce ........................ [[31mERROR[0m in 0.14s]
[0m11:34:34.352398 [debug] [Thread-1 (]: Finished running node model.dbt_databricks_cicd.stg_ecommerce
[0m11:34:34.355398 [debug] [Thread-4 (]: Marking all children of 'model.dbt_databricks_cicd.stg_ecommerce' to be skipped because of status 'error'.  Reason: Compilation Error in model stg_ecommerce (dbt_databricks_cicd/models\staging\sqlserver\stg_ecommerce.sql)
  When searching for a relation, dbt found an approximate match. Instead of guessing 
  which relation to use, dbt will move on. Please delete "MY_DB"."dbo"."stg_ecommerce", or rename it to be less ambiguous.
  Searched for: "my_db"."dbo"."stg_ecommerce"
  Found: "MY_DB"."dbo"."stg_ecommerce"
  
  > in macro load_cached_relation (macros\adapters\relation.sql)
  > called by macro materialization_view_sqlserver (macros\materializations\models\view\view.sql)
  > called by model stg_ecommerce (dbt_databricks_cicd/models\staging\sqlserver\stg_ecommerce.sql).
[0m11:34:34.359401 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.accepted_values_stg_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.bea16a73a6
[0m11:34:34.362399 [info ] [Thread-1 (]: 3 of 13 SKIP test accepted_values_stg_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery  [[33mSKIP[0m]
[0m11:34:34.366399 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.accepted_values_stg_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.bea16a73a6
[0m11:34:34.369398 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.dbt_utils_expression_is_true_stg_ecommerce_Discount__Discount_0.bad1b98323
[0m11:34:34.370397 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.accepted_values_stg_ecommerce_Payment_Method__Credit_Card__Net_Banking__UPI__Debit_Card__Cash_on_Delivery.bea16a73a6' to be skipped because of status 'skipped'. 
[0m11:34:34.372397 [info ] [Thread-1 (]: 4 of 13 SKIP test dbt_utils_expression_is_true_stg_ecommerce_Discount__Discount_0  [[33mSKIP[0m]
[0m11:34:34.417410 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.dbt_utils_expression_is_true_stg_ecommerce_Discount__Discount_0.bad1b98323
[0m11:34:34.420398 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.dbt_utils_expression_is_true_stg_ecommerce_Price__Price_0.13eba22b2d
[0m11:34:34.422403 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.dbt_utils_expression_is_true_stg_ecommerce_Discount__Discount_0.bad1b98323' to be skipped because of status 'skipped'. 
[0m11:34:34.427399 [info ] [Thread-1 (]: 5 of 13 SKIP test dbt_utils_expression_is_true_stg_ecommerce_Price__Price_0 .... [[33mSKIP[0m]
[0m11:34:34.482423 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.dbt_utils_expression_is_true_stg_ecommerce_Price__Price_0.13eba22b2d
[0m11:34:34.484397 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Category.24412d1c14
[0m11:34:34.485399 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.dbt_utils_expression_is_true_stg_ecommerce_Price__Price_0.13eba22b2d' to be skipped because of status 'skipped'. 
[0m11:34:34.488401 [info ] [Thread-1 (]: 6 of 13 SKIP test not_null_stg_ecommerce_Category .............................. [[33mSKIP[0m]
[0m11:34:34.495054 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Category.24412d1c14
[0m11:34:34.497403 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Discount.75a7cab836
[0m11:34:34.498404 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.not_null_stg_ecommerce_Category.24412d1c14' to be skipped because of status 'skipped'. 
[0m11:34:34.501420 [info ] [Thread-1 (]: 7 of 13 SKIP test not_null_stg_ecommerce_Discount .............................. [[33mSKIP[0m]
[0m11:34:34.505407 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Discount.75a7cab836
[0m11:34:34.509877 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Final_Price.8088a297fc
[0m11:34:34.511241 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.not_null_stg_ecommerce_Discount.75a7cab836' to be skipped because of status 'skipped'. 
[0m11:34:34.513246 [info ] [Thread-1 (]: 8 of 13 SKIP test not_null_stg_ecommerce_Final_Price ........................... [[33mSKIP[0m]
[0m11:34:34.597946 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Final_Price.8088a297fc
[0m11:34:34.599947 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Payment_Method.cc40c1cb5d
[0m11:34:34.600945 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.not_null_stg_ecommerce_Final_Price.8088a297fc' to be skipped because of status 'skipped'. 
[0m11:34:34.602946 [info ] [Thread-1 (]: 9 of 13 SKIP test not_null_stg_ecommerce_Payment_Method ........................ [[33mSKIP[0m]
[0m11:34:34.606975 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Payment_Method.cc40c1cb5d
[0m11:34:34.609950 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Price.5d9418fe6a
[0m11:34:34.612066 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.not_null_stg_ecommerce_Payment_Method.cc40c1cb5d' to be skipped because of status 'skipped'. 
[0m11:34:34.613067 [info ] [Thread-1 (]: 10 of 13 SKIP test not_null_stg_ecommerce_Price ................................ [[33mSKIP[0m]
[0m11:34:34.617066 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Price.5d9418fe6a
[0m11:34:34.619292 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Product_ID.e217887b0c
[0m11:34:34.620290 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.not_null_stg_ecommerce_Price.5d9418fe6a' to be skipped because of status 'skipped'. 
[0m11:34:34.622293 [info ] [Thread-1 (]: 11 of 13 SKIP test not_null_stg_ecommerce_Product_ID ........................... [[33mSKIP[0m]
[0m11:34:34.627829 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Product_ID.e217887b0c
[0m11:34:34.629833 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Purchase_Date.325f903307
[0m11:34:34.630834 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.not_null_stg_ecommerce_Product_ID.e217887b0c' to be skipped because of status 'skipped'. 
[0m11:34:34.633830 [info ] [Thread-1 (]: 12 of 13 SKIP test not_null_stg_ecommerce_Purchase_Date ........................ [[33mSKIP[0m]
[0m11:34:34.678099 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_stg_ecommerce_Purchase_Date.325f903307
[0m11:34:34.680100 [debug] [Thread-1 (]: Began running node test.dbt_databricks_cicd.not_null_stg_ecommerce_User_ID.b27c1a90bf
[0m11:34:34.681100 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.not_null_stg_ecommerce_Purchase_Date.325f903307' to be skipped because of status 'skipped'. 
[0m11:34:34.684116 [info ] [Thread-1 (]: 13 of 13 SKIP test not_null_stg_ecommerce_User_ID .............................. [[33mSKIP[0m]
[0m11:34:34.714534 [debug] [Thread-1 (]: Finished running node test.dbt_databricks_cicd.not_null_stg_ecommerce_User_ID.b27c1a90bf
[0m11:34:34.717780 [debug] [Thread-4 (]: Marking all children of 'test.dbt_databricks_cicd.not_null_stg_ecommerce_User_ID.b27c1a90bf' to be skipped because of status 'skipped'. 
[0m11:34:34.721780 [debug] [MainThread]: On master: COMMIT
[0m11:34:34.724786 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:34:34.727788 [debug] [MainThread]: Connection 'list_my_db' was properly closed.
[0m11:34:34.729802 [debug] [MainThread]: Connection 'list_my_db_dbo' was properly closed.
[0m11:34:34.731783 [debug] [MainThread]: Connection 'model.dbt_databricks_cicd.stg_ecommerce' was properly closed.
[0m11:34:34.734779 [info ] [MainThread]: 
[0m11:34:34.737781 [info ] [MainThread]: Finished running 11 data tests, 2 view models in 0 hours 0 minutes and 2.06 seconds (2.06s).
[0m11:34:34.745781 [debug] [MainThread]: Command end result
[0m11:34:35.025777 [debug] [MainThread]: Wrote artifact WritableManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\manifest.json
[0m11:34:35.030987 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\semantic_manifest.json
[0m11:34:35.043351 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\Documentos\Python-SQL\Projetos\Projeto DBT Databricks CICD\target\run_results.json
[0m11:34:35.045351 [info ] [MainThread]: 
[0m11:34:35.047580 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m11:34:35.048582 [info ] [MainThread]: 
[0m11:34:35.050582 [error] [MainThread]: [31mFailure in model src_ecommerce (dbt_databricks_cicd/models\raw\sqlserver\src_ecommerce.sql)[0m
[0m11:34:35.053580 [error] [MainThread]:   Compilation Error in model src_ecommerce (dbt_databricks_cicd/models\raw\sqlserver\src_ecommerce.sql)
  When searching for a relation, dbt found an approximate match. Instead of guessing 
  which relation to use, dbt will move on. Please delete "MY_DB"."dbo"."src_ecommerce", or rename it to be less ambiguous.
  Searched for: "my_db"."dbo"."src_ecommerce"
  Found: "MY_DB"."dbo"."src_ecommerce"
  
  > in macro load_cached_relation (macros\adapters\relation.sql)
  > called by macro materialization_view_sqlserver (macros\materializations\models\view\view.sql)
  > called by model src_ecommerce (dbt_databricks_cicd/models\raw\sqlserver\src_ecommerce.sql)
[0m11:34:35.055587 [info ] [MainThread]: 
[0m11:34:35.059582 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\raw\sqlserver\src_ecommerce.sql
[0m11:34:35.062819 [info ] [MainThread]: 
[0m11:34:35.064818 [error] [MainThread]: [31mFailure in model stg_ecommerce (dbt_databricks_cicd/models\staging\sqlserver\stg_ecommerce.sql)[0m
[0m11:34:35.066819 [error] [MainThread]:   Compilation Error in model stg_ecommerce (dbt_databricks_cicd/models\staging\sqlserver\stg_ecommerce.sql)
  When searching for a relation, dbt found an approximate match. Instead of guessing 
  which relation to use, dbt will move on. Please delete "MY_DB"."dbo"."stg_ecommerce", or rename it to be less ambiguous.
  Searched for: "my_db"."dbo"."stg_ecommerce"
  Found: "MY_DB"."dbo"."stg_ecommerce"
  
  > in macro load_cached_relation (macros\adapters\relation.sql)
  > called by macro materialization_view_sqlserver (macros\materializations\models\view\view.sql)
  > called by model stg_ecommerce (dbt_databricks_cicd/models\staging\sqlserver\stg_ecommerce.sql)
[0m11:34:35.068816 [info ] [MainThread]: 
[0m11:34:35.071817 [info ] [MainThread]:   compiled code at target\compiled\dbt_databricks_cicd\dbt_databricks_cicd/models\staging\sqlserver\stg_ecommerce.sql
[0m11:34:35.075837 [info ] [MainThread]: 
[0m11:34:35.080901 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=11 NO-OP=0 TOTAL=13
[0m11:34:35.084896 [debug] [MainThread]: Command `dbt build` failed at 11:34:35.084896 after 17.75 seconds
[0m11:34:35.086897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFF97D1290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFF97D0ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFF97D0250>]}
[0m11:34:35.088908 [debug] [MainThread]: Flushing usage events
[0m11:34:35.681276 [debug] [MainThread]: An error was encountered while trying to flush usage events
